<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://shvtr159.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shvtr159.github.io/" rel="alternate" type="text/html" /><updated>2023-03-02T22:45:00+09:00</updated><id>https://shvtr159.github.io/feed.xml</id><title type="html">Study Blog</title><subtitle>for study</subtitle><author><name>KYG</name></author><entry><title type="html">[논문 리뷰] Self-Supervised Pillar Motion Learning for Autonomous Driving</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/self-supervised-pillar-motion-learning-for-autonomous-driving/" rel="alternate" type="text/html" title="[논문 리뷰] Self-Supervised Pillar Motion Learning for Autonomous Driving" /><published>2022-01-12T00:00:00+09:00</published><updated>2022-01-12T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/self-supervised-pillar-motion-learning-for-autonomous-driving</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/self-supervised-pillar-motion-learning-for-autonomous-driving/">&lt;p&gt;본 논문은 CVPR 2021에 게재된 논문으로  CVPR 2019에 게재된 PointPillars라는 논문의 아이디어를 기반으로 Self-Supervised learning을 수행하였다. BEV로 표현하는 방법의 장점을 이용하기 위해 raw point cloud를 pillar로 organize한다. 이후 각각의 pillar 속도인 pillar motion을 예측한다. 이때, LiDAR의 sparse scan 특성 상 data가 조밀하지 않아 camera image로부터 얻어진 optical flow를 함께 사용한다.  이 구조는 다음과 같은 상호작용을 하며 학습한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Point cloud는 optical flow에서 ego-motion을 제외하는데 도움을 준다.&lt;/li&gt;
  &lt;li&gt;Image motion은 pillar motion을 학습하는데 auxiliary(보조) regularization를 제공한다.&lt;/li&gt;
  &lt;li&gt;back-project된 optical flow에 의해 생성된 probabilistic motion masking은 point cloud에서 구조적 일관성을 유지하도록 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;자세한 내용은 이 논문에서 설정한 Loss들을 확인해보면 알수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149094364-1b9ab86b-f4f0-4043-aa93-62bdae0e09bf.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;pillar motion estimation을 위해 제안된 self-supervised learning framework의 schematic overview&lt;/span&gt;&lt;/center&gt;
&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h3&gt;
&lt;p&gt;시간 $t$에서의 point cloud와 paired camera images를 $\mathcal{P}_ t = {\lbrace P_ i^t \rbrace}^{N_t}_ {i=1}$, $\mathcal{I}_ t = {\lbrace P_ i^t \rbrace}^{N_c}_ {i=1}$로 나타낸다($P_ i^t$ : point, $N_t$ : received point의 수, $I_ i^t$ : image, $N_c$ : 차에 장착된 camera의 수).&lt;/p&gt;

&lt;p&gt;다시 $\mathcal{P}_ t$는 겹치지 않는 pillar ${\lbrace \rho_ i^t \rbrace}^{N_p}_ {i=1}$로 이산화(discretized)된다($\rho_ i^t$ : pillar index, $N_p$ : pillar의 수). Pillar motion field는 $\mathcal{M}_ t = {\lbrace M_ i^t \rbrace}^{N_p}_ {i=1}$로 나타내면 다음 time에서의 각 pillar 위치는 다음과 같이 표현할 수 있다 : $\rho_ i^{
\sim t+1}=M_ i^t(\rho_i^t),\;M_ i^t\in\mathbb{R}^2$.&lt;/p&gt;

&lt;h3 id=&quot;lidar--based-structural-consistency&quot;&gt;LiDAR  based Structural Consistency&lt;/h3&gt;
&lt;p&gt;위에서 정의한 pillar motion $\mathcal{M}_ t$에 따라 각 pillar $\rho_i^t$의 motion vector $M_i^t$를 pillar 내 모든 point에 할당하여 point 별 motion을 얻을 수 있고, 수직 방향의 motion은 0으로 설정한다. 이렇게 point 별 motion을 얻으면 origin point cloud $\mathcal{P}_ t$로부터 다음 timestamp의 예상 point cloud인 $\widetilde{\mathcal{P}}_ {t+1}$를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 transformed point cloud $\widetilde{\mathcal{P}}_ {t+1}$와 실제 point cloud $\mathcal{P}_ {t+1}$간의 structural consistency를 pillar motion learning을 위한 free supervision으로 사용한다. 이를 나타내는 Loss는 다음과 같이 표현된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149133556-f71835ac-7fe7-4757-8f62-dad0b7bc3ed5.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;간단히 이야기하면, tramsform된 point들과 실제 point들 간에 한 point에서 가장 가까운 point와의 거리를 Loss로 사용한다고 할 수 있다. 그러나 LiDAR로 얻는 point cloud의 특성 상 $t$에서 얻은 point가 정확히 이동하여 $t+1$에 똑같이 존재할 수 없다. 때문에 이것만 사용하면 noise의 영향이 매우 크다.&lt;/p&gt;

&lt;h3 id=&quot;cross-sensor-motion-regularization&quot;&gt;Cross-Sensor Motion Regularization&lt;/h3&gt;
&lt;p&gt;LiDAR의 data가 sparse한것에 비해 카메라 image는 point cloud보다 dense하고 구분하기 쉬운 외관을 얻을 수 있다. 이 장점을 활용하기 위해 image도 같이 사용한다. 그 방법은 pillar motion을 projection하여 얻은 scene flow와 image로부터 얻은 optical flow를 비교하는 것이다.&lt;/p&gt;

&lt;p&gt;그러나 optical flow는 ego-motion, 즉 자기 자신이 움직이는것 때문에 관측하려는 물체의 실제 motion이 달라지게 된다. 때문에$I^t$와 $I^{t+1}$로 계산되는 $F^t$의 $(u,v)$픽셀에서의 optical flow는 다음과 같이 나타난다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149141628-e5ce6a96-162d-4d41-bdfc-d4e66b7d103f.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서, $I^t$와 관련된 point cloud $\mathcal{P}_ t$의 point $P^t_i$를 projection 했을 때 camera image에서의 위치는 다음과 같이 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149275347-bb378ace-973a-4922-91ea-3264c15c9e40.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;45%&quot; height=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$K$는 camera intrinsic parameter이고, $T_{L\to C}$는 LiDAR와 camera간의 상대적 pose이다. 이를 이용하면 ego-motion에 의해 유도된 optical flow를 다음과 같이 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149141290-141ad651-3805-4ca8-98b1-59b7103c91a7.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이때 $T_{t \to t+1}$은 ego-vehicle의 pose 변화로, ego-vehicle의 움직이 포함된 optical flow에서 이를 고려하지 않은 optical flow를 빼줌으로써 순수하게 ego-vehicle의 motion만 계산하는 것이다. (2)번 식과 (4)번식을 합치면 순수한 $F_{obj}^t$만을 얻을 수 있다. 유의해야할 것은 정확한 $F_{ego}^t$를 계산하기 위해 projected point와 대응되는 pixel에서만 $F_{obj}^t$를 계산한다.&lt;/p&gt;

&lt;p&gt;이제 pillar 별 motion vector $M^t_i$를 3번 식을 이용해 대응되는 image로 projection하면 projected optical flow인 $\widetilde{\mathcal{P}}^t(u_i,v_i)$를 얻을 수 있다. 이렇게 얻어진 2가지 optical flow의 차이를 다음과 같은 식으로 표현하고, Loss로 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149146801-fc942fd6-fbbb-4e5b-8487-d47da4288e82.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 loss는 이전의 structural consistency를 보조하고, sparse한 point cloud의 ambiguity를 완화한다. optical flow estimation과 관련하여, 전체 framework를 통합하여 self-supervised learning을 수행할 수 있도록 unsupervised 방법&lt;sup&gt;&lt;a href=&quot;#footnote_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;을 사용해 model을 training 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;probabilistic-motion-masking&quot;&gt;Probabilistic Motion Masking&lt;/h3&gt;
&lt;p&gt;ego-motion을 제외하고 남은 object motion에서 멈춰있는 물체가 noise로 인해 움직이는것으로 인식되어 똑같이 weight를 가지게 되면 제대로 된 motion을 예측하기 어렵다. 때문에 여기서는 다음 식과 같은 확률적 motion masking을 이용해 해당 object가 static할 확률을 계산하여 weight를 조절할 수 있도록 하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149274288-d8078642-1a3d-4b3e-9a3c-1e197ca4ec90.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;65%&quot; height=&quot;65%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;
&lt;p&gt;optical flow 추정을 위한 spatical smoothness constraint와 유사하게 pillar motion learning에서도 유사한 local smoothness loss를 사용한다. spatical smoothness constraint란 인접한 pixel은 일반적으로 기준 pixel과 유사한 움직임을 가질것이라는 것에서 비롯된 제한이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149275784-111c8736-9b13-442f-9d72-c909cf1fdb38.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\mathcal{M}^x_t$와 $\mathcal{M}^y_t$는 예측한 pillar motion field $\mathcal{M}_ t$의 $x, y$ 성분이고, $\bigtriangledown_x, \bigtriangledown_y$는 $x, y$방향의 기울기이다. 이 Loss는 동일한 물체에 속한 pillar가 유사한 움직임을 가지도록 하는데 도움을 준다.&lt;/p&gt;

&lt;h3 id=&quot;total-loss&quot;&gt;Total Loss&lt;/h3&gt;
&lt;p&gt;total loss는 위에서 나온 3개의 loss term의 weighted sum으로 계산한다. $\lambda$는 balancing coefficient이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/149296453-b4ff3f48-0adc-4d7a-b451-c8e4c2f5f41d.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;a name=&quot;footnote_1&quot;&gt;1&lt;/a&gt;: Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, and Wei Xu. Occlusion aware unsupervised learning of optical flow. In CVPR, 2018.&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">본 논문은 CVPR 2021에 게재된 논문으로 CVPR 2019에 게재된 PointPillars라는 논문의 아이디어를 기반으로 Self-Supervised learning을 수행하였다. BEV로 표현하는 방법의 장점을 이용하기 위해 raw point cloud를 pillar로 organize한다. 이후 각각의 pillar 속도인 pillar motion을 예측한다. 이때, LiDAR의 sparse scan 특성 상 data가 조밀하지 않아 camera image로부터 얻어진 optical flow를 함께 사용한다. 이 구조는 다음과 같은 상호작용을 하며 학습한다. Point cloud는 optical flow에서 ego-motion을 제외하는데 도움을 준다. Image motion은 pillar motion을 학습하는데 auxiliary(보조) regularization를 제공한다. back-project된 optical flow에 의해 생성된 probabilistic motion masking은 point cloud에서 구조적 일관성을 유지하도록 한다.</summary></entry><entry><title type="html">[논문 리뷰] Deep Learning for 3D Point Clouds: A Survey</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/deep-learning-for-3d-point-clouds-a-survey/" rel="alternate" type="text/html" title="[논문 리뷰] Deep Learning for 3D Point Clouds: A Survey" /><published>2021-12-30T00:00:00+09:00</published><updated>2021-12-30T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/deep-learning-for-3d-point-clouds-a-survey</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/deep-learning-for-3d-point-clouds-a-survey/">&lt;p&gt;본 survey는 IEEE TPAMI에 게재된 것으로 2020년 까지 Point cloud를 사용하는 Deep Learning에 대해 정리하였다.&lt;sup&gt;&lt;a href=&quot;#footnote_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 다음과 같이 크게 3개의 task에 대해 설명하지만 이 글에서는 Segmentation을 제외한 2가지 task에 대해 정리한다.&lt;/p&gt;
&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;h3 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h3&gt;
&lt;p&gt;각 task를 평가하기 위해 다양한 방법이 사용된다. 주로 사용되는 방법은 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;3D shape classification&lt;/strong&gt; : Overall Accuracy(OA), mean class accuracy(mAcc). OA는 모든 etst instances에 대한 평균 accuracy이고, mAcc는 모든 shape classes의 평균 accuracy를 의미한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3D object detection&lt;/strong&gt; : Average Precision(AP). precision-recall curve의 아래 면적으로 계산.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3D single object tracker&lt;/strong&gt; : Precision, Success&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3D multi object tracker&lt;/strong&gt; : Average Multi-Object Tracking Accuracy(AMOTA), Average Multi-Object Tracking Precision(AMOTP)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3D point cloud segmentation&lt;/strong&gt; : OA, mean Intersection over Union(mIoU), mean class Accuracy(mAcc), mean Average Precision(mAP).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3d-shape-classification&quot;&gt;3D Shape Classification&lt;/h2&gt;
&lt;h3 id=&quot;1-multi-view-based-methods&quot;&gt;1. Multi-view based Methods&lt;/h3&gt;
&lt;p&gt;이 방법은 먼저 3D shape를 다양한 view로 projection을 하고 각 view에서의 feature를 추출한다. 그리고 이 feature들을 fu&lt;/p&gt;
&lt;h2 id=&quot;3d-object-detection-and-tracking&quot;&gt;3D Object Detection and Tracking&lt;/h2&gt;
&lt;h3 id=&quot;3d-object-detection&quot;&gt;3D Object Detection&lt;/h3&gt;
&lt;h3 id=&quot;3d-object-tracking&quot;&gt;3D Object Tracking&lt;/h3&gt;
&lt;p&gt;첫 frame에서 object의 위치가 주어지면, 연속되는 frame에서 이 object의 state를 추정하고 위치를 찾는다. Point cloud는 풍부한 geometric 정보를 사용할 수 있기 때문에 image 기반의 tracking에서 겪던 occlusion이나 조명 및 scale 변화같은 단점을 극복할 수 있다. image 기반 tracking에서 Siamese network를 성공적으로 사용한것이 기반해서 SC3D는 shape completion regularization을 사용하는 3D Siamese network를 제안하였다. 여기서는 먼저 Kalman filter를 사용해서 후보를 생성하고, shape regularization을 사용하여 model과 후보들을 encoding 하였다. 그리고 다음 frame에서 tracking 하는 object의 위치를 찾기 위해 cosine similarity를 사용한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&quot;footnote_1&quot;&gt;1&lt;/a&gt;: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9127813&quot;&gt;Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu and M. Bennamoun, “Deep Learning for 3D Point Clouds: A Survey,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 12, pp. 4338-4364, 1 Dec. 2021, doi: 10.1109/TPAMI.2020.3005434.&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">본 survey는 IEEE TPAMI에 게재된 것으로 2020년 까지 Point cloud를 사용하는 Deep Learning에 대해 정리하였다.1 다음과 같이 크게 3개의 task에 대해 설명하지만 이 글에서는 Segmentation을 제외한 2가지 task에 대해 정리한다. Background Evaluation Metrics 각 task를 평가하기 위해 다양한 방법이 사용된다. 주로 사용되는 방법은 다음과 같다. 3D shape classification : Overall Accuracy(OA), mean class accuracy(mAcc). OA는 모든 etst instances에 대한 평균 accuracy이고, mAcc는 모든 shape classes의 평균 accuracy를 의미한다. 3D object detection : Average Precision(AP). precision-recall curve의 아래 면적으로 계산. 3D single object tracker : Precision, Success 3D multi object tracker : Average Multi-Object Tracking Accuracy(AMOTA), Average Multi-Object Tracking Precision(AMOTP) 3D point cloud segmentation : OA, mean Intersection over Union(mIoU), mean class Accuracy(mAcc), mean Average Precision(mAP).</summary></entry><entry><title type="html">[MLPR #] Unsupervised Classification 1 (fisher)</title><link href="https://shvtr159.github.io/mlpr/mlpr-unsupervised-classification-1/" rel="alternate" type="text/html" title="[MLPR #] Unsupervised Classification 1 (fisher)" /><published>2021-12-04T00:00:00+09:00</published><updated>2021-12-04T00:00:00+09:00</updated><id>https://shvtr159.github.io/mlpr/mlpr-unsupervised-classification-1</id><content type="html" xml:base="https://shvtr159.github.io/mlpr/mlpr-unsupervised-classification-1/">&lt;p&gt;Supervised 에서는 data에 label도 있고 전체 class의 갯수를 알 수 있기때문에 data로부터 그 분포를 추정할 수 있었다. unsupervised classification은 이러한 정보가 없을 때 data를 구분하는 방법이다. 
Unknown targets를 분류하는 방법에는 다양한 방법이 있는데 다음과 같은 방법이 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Criterion function 방법 : Sum of squared error, Min. variance, Scatter matrices, Optimization problem&lt;/li&gt;
  &lt;li&gt;Heuristic 방법 : Chain, Hierarchical, Min. spanning tree&lt;/li&gt;
  &lt;li&gt;Unmixing 방법 : Gaussian mixture, PCA, ICA&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;앞으로 다음 순서로 진행하며 unsupervised classification 방법에 대해 알아본다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Similarity and Similarity Measures&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chain Method of Clustering&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Clustering Criterion Functions&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Iterative Optimization&lt;/li&gt;
  &lt;li&gt;Clustering procedure – basic min. squared error&lt;/li&gt;
  &lt;li&gt;K-means Clustering&lt;/li&gt;
  &lt;li&gt;Hierarchical Clustering&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기까지는 Non-statistical한 방법이고, 이후 두 주제는 statistical한 방법이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mixture Densities: Gaussian Mixtures&lt;/li&gt;
  &lt;li&gt;Component Analysis: PCA, ICA&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;similarity-and-similarity-measures&quot;&gt;Similarity and Similarity Measures&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/146315768-90f03044-8073-4775-8140-9664492d7648.png&quot; alt=&quot;image&quot; width=&quot;40%&quot; height=&quot;40%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지는 2차 statistical(평균과 공분산)이 모두 같지만 smaple의 분포는 모두 다르다. 이를 분포의 혼합으로 만들어진다고 가정하면 근사화할 수 있겠지만 쉬운 작업은 아니다. 이러한 이유로 clustering을 사용하곤 한다. 그러나 clustering된 sample간의 유사성이나 분류를 평가하기 위해서는 측정 방법이 필요하다.&lt;/p&gt;

&lt;p&gt;클러스터링 결과를 Euclidean distance를 이용하여 측정하게 된다면 feature space에서의 translationis이나 rotation과 같은 변환에는 결과에 차이가 없지만, 일반적으로 scaling과 같은 선형 변환에 따라 결과가 변형되기 때문에 조심해야한다.&lt;/p&gt;

&lt;p&gt;다른 similarity  측정 방법&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Angular Similarity&lt;/strong&gt;&lt;br /&gt;
nonmetric similarity function으로 $S(x_i,x_j)=x_i^Tx_j/\left |x_i  \right |\left |x_j \right |$ (normalized 내적, $x_i$와 $x_j$사이 각의 코사인 값). 두 vector 사이의 각이 의미가 있을때 유용하다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Binary Similarity&lt;/strong&gt;&lt;br /&gt;
$S(x_i,x_j)=x_i^Tx_i+x_j^Tx_j-x_i^Tx_j$로 계산된다. $x_i^Tx_j$는 공통으로 나타나는 feature의 개수로 결국 $S$는공통으로 나타내는 feature의 비율이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chain-method-of-clustering&quot;&gt;Chain Method of Clustering&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;첫번째 sample을 cluster #1 ($S_1$)으로 할당한다.&lt;/li&gt;
  &lt;li&gt;다음 sample과 지금 sample간의 거리 $d$를 계산하고  $d_0$(미리 정해둔 threshold)와 비교한다. 만약 $d&amp;lt;d_0$이면 sample을 같은 cluster로 분류하고, 반대의 경우 새로운 cluster를 생성한다.&lt;/li&gt;
  &lt;li&gt;다음 sample도 존재하는 모든 cluster와의 거리 $d$를 계산하고 최소값을 찾는다. 만약 $d_{min}&amp;lt;d_{0}$이면 해당 cluster로 분류하고 반대의 경우 이전과 같이 새로운 cluster를 생성한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이때, $d$는 해당 cluster의 첫번째 sample과의 거리로 계산하거나 cluster의 sample mean과의 거리로 계산한다. 그러나 이 방법은 $d_0$와 sample의 차수에 sensitive하다는 문제점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;clustering-criterion-functions&quot;&gt;Clustering Criterion Functions&lt;/h2&gt;
&lt;p&gt;Clustering을 수행하고 나면 clustering이 잘 되었는지 확인해야 한다. 그래서 criterion fucntion을 사용해 이 function을 최소화하거나 최대화하는 값을 찾아 좋은 clustering을 선택한다. 앞으로 사용할 표기를 먼저 정의한다.&lt;/p&gt;

&lt;p&gt;$J$개의 sample $x_1, …, x_j$의 집합 $z$를 $K$개의 subset $z_1, z_2, …, z_k$로 나눈다. 이 $K$개의 집합들의 $J$개의 sample들의 cluster quality를 평가한다.&lt;/p&gt;
&lt;h3 id=&quot;1-sum-of-squared-error-criterion&quot;&gt;1. Sum of Squared Error Criterion&lt;/h3&gt;
&lt;p&gt;가장 간단하면서도 널리 쓰이는 방법으로 cluster의 평균과 그 sample들간의 거리의 차의 제곱을 모두 합한다.&lt;/p&gt;

\[J_e=\sum^K_{i=1}{\sum_{x_j\in z_i}\left \| x_j-m_i \right \|^2}\]

&lt;p&gt;$m_i$는 $z_i$의 sample들의 평균. Sum square error는 $J_e$값이 최소가 될때 최소의 variance를 가지므로 이때 cloud가 compact하고 각각 잘 분리된 좋은 cluster라고 이야기 할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/146320721-d69de763-7cc4-4a4f-97d9-7949a3bf2128.png&quot; alt=&quot;image&quot; width=&quot;40%&quot; height=&quot;40%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러나 위 이미지와 같이 분포가 다른 경우 잘못된 clustering을 수행할 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;2-minimum-variance-criteria&quot;&gt;2. Minimum-Variance Criteria&lt;/h3&gt;
&lt;p&gt;이 방법은 cluster의 모든 sample들간의 평균 제곱 거리를 이용한다.&lt;/p&gt;

\[J_e=\frac{1}{2}\sum^K_{i=1}J_i\bar{s}_ i\]

\[\bar{s}_ i=\frac{1}{J_i^2}\sum_{x_j\in z_i}{\sum_{x_l\in z_i}\left \| x_j-x_l \right \|^2}\]

&lt;p&gt;$\bar{s}_ i$의 다른 방법으로는  $\left | x_j-x_l \right |^2$ 대신 $s(x_j,x_l)$($s$는 similarity function)을 사용하거나 전체 합 대신 최소값을 사용하여 $\bar{s}_ i=\min_{x_j,x_l\in z_i}[s(x_j,x_l)]$을 사용하는 방법도 있다.&lt;/p&gt;

&lt;h3 id=&quot;3-scattering-criteria&quot;&gt;3. Scattering Criteria&lt;/h3&gt;
&lt;p&gt;Scatter matrix로부터 sample의 scattering을 측정한다. 여기에 사용되는 mean vector와 scatter matrix에는 다음이 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/146324824-0ddcbd07-8698-4076-9c66-836647f71f62.png&quot; alt=&quot;image&quot; width=&quot;90%&quot; height=&quot;90%&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체 scatter matrix$S_T$는 within-cluster(클러스터 내) scatter matrix$S_W$와 Between-cluster(클러스터 간) scatter matrix$S_B$의 합으로 이루어진다. Scatter의 양에 관해 더 정확하게 하기위해 scatter matrix의 크기의 Scalar 측정을 필요로 한다. 이 측정에는 3가지 기준이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Trace 기준&lt;/strong&gt;&lt;br /&gt;
대략적으로 trace는 좌표 방향들에서 분산들의 합의 비례하기 대문에 scattering 반경의 제곱을 측정한다.&lt;/li&gt;
&lt;/ul&gt;

\[Tr\{S_W\}=\sum^K_{i=1}{Tr\{S_i\}}=\sum^K_{i=1}\sum_{x_j\in z_i}{\left \| x_j-m_i \right \|^2}\]

\[Tr\{S_B\}=\sum^K_{i=1}J_i{\left \| m_i-m \right \|^2}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Determinant 기준&lt;/strong&gt;&lt;br /&gt;
Determinant는 main axis들의 방향에서 분산들의 곱에 비례하므로, scattering volume의 제곱을 측정한다.&lt;/li&gt;
&lt;/ul&gt;

\[J_d=\left |S_W\right |=\sum^K_{i=1}{\left |S_i\right |}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Invariant 기준 (불변적 기준)&lt;/strong&gt;&lt;br /&gt;
$\lambda_n$는 $S_W^{-1}S_B$의 n번째 eigenvalue이고 이는 lineartransformation에 불변하다.&lt;/li&gt;
&lt;/ul&gt;

\[J_f=Tr\{S_W^{-1}S_B\}=\sum^N_{n=1}{\lambda_n}\]

&lt;h2 id=&quot;fishers-linear-discriminent&quot;&gt;Fisher’s Linear Discriminent&lt;/h2&gt;</content><author><name>KYG</name></author><category term="MLPR" /><summary type="html">Supervised 에서는 data에 label도 있고 전체 class의 갯수를 알 수 있기때문에 data로부터 그 분포를 추정할 수 있었다. unsupervised classification은 이러한 정보가 없을 때 data를 구분하는 방법이다. Unknown targets를 분류하는 방법에는 다양한 방법이 있는데 다음과 같은 방법이 있다. Criterion function 방법 : Sum of squared error, Min. variance, Scatter matrices, Optimization problem Heuristic 방법 : Chain, Hierarchical, Min. spanning tree Unmixing 방법 : Gaussian mixture, PCA, ICA</summary></entry><entry><title type="html">[MLPR #] Bayes Decision Classification</title><link href="https://shvtr159.github.io/mlpr/mlpr-statistical-cassification1/" rel="alternate" type="text/html" title="[MLPR #] Bayes Decision Classification" /><published>2021-10-23T00:00:00+09:00</published><updated>2021-10-23T00:00:00+09:00</updated><id>https://shvtr159.github.io/mlpr/mlpr-statistical-cassification1</id><content type="html" xml:base="https://shvtr159.github.io/mlpr/mlpr-statistical-cassification1/">&lt;p&gt;Bayes Decision Theory : 이 방식은 모든 확률 값들을 알고 있다고 가정한다(평균, 분산 등).&lt;/p&gt;
&lt;h2 id=&quot;bayes-formula&quot;&gt;Bayes Formula&lt;/h2&gt;

\[P(S\mid x)=\frac{P(x\mid S)P(S_k)}{P(x)}\]

&lt;ul&gt;
  &lt;li&gt;$P(S\mid x)$ : posterior probability (사후 확률)&lt;/li&gt;
  &lt;li&gt;$P(x\mid S)$ : likelihood (우도, 가능도)&lt;/li&gt;
  &lt;li&gt;$P(S)$ : prior probability (사전 확률)&lt;/li&gt;
  &lt;li&gt;$P(x)$ : Evidence&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayes-decision-rule-for-최소-error-2-class&quot;&gt;Bayes Decision Rule for 최소 Error (2-class)&lt;/h2&gt;

\[\begin{matrix}
p(S_1\mid \underline{x})&amp;gt;p(S_2\mid \underline{x})\rightarrow \underline{x}\in S_1 \\ \\
p(S_1\mid \underline{x})&amp;lt;p(S_2\mid \underline{x})\rightarrow \underline{x}\in S_2
\end{matrix}\]

&lt;p&gt;즉, x는 두 class 중 확률이 더 높은 class에 속한다고 판단한다. 이를 Bayes Theorem을 이용하여 바꾸면&lt;/p&gt;

&lt;h3 id=&quot;decision-rule&quot;&gt;Decision Rule&lt;/h3&gt;

\[\begin{matrix}
p(\underline{x} \mid S_1)p(S_1)&amp;gt;p(\underline{x} \mid S_2)p(S_2) \rightarrow \underline{x}\in S_1 \\ \\
p(\underline{x} \mid S_1)p(S_1)&amp;lt;p(\underline{x} \mid S_2)p(S_2) \rightarrow \underline{x}\in S_2
\end{matrix}\]

&lt;p&gt;으로 표현되고 다음과 같이 다시 정리할 수 있다.&lt;/p&gt;

\[\frac{p(\underline{x} \mid S_1)}{p(\underline{x} \mid S_2)}\quad\begin{matrix}
\underline{x}\in S_1\\ 
&amp;gt;\\ 
&amp;lt;\\ 
\underline{x}\in S_2
\end{matrix}\quad \frac{P(S_1)}{P(S_2)}\]

&lt;p&gt;이 때, $l(x) = \frac{p(\underline{x} \mid S_1)}{p(\underline{x} \mid S_2)}$를 likelihood ratio(우도 비), $\frac{P(S_1)}{P(S_2)} = T$를 threshold value 라고 한다.&lt;/p&gt;

&lt;p&gt;위 정리된 식은 Log를 취해 *를 +로 바꿔 cost를 크게 줄일 수 있고, 다음과 같이 변경된다.&lt;/p&gt;

\[h(\underline{x})=-\textup{ln}(l(\underline{x}))=\textup{ln}(p(\underline{x} \mid S_2))-\textup{ln}(p(\underline{x} \mid S_1))&amp;lt;\textup{ln}(\frac{P(S_1)}{P(S_2)})\quad \Rightarrow \; \underline{x}\in S_1\]

&lt;h3 id=&quot;probability-of-error&quot;&gt;Probability of Error&lt;/h3&gt;
&lt;p&gt;이는 error region을 integrate 하는 형식으로 구해진다. 2 class의 경우&lt;/p&gt;

\[P_e=p(S_1)\int_{\Gamma_2}p(\underline{x} \mid S_1)dx+p(S_2)\int_{\Gamma_1}p(\underline{x} \mid S_2)dx\]

&lt;p&gt;이며 다중 class의 경우 $P_e=1-P(correct)$ 으로 알기 쉽게 표현할 수 있다. 이때, $P(correct)$ 은 $\sum_{i=1}^{K}\int_{\Gamma_i}p(\underline{x} \mid S_i)P(S_i)d\underline{x}$ 이다.&lt;/p&gt;
&lt;h2 id=&quot;bayes-decision-rule-for-최소-risk-2-class&quot;&gt;Bayes Decision Rule for 최소 Risk (2-class)&lt;/h2&gt;
&lt;p&gt;Bayes minimun error를 일반화 한다. $C(S_k \mid S_j)$를 $\underline{x}$가 $S_j$인데 $S_k$로 판별했을 때의 cost라고 하자. 그리고 이는 간단히 $C_{kj}=C(S_k \mid S_j)$로 표기한다(책에서는 $C_{kj}$ 대신 $\lambda_{kj}$로 표현하였다). 일반적으로 틀렸을 때 초래되는 손실은 맞을 때보다 더 크므로 $C_{21}&amp;gt;C_{11}$, $C_{12}&amp;gt;C_{22}$이다. 이를 한번에 matrix로 다음과 같이 표기한다.&lt;/p&gt;

\[\underline{C}=\begin{bmatrix}
C_{11} &amp;amp; C_{12}\\ 
C_{21} &amp;amp; C_{22}
\end{bmatrix}\]

&lt;p&gt;이때, 조건부 평균 risk (conditional avarage loss or risk) $R(S_k \mid \underline{x})$이고 2 class 에서 이 식은 모든 class의 값을 합한&lt;/p&gt;

\[\begin{matrix}
R(S_1 \mid \underline{x})=C(S_1 \mid S_1)P(S_1 \mid \underline{x})+C(S_1 \mid S_2)P(S_2 \mid \underline{x})\\ 
R(S_2 \mid \underline{x})=C(S_2 \mid S_1)P(S_1 \mid \underline{x})+C(S_2 \mid S_2)P(S_2 \mid \underline{x})
\end{matrix}\]

&lt;p&gt;이며, Decision cule은 total expected risk를 최소화 하기위한 action으로 다음과 같다.&lt;/p&gt;

\[R=\int_{\Gamma_1}R(S_1 \mid \underline{x})p(x)d\underline{x}+\int_{\Gamma_2}R(S_2 \mid \underline{x})p(x)d\underline{x}\]

&lt;p&gt;여기서 R을 최소화하기 위해 각 $\underline{x}$에 대한 $R(S_k \mid \underline{x})$이 최소화되는 영역으로 선택한다(Risk는 작을수록 좋으므로).&lt;/p&gt;

\[\begin{matrix}
R(S_1 \mid \underline{x}) &amp;lt; R(S_2 \mid \underline{x}) \rightarrow S_1 \\ 
R(S_1 \mid \underline{x}) &amp;gt; R(S_2 \mid \underline{x}) \rightarrow S_2 
\end{matrix}\]

&lt;h3 id=&quot;decision-rule-minimum-risk-classifier&quot;&gt;Decision Rule (Minimum Risk Classifier)&lt;/h3&gt;

\[R(S_1 \mid \underline{x}) \; \begin{matrix}
S_1\\ 
&amp;lt;\\ 
&amp;gt;\\ 
S_2
\end{matrix}\; R(S_2 \mid \underline{x})\]

&lt;p&gt;이를 다시 이전의 $C$를 이용한 식으로 바꾸면&lt;/p&gt;

\[C_{11}P(S_1 \mid \underline{x})+C_{12}P(S_2 \mid \underline{x}) \quad \begin{matrix}
&amp;lt;\\ 
&amp;gt;
\end{matrix}\; C_{21}P(S_1 \mid \underline{x})+C_{22}P(S_2 \mid \underline{x})\]

&lt;p&gt;이고, 같은 $P$끼리 정리하여 묶어낸 뒤 $P(S_k\mid x)=\frac{P(S_k)P(x\mid S_k)}{P(x)}$ 로 바꿔 쓴 뒤, likelihood ratio가 나타나도록 정리하면 최종적으로 다음 식이 된다.&lt;/p&gt;

\[l(\underline{x})=\frac{p(\underline{x} \mid S_1)}{p(\underline{x} \mid S_2)}\quad \begin{matrix}
&amp;gt;\\ 
&amp;lt;\\ 
\end{matrix}\quad \frac{(C_{12}-C_{22})P(S_2)}{(C_{21}-C_{11})P(S_1)}\]

&lt;p&gt;이때, 만약 $C_{11}=0, C_{12}=1, C_{21}=1,C_{22}=0$ 이면 다음과 같은 형태의 &lt;strong&gt;Bayes minimum error rule&lt;/strong&gt;이 된다.&lt;/p&gt;

\[l(\underline{x})=\frac{p(\underline{x} \mid S_1)}{p(\underline{x} \mid S_2)}\; \begin{matrix}
&amp;gt;\\ 
&amp;lt;\\ 
\end{matrix}\; \frac{P(S_2)}{P(S_1)}\]

&lt;h2 id=&quot;bayes-minimum-error-and-minimum-risk-다중-class&quot;&gt;Bayes Minimum Error and Minimum Risk (다중 class)&lt;/h2&gt;
&lt;h3 id=&quot;bayes-minimum-error--multiple-classes&quot;&gt;Bayes Minimum Error – Multiple Classes&lt;/h3&gt;
&lt;p&gt;모든 $i \neq j$에 대해 $P(S_i \mid \underline{x} )&amp;gt;P(S_j \mid \underline{x})$ 이면 $\underline{x}\in S_i$ 이다. 다시 변경하면, 모든 $i \neq j$에 대해 $p(\underline{x} \mid S_i)p(S_i)&amp;gt;p(\underline{x} \mid S_j)p(S_j)$ 이면 $\underline{x}\in S_i$ 이다. Discriminant function은 다음과 같다.&lt;/p&gt;

\[g_i(\underline{x})=p(\underline{x} \mid S_i)p(S_i)\]

&lt;h3 id=&quot;bayes-minimum-risk--multiple-classes&quot;&gt;Bayes Minimum Risk – Multiple Classes&lt;/h3&gt;
&lt;p&gt;$R(S_k \mid \underline{x})=\sum_{i=1}^{K}C_{ki}P(S_i\mid \underline{x})$를 conditional loss로 변경하면 $R(S_k \mid \underline{x})=\sum_{i=1}^{K}C_{ki}P(\underline{x} \mid S_i)P(S_i)$ 이다. 만약, 모든 $i \neq j$에 대해 $R_c(S_i \mid \underline{x})&amp;lt;R_c(S_j \mid \underline{x})$ 이면 $\underline{x}\in S_i$ 이다. Discriminant function은 다음과 같다.&lt;/p&gt;

\[g_k(\underline{x})=-R_c(S_k \mid \underline{x})\]

&lt;p&gt;’$-$’를 곱해주는 이유는 위의 표현처럼 $g_i(\underline{x})&amp;gt;g_j(\underline{x})$ 를 유지하기 위해서 이다.&lt;/p&gt;

&lt;p&gt;$R_c$ 식은 다음과 같이 행렬식의 곱 형태로 나타낼 수 있고 이 경우 2가지 special case를 확인하기에 편리하다.&lt;/p&gt;

\[R_c(S_k \mid \underline{x})=\begin{bmatrix}
C_{11} &amp;amp; C_{12} &amp;amp; \cdots\\ 
C_{21}&amp;amp; \ddots  &amp;amp; \\ 
\cdots  &amp;amp;  &amp;amp; C_{kk}
\end{bmatrix}\begin{bmatrix}
p(\underline{x} \mid S_1)P(S_1)\\ 
p(\underline{x} \mid S_2)P(S_2)\\ 
\cdots
\end{bmatrix}\]

&lt;ol&gt;
  &lt;li&gt;special case 1 : Symmetric 0-1 cost function&lt;/li&gt;
  &lt;li&gt;special case 2 : Diagonal Cost Function&lt;/li&gt;
&lt;/ol&gt;</content><author><name>KYG</name></author><category term="MLPR" /><summary type="html">Bayes Decision Theory : 이 방식은 모든 확률 값들을 알고 있다고 가정한다(평균, 분산 등). Bayes Formula</summary></entry><entry><title type="html">[MLPR #1] Introduction</title><link href="https://shvtr159.github.io/mlpr/mlpr-1-notation/" rel="alternate" type="text/html" title="[MLPR #1] Introduction" /><published>2021-10-20T00:00:00+09:00</published><updated>2021-10-20T00:00:00+09:00</updated><id>https://shvtr159.github.io/mlpr/mlpr-1-notation</id><content type="html" xml:base="https://shvtr159.github.io/mlpr/mlpr-1-notation/">&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$\underline{ \textbf{x}}$ : data 벡터&lt;/li&gt;
  &lt;li&gt;prototypes:
    &lt;ul&gt;
      &lt;li&gt;$\underline{y}_ m^{(k)}$ : class $S_ {k}$에 속한 m번째 prototype&lt;/li&gt;
      &lt;li&gt;k = class index&lt;/li&gt;
      &lt;li&gt;$m_k$ = $S_ {k}$에 있는 prototype들의 번호&lt;/li&gt;
      &lt;li&gt;$\underline{y}_ m^{(k)}$, m = 1,2, … , $m_k$ 는  class $S_ {k}$의 모든 prototype로 정의&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;
&lt;p&gt;각각의 Class를 나누기 위해서는 Decision Rule이 결정되어야 한다. 예를 들어 변수 값에 따라 분류 할 때, 값이 5.5 초과면 $S_ {1}$, 이하면 $S_ {2}$로 분류할 수 있다. 그러나 이러한 feature가 1개일때보다는 2개일때 더 정확한 분류를 할 수 있고, 더 많아질수록 더 정확히 분류할 수 있다. 이렇게 fature의 차원이 커짐에 따라 decision surface의 차원도 점, 선, 면 으로 점점 증가하게 된다.&lt;/p&gt;

&lt;p&gt;이렇게 나눠진 많은 Class들 중 $\underline{ \textbf{x}}$ 가 어디에 속하는 지 알기 위해서는 해당 class와 $\underline{ \textbf{x}}$의 거리를 측정해야 한다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="MLPR" /><summary type="html">Notation $\underline{ \textbf{x}}$ : data 벡터 prototypes: $\underline{y}_ m^{(k)}$ : class $S_ {k}$에 속한 m번째 prototype k = class index $m_k$ = $S_ {k}$에 있는 prototype들의 번호 $\underline{y}_ m^{(k)}$, m = 1,2, … , $m_k$ 는 class $S_ {k}$의 모든 prototype로 정의</summary></entry><entry><title type="html">[논문 리뷰] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space/" rel="alternate" type="text/html" title="[논문 리뷰] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space" /><published>2021-09-27T00:00:00+09:00</published><updated>2021-09-27T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space/">&lt;p&gt;PointNet++은 PointNet이 point가 존재하는 metric space에 의해 유도된 local structure를 활용하지 못해서 fine-grained&lt;sup&gt;&lt;a href=&quot;#footnote_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 패턴들을 잘 인식하지 못하고 복잡한 scene들을 일반화하는 능력이 떨어지는 문제를 해결하기 위해 제안되었다. PointNet++은 입력 point set의 nested partitioning에 PointNet을 recursive하게 적용하는 hierarchical 신경망이다. 이 네트워크는 metric space distance를 활용하여 contextual scale이 증가하는 local feature를 학습할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;기존 PointNet의 아이디어는 각 point의 공간 encoding을 학습하고, 모든 개별 point feature들을 global 포인트 클라우드 signature와 aggregate하는 것이었다. 그러나 이런 구조는 metric에 의해 유도된 local structure를 활용하지 않는다. 그러나 convolutional architecture의 성공에 local structure를 활용하는 것이 중요한 요소라는 것이 증명되어왔다. CNN은 multi-resolution hierarchy를 따라 점점 더 큰 scale의 feature를 찾아낼 수 있다. CNN 구조 특성상 낮은 level의 layer에서는 작은 receptive field를 가지고 높은 layer에서는 큰 receptive field를 가지기 때문이다. 이러한 hierarchy를 따라 local pattern들을 추상화하는 기능은 일반화에 더욱더 유용하다.&lt;/p&gt;

&lt;p&gt;해서, PointNet++이라는 hierarchical 신경망을 소개한다. 먼저, point set을 distance metric에 따라 겹치는 local 영역으로 분할한다. 이를 통해 위에서 이야기한 CNN과 마찬가지로 작은 neighborhood에서 작은 geometric structure를 고려하는 local feature를 추출한다. 이 local feature는 더 큰 unit으로 그룹화되고, 더 높은 level의 feature가 생성되도록 한다. 이 작업을 전체 point set의 feature를 얻을 때까지 반복한다.&lt;/p&gt;

&lt;p&gt;PointNet++은 다음 두 가지 문제를 해결할 수 있어야 한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;point set의 분할을 생성하는 방법&lt;/li&gt;
  &lt;li&gt;local feature learner를 통해 point set이나 local featrue를 추상화하는 방법&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 두 문제는 서로 연관되어있는데 그 이유는 pointset의 분할은 여러 partition들에 걸쳐 공통적인 구조를 만들어 내야 하기 때문이다. 따라서, convolution 설정에서와 같이 local feature learner들은 weight를 공유할 수 있다. 이 local feature learner로 PointNet을 사용한다. PointNet은 basic building block으로서, local points나 features를 더 높은 level의 표현으로 추상화한다. 이러한 관점에서, PontNet++은 input set의 nested partitioning에 PointNet을 recursive하게 적용한다고 할 수 있다.&lt;/p&gt;

&lt;p&gt;point set의 overlapping partitioning 방법도 주된 문제이다. 일단, 각 partition은 Euclidean space에서 중심 위치와 scale을 parameter로 가지는 neighborhood ball로 정의한다. 또, 모든 set을 균일하게 다루기 위해 farthest point smapling (FPS)&lt;sup&gt;&lt;a href=&quot;#footnote_2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 알고리즘을 이용해 중심점을 선택한다.&lt;/p&gt;

&lt;p&gt;PointNet++의 주된 contribution은 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여러 scale에서 neighborhoods를 활용해 robustnessd와 detail capture를 달성하였다.&lt;/li&gt;
  &lt;li&gt;학습시키는 동안 random input dropout을 이용해 network가 찾아진 pattern들에 대해 adaptive하게 weight를 적용하고, input data에 따라 multi-scale feature들을 결합하는 방법을 학습하도록 하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-problem-statement&quot;&gt;2. Problem Statement&lt;/h2&gt;
&lt;p&gt;$\mathcal{X}=(M,d)$ 가 Euclidean space $\mathbb R^n$에서 상속된 discrete metric space라고 가정한다. 이때, $M\subseteq \mathbb R^n$ 은 point set이고, $d$는 distance metric이다. 또, Euclidean space $M$는 밀도가 균일하지 않은 부분도 있다. 이러한 $\mathcal{X}$를 입력으로 받고 $\mathcal{X}$에 대한 semantic interest의 정보를 생성하는 set function $f$를 생성하도록 학습시킨다. 이 $f$는 $\mathcal{X}$에 label을 할당하는 classification 함수나 $M$의 각 멤버에 point 별 label을 할당하는 segmentation 함수가 될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;3. Method&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/135207876-43abf960-6fd9-4205-9eb1-e45c67ad0028.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;hierarchical feature learning 구조&lt;/span&gt;&lt;/center&gt;

&lt;h3 id=&quot;32-hierarchical-point-set-feature-learning&quot;&gt;3.2 Hierarchical Point Set Feature Learning&lt;/h3&gt;
&lt;p&gt;PointNet++의 hierarchical 구조는 여러 개의 &lt;em&gt;set abstraction&lt;/em&gt; level로 구성된다. 각 level에서는 points set이 처리되고 추상화되어 더 적은 수의 element를 가지는 새로운 set이 생성된다. set abstraction level은 다음 3가지 key layer로 구성되며, 자세한 설명은 뒤에 다시 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Sampling layer&lt;/em&gt; : local region의 centroid를 찾는다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Grouping layer&lt;/em&gt; : centroid 주위의 “neighboring” point들을 찾아 local region sets를 구성한다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;PoinNet layer&lt;/em&gt; : mini-PointNet을 사용해서 local region pattern들을 feature vector로 encoding 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;set abstraction level은  input으로 $N\times (d+C)$ ($N$ : point 개수, $d$ : coordinates dimension, $C$ : point feature dimension) matrix를 받고  output으로 $N’\times (d+C’)$ ($N’$ : subsampling된 point 개수, $C’$ : local context를 요약한 feature vector dimensioin)  matrix를 출력한다.&lt;/p&gt;

&lt;h4 id=&quot;sampling-layer&quot;&gt;Sampling layer&lt;/h4&gt;
&lt;p&gt;input points $\lbrace x_{1}, x_{2}, …, x_{n} \rbrace$가 주어질 때, iterative farthest point sampling (FPS)을 사용하여 input의 부분집합 $\lbrace x_{i_{1}}, x_{i_{2}}, …, x_{i_{m}} \rbrace$를 생성한다. FPS는 이전까지 sampling된 point들에서 가장 멀리 있는 point를 선택하는 방법으로 이때 $x_{i_{j}}$는 $\lbrace x_{i_{1}}, x_{i_{2}}, …, x_{i_{j-1}} \rbrace$의 point들, 즉 이전까지 sampling된 point들에서 가장 먼 point가 된다. 이렇게 선택된 $N’$개의 centroid와 그 좌표값($N’\times d$)을 Grouping layer의 input으로 사용한다. 이 방법은 random sampling과 비교했을 때 더 우수한 결과를 얻을 수 있었고, point cloud는 데이터 분포가 균일하지 않기 때문에 이러한 방식으로 데이터에 의존하여 receptive field를 생성한다.&lt;/p&gt;

&lt;h4 id=&quot;grouping-layer&quot;&gt;Grouping layer&lt;/h4&gt;
&lt;p&gt;Grouping layer는 $N\times (d+C)$ 크기의 point set과 $N’\times d$ 크기의 centriods set를 input으로 받아 grouping을 수행한다. output은 $N’\times K\times (d+C)$ 의 point set 그룹들이다. 이 각 그룹은 local region이고, $K$는 centriod points의 neighborhood point 개수(local region에 속하는 point 개수)이다. 여기서는 ball query를 이용해 $K$가 그룹에 따라 다르지만, 다음의 PointNet layer가 point의 수를 flexible하게 고정된 길이의 local region feature vector로 변환해준다.&lt;/p&gt;

&lt;p&gt;ball query는 query point에서 radius 내에 있는 모든 point를 찾는다. 이 방법은 고정된 개수의 주위 point를 찾는 $K$ nearest neighbor (kNN)과 비교했을 때, 영역의 크기가 고정되어있기 때문에 공간 전반에 걸쳐 local region feature가 더 일반화될 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;pointnet-layer&quot;&gt;PointNet layer&lt;/h4&gt;
&lt;p&gt;input은Grouping layer에서 나온 $N’\times K\times (d+C)$의 크기를 가지는 $N’$개의 local region이다. output의 각 local region은 그 영역의 centriod와 centroid의 주변을 encoding 하는 local feature에 의해 추상화된다. 그 크기는 $N’\times (d+C’)$이다.&lt;/p&gt;

&lt;p&gt;local 영역에서 point의 좌표는 $x_i^{(j)} = x_i^{(j)} - \hat{x}^{(j)}$ ($i=1, …, K, j=1, …, d, \hat{x}$ : centroid의 좌표)을 이용해  centriod과의 상대적인 좌표로 바꿔 사용한다. 이 상대적인 좌표를 이용해 local 영역에 있는 point간의 위치적 관계를 담을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;33-robust-feature-learning-under-non-uniform-sampling-density&quot;&gt;3.3 Robust Feature Learning under Non-Uniform Sampling Density&lt;/h3&gt;
&lt;p&gt;Point set은 대부분 일정하지 않은 밀도로 분포되어 있다. density한 밀도 분포를 가진 point cloud로 학습된 네트워크는 sparse한 point cloud로 generalize 하기 어렵고 그 반대도 마찬가지이다. 때문에 dense하게 얻어진 영역에서 확대하며 작은 디테일을 확인하고 싶지만, sparse한 영역에서 이를 수행하면 제대로 된 local pattern을 얻을 수 없다. 이러한 경우 더 큰 scale에서 pattern을 확인해야 하는데 이를 위해 input sampling density가 달라질 때, 서로 다른 scale의 영역에서 얻어진 featture를 결합하는 방법을 학습하는 density adaptive PointNet layer를 제안하였다. 이 PointNet layer를 이용한 hierarchical network를 PointNet++이라고 이름 지었다.&lt;/p&gt;

&lt;p&gt;3.2절에서 각각의 abstraction level은 하나의 scale에서 feature extraction과 grouping을 수행하였다. PointNet++은 각 abstraction level은 다양한 scale의 local pattern을 추출하고 이를 local point 밀도에 따라 지능적으로 결합한다. local 영역을 그룹화하고 서로 다른 scale에서 얻어진 feature를 결합하기 위해 다음 2가지의 density adaptive layer를 제안하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/135259615-d7e6b0f9-8f02-4970-8752-735d3149a691.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;multi-scale-grouping-msg&quot;&gt;Multi-scale grouping (MSG)&lt;/h4&gt;
&lt;p&gt;그림에서 보이는 것처럼 가장 간단하면서도 효과적으로 다양한 scale의 pattern을 찾아내는 방법은 다른 scale의 grouping layer를 적용한 뒤 PointNet을 이용해 각각 다른 scale에서 feature를 추출하는 것이다. 이 다른 scale에서 추출한 feature들을 concatenate하여 multi-scale feature를 생성한다.&lt;/p&gt;

&lt;p&gt;저자는 network를 훈련시키기 위해 &lt;em&gt;random input dropout&lt;/em&gt;을 수행했다. &lt;em&gt;random input dropout&lt;/em&gt;이란 각 instance를 랜덤한 확률로 input point들 중 일부를 랜덤하게 dropping out 하는 것으로 random하게 downsampling을 수행한다고 생각할 수 있다. 이 방법을 이용해 다양한 sparsity와 uniformity를 가지는 training set으로 네트워크를 훈련시킬 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;multi-resolution-grouping-mrg&quot;&gt;Multi-resolution grouping (MRG)&lt;/h4&gt;
&lt;p&gt;위에서 설명한 MSG는 모든 centroid point에 대해 넓은 scale neighborhoods에서 local PointNet을 실행하기 때문에 계산 비용이 많이 든다. 특히 가장 낮은 level에서는 centroid point의 수가 매우 많기 때문에 time cost가 매우 중요하다.&lt;/p&gt;

&lt;p&gt;이러한 이유로 MRG라는 새로운 대안을 제시한다. MRG 이미지에서 level $L_i$의 영역에서의 두 vector를 concatenation하여 feature vector를 만든다. 한 vector(왼쪽 vector)는 한 단계 낮은 level $L_{i-1}$에서 set abstraction level을 이용하여 각 subregion의 feature를 요약해서 얻는다. 다른 한 vector(오른쪽)는 local 영역의 모든 raw point들을 단일 PointNet을 사용하여 직접 처리하여 얻은 feature이다.&lt;/p&gt;

&lt;p&gt;만약 local 영역의 density가 낮다면, 첫 번째 vector를 계산하는 subregion은 더 spaerse한 point들을 포함하고 sampling 부족 문제를 더 많이 겪기 때문에 첫 번째 vector의 신뢰도가 두 번째 vector에 비해 더 떨어진다. 반대로 local 영역의 density가 높다면, 첫 번째 vector가  더 낮은 level에서 높은 해상도로 recursive하게 검사할 수 있는 능력이 있기 때문에 더 작고 디테일한 정보를 제공해줄 수 있다.&lt;/p&gt;

&lt;p&gt;MSG와 비교했을 때, 이 방법이 더 효율적이다.&lt;/p&gt;

&lt;h3 id=&quot;34-point-feature-propagation-for-set-segmentation&quot;&gt;3.4 Point Feature Propagation for Set Segmentation&lt;/h3&gt;
&lt;p&gt;set abstraction layer에서 기존의 point set은 subsampling 된다. 그러나 semantic point labeling과 같은 segmentation task에서 기존에 있던 모든 point에 대한 point feature를 얻어야 한다.&lt;/p&gt;

&lt;p&gt;이를 위해 거리 기반 interpolation과 level을 통과하는 skip link를 사용하는 계층적 전파(hierarchical propagation) 전략을 사용한다. &lt;em&gt;feature propagation&lt;/em&gt; level에서, $N_l \times (d+C)$ point에서 $N_{l-1}$로 point feature를 propagation 한다($N_l, N_{i-1}$은 set abstraction level $l$에서의 input과 output point set 크기이며 $N_l\leq N_{i-1}$이다).&lt;/p&gt;

&lt;p&gt;interpolation : $N_{l-1}$ 개의 point의 좌표에서 $N_l$ point들의 feature value $f$를 interpolation하여 feature propagation을 수행한다. interpolation 방법으로는 $k$ nearest neighbors 방법 기반의 inverse distance weighted average를 사용한다(문단 이후 수식). $N_{l-1}$ point들에서 얻어진 interpolated feature들은 set abstraction level에서 온 skip linked point feature들과 concatenate된다. 이후 concatenate된 feature들은 “unit pointnet”을 통과하는데, 이는 CNN에서 $1 \times 1$ 컨볼루션과 유사하다. 이 과정은 기존 point set에 feature를 전파할 때까지 반복된다.&lt;/p&gt;

\[f^{(j)}(x)=\frac{\sum_{i=1}^{k}w_{i}(x)f_{i}^{(j)}}{\sum_{i=1}^{k}w_{i}(x)}\quad \textup{where}\quad w_{i}(x)=\frac{1}{d(x,x_{i})^p},\;j=1, ...,C\]

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&quot;footnote_1&quot;&gt;1&lt;/a&gt;: 세분화된 것을 의미하는 말로 여기서는 작은 패턴들을 의미한다. 반대말로 coarse-grained가 있다.&lt;br /&gt;
&lt;a name=&quot;footnote_2&quot;&gt;2&lt;/a&gt;: 서로 간의 거리가 가장 먼 점들을 선택하는 과정&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">PointNet++은 PointNet이 point가 존재하는 metric space에 의해 유도된 local structure를 활용하지 못해서 fine-grained1 패턴들을 잘 인식하지 못하고 복잡한 scene들을 일반화하는 능력이 떨어지는 문제를 해결하기 위해 제안되었다. PointNet++은 입력 point set의 nested partitioning에 PointNet을 recursive하게 적용하는 hierarchical 신경망이다. 이 네트워크는 metric space distance를 활용하여 contextual scale이 증가하는 local feature를 학습할 수 있다.</summary></entry><entry><title type="html">머신러닝 시스템의 종류</title><link href="https://shvtr159.github.io/ml/ml/" rel="alternate" type="text/html" title="머신러닝 시스템의 종류" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>https://shvtr159.github.io/ml/ml</id><content type="html" xml:base="https://shvtr159.github.io/ml/ml/">&lt;p&gt;핸즈온 머신러닝의 머신러닝 내용 일부를 정리.&lt;/p&gt;
&lt;h2 id=&quot;머신러닝-시스템의-종류&quot;&gt;머신러닝 시스템의 종류&lt;/h2&gt;
&lt;p&gt;머신러닝 시스템은 다음과 같이 분류할 수 있지만 서로 배타적이지 않고 원하는 대로 연결 가능하다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;훈련 지도 여부 : 지도 학습, 비지도 학습, 준지도 학습, 강화 학습&lt;/li&gt;
  &lt;li&gt;실시간 훈련 여부 : 온라인 학습, 배치 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;훈련-지도-여부&quot;&gt;훈련 지도 여부&lt;/h3&gt;
&lt;h4 id=&quot;지도-학습-supervised-learning&quot;&gt;지도 학습 (supervised learning)&lt;/h4&gt;
&lt;p&gt;지도학습에는 훈련 데이터에 label(혹은 target이란 표현도 사용됨) 이라는 답이 포함된다. 지도학습에는 분류(classification), 회귀(regression)이 해당된다. 일부 회귀 알고리즘은 분류에 사용할 수도 있고, 반대의 경우도 있다. 그 예로 로지스틱 회귀는 class에 속할 확률을 출력한다.&lt;/p&gt;

&lt;p&gt;중요한 지도 학습 알고리즘들&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;k-최근접 이웃 (K-Nearest Neightbors)&lt;/li&gt;
  &lt;li&gt;선형 회귀 (Linear Regression)&lt;/li&gt;
  &lt;li&gt;로지스틱 회귀 (Logistic Regrassion)&lt;/li&gt;
  &lt;li&gt;서포트 벡터 머신(Supprot Vector Machines, SVM)&lt;/li&gt;
  &lt;li&gt;결정 트리(Decision Tree)와 랜덤 포레스트(Random Forests)&lt;/li&gt;
  &lt;li&gt;신경망 (Neural networks)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;비지도-학습-unsupervised-learning&quot;&gt;비지도 학습 (unsupervised learning)&lt;/h4&gt;
&lt;p&gt;lable 없는 훈련 데이터를 이용해 시스템 스스로 학습.&lt;/p&gt;

&lt;p&gt;대표적 비지도 학습&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;군집 (clustering)&lt;/li&gt;
  &lt;li&gt;시각화 (visualization)와 차원 축소 (dimensionality reduction)&lt;/li&gt;
  &lt;li&gt;이상치 탐지 (anomaly detection)&lt;/li&gt;
  &lt;li&gt;연관 규칙 학습 (Association rule learning)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;준지도-학습-semisupervised-learning&quot;&gt;준지도 학습 (semisupervised learning)&lt;/h4&gt;
&lt;p&gt;레이블이 일부만 있는 데이터. 대부분 지도 학습과 비지도 학습의 조합으로 이루어져 있다.&lt;/p&gt;

&lt;h4 id=&quot;강화-학습-reinforcement-learning&quot;&gt;강화 학습 (reinforcement learning)&lt;/h4&gt;
&lt;p&gt;에이전트(학습 시스템)이 환경을 관찰해서 행동을 실행하고 그 결과로 보상 또는 벌점을 받는다. 시간이 지나면서 가장 큰 보상을 얻기 위해 정책(policy)라고 부르는 최상의 전략을 스스로 학습한다.&lt;/p&gt;

&lt;h3 id=&quot;실시간-훈련-여부&quot;&gt;실시간 훈련 여부&lt;/h3&gt;
&lt;h4 id=&quot;배치-학습-batch-learning&quot;&gt;배치 학습 (batch learning)&lt;/h4&gt;
&lt;p&gt;시스템이 가용한 데이터를 모두 사용해 훈련한다. 즉 먼저 시스템을 훈련시키고 더 이상의 학습 없이 제품 시스템에 적용한다. 오프라인 학습(offline learning)이라고 한다. 이는 컴퓨팅 자원이 많이 필요하다는 점을 고려해야 한다.&lt;/p&gt;

&lt;h4 id=&quot;온라인-학습-online-learning&quot;&gt;온라인 학습 (online learning)&lt;/h4&gt;
&lt;p&gt;데이터를 순차적으로 한 개씩 또는 미니배치(mini-batch) 단위로 훈련한다. 매 학습 단계가 빠르고 비용이 적게 들어 데이터가 도착하는 즉시 학습할 수 있다. 그러나 나쁜 데이터가 주입되었을 때 시스템 성능이 점진적으로 감소하기 때문에 모니터링이 필요하다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="ML" /><summary type="html">핸즈온 머신러닝의 머신러닝 내용 일부를 정리. 머신러닝 시스템의 종류 머신러닝 시스템은 다음과 같이 분류할 수 있지만 서로 배타적이지 않고 원하는 대로 연결 가능하다. 훈련 지도 여부 : 지도 학습, 비지도 학습, 준지도 학습, 강화 학습 실시간 훈련 여부 : 온라인 학습, 배치 학습</summary></entry><entry><title type="html">[논문 리뷰] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation/" rel="alternate" type="text/html" title="[논문 리뷰] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation" /><published>2021-09-13T00:00:00+09:00</published><updated>2021-09-13T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;기존에는 point cloud의 irregular한 형식때문에  regular한 3D voxel grid 또는 collection of images로 바꿔 사용하곤 했지만 이것은 불필요하게 많은 data가 렌더링 되는 문제가 있다. 그래서 여기서는 point cloud를 바로 사용하며 input point들의 permutation invariance&lt;sup&gt;&lt;a href=&quot;#footnote_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;를 잘 고려하는 neural network를 설계하였다. PointNet은 object detection, part segmentation, scene semantic parsing에 이르는 application들을 위한 통일된 구조를 제공한다. 또한, 이론적으로, network가 무엇을 학습했는지와 network가 왜 perturbation(noise와 유사)과 corruption에 robust한지 이해하기 위한 분석을 제공한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;기존에는 point cloud의 irregular한 형식 때문에 대부분이 deep net architecture에 넣기 전  regular한 3D voxel grid 또는 collection of images (e.g, views)로 변형하곤 했다. 그러나 이것은 불필요하게 많은 data가 렌더링 되는 문제가 있다. 그래서 point cloud를 직접 사용하도록 하였다. 이를 위해서는 permutation invariance 한 성질로 인해 net computation에서 특정 대칭화가 필요하고, rigid motion&lt;sup&gt;&lt;a href=&quot;#footnote_2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;에 invariance하다는 사실을 고려해야 한다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;2. Related work&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Point Cloud Fetures&lt;/strong&gt; 기존 point cloud feature는  대부분 특정 task를 위해 handcrafted 되었다. 이는 일반적으로 intrinsic 또는 extrinsic하게 분류되는 특정 transformation에 대해 invariant하게 설계된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning on 3D Data&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Volumetric CNNs&lt;/em&gt; : voxelized shape에 적용되는 network이지만 data sparsity과 계산 비용때문에 resolution이  제한된다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Multiview CNNs&lt;/em&gt; : 3D point cloud나 shape를 2D 이미지로 렌더링한 다음 2D conv net를 이용해 분류. 그러나 scene understanding이나 3D task(point classification, shape completion 등)로 확장하는 것이 어렵다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Spectral CNNs&lt;/em&gt; : mesh에 적용하는 방법. 그러나 유기물과 같은 manifold mesh에 제약을 받고 가구와 같은 non-isometric 모양으로 어떻게 확장하는지 명확하지 않다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Feature-based DNNs&lt;/em&gt; : traditional shape features를 추출하여 3D data를 vector로 변환한 뒤 FC net를 이용해 shape를 분류한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning on Unorderes Sets&lt;/strong&gt; 데이터 구조 관점에서, point cloud는 순서 없는 vector의 집합이다. 딥러닝에서 대부분의 작업은 sequence(음성 및 언어 처리), images와 volume(video 또는 3D data)와 같은 regular한  input에 초점을 맞추고 있지만, point set에 대해서는 잘 이루어지지 않았다.&lt;/p&gt;

&lt;h2 id=&quot;3-problem-statement&quot;&gt;3. Problem Statement&lt;/h2&gt;
&lt;p&gt;point cloud는 각 point $P_{i}$가 $(x, y, z)$좌표에 color, normal과 같은 추가적인 feature channels를 가지고, 이 point들이 모인 set을 $\lbrace P_i \mid i=1,…,n \rbrace$로 표현한다. 여기서는 point의 channel로 $(x,y,z)$ 좌표만을 사용한다.&lt;/p&gt;

&lt;p&gt;Object classification task에서, input point cloud는 shape에서 직접 sampling되거나, 장면 point cloud에서 사전에 segmentation된다. PointNet에서는 모든 $k$ 후보 classes에 대해 $k$개의 scores를 출력한다.&lt;/p&gt;

&lt;p&gt;Semantic segmentation에서는 input은 부분 region segmentation을 위한 단일 object이거나 object region segmentation 3D scene의 sub-volume일 수 있다. PointNet에서는 각 $n$개의 point 및 $m$개의 semantic subcategory에 대해 $n\times m$개의 score를 출력한다.&lt;/p&gt;

&lt;h2 id=&quot;4-deep-learning-on-point-sets&quot;&gt;4. Deep Learning on Point Sets&lt;/h2&gt;
&lt;h3 id=&quot;41-properties-of-point-sets-in-mathbb-rn&quot;&gt;4.1. Properties of Point Sets in $\mathbb R^n$&lt;/h3&gt;
&lt;p&gt;Network의 input은 Euclidean space 상 point의 subset이다. 이는 다음 3가지 특징을 가지고 있고 이 특징들을 해결하기 위해 4.2에서 3가지의 key module을 사용하였다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unordered.&lt;/strong&gt; 이미지의 pixel 배열이나 voxel 배열과 달리 point cloud는 특정한 순서가 없다. 다시 말해 네트워크가 N개의 3D point를 사용한다면 이 N개 point의 순서에 따라 결과가 달라지지 않아야 한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interaction among points.&lt;/strong&gt; point들은 distance metric&lt;sup&gt;&lt;a href=&quot;#footnote_3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;이 존재하는 공간에 있다. 즉, 이는 point들이 고립되어있지 않고 주변 point들과 의미 있는 subset을 형상한다는 것을 의미한다. 따라서, model은 주변 point에서 local 구조 및 local 구조 간의 combinatorial interaction을 확인할 수 있어야 한다. 간단히 이야기 하면 point들간의 거리 정보만을 통해 의미를 찾아야 한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Invariance under transformations.&lt;/strong&gt;  geometric object로서 point cloud의 학습된 표현은 특정 transformation을 수행해도 불변해야 한다. 예를 들어 모든 포인트들을 회전하거나 이동시켜도 그 특성을 변하지 않아야 한다는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-pointnet-architecture&quot;&gt;4.2. PointNet Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133418360-97952ac2-feeb-4b00-8ead-d466066648ea.png&quot; alt=&quot;image&quot; /&gt;&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Figure 2. PointNet의 구조&lt;/span&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;classification network는 $n$개의 point를 input으로 받으며, 여기서는 $(x, y, z)$ 좌표만을 고려하여 $n\times 3$의 vector를 input으로 받는다. 이 input을 transformation 한 뒤  다시 feature tranformation 한 다음 max pooling을 통해 point feature를 aggregate한다. 최종 output은 $k$개의 class에 대한 각 classification score이다. 사진의 구조는 classification network이지만 classification network와 segmentation network는 서로 구조의 많은 부분을 공유한다. Batchnorm은 ReLU와 함께 모든 layer에 적용하고, dropout은 classification net의 마지막 mlp에만 적용한다.&lt;/p&gt;

&lt;p&gt;자세한 내용으로 PointNet은 다음 3가지 key module을 가진다.&lt;/p&gt;
&lt;h4 id=&quot;symmetry-function-for-unordered-input&quot;&gt;Symmetry Function for Unordered Input&lt;/h4&gt;
&lt;p&gt;input 순서에 invariant한 model을 만들기 위한 전략으로는 다음 3가지가 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;입력 순서를 canonical order로 정렬한다.(여기서 canonical order는 표준 형식을 따르는 정렬으로 특정 알고리즘을 지칭하지는 않는다)&lt;/li&gt;
  &lt;li&gt;input을 RNN을 학습시키기 위한 sequence로 취급하지만, 모든 종류의 순열을 이용해 training data를 augmentation 한다.&lt;/li&gt;
  &lt;li&gt;각 point에서의 정보를 aggregate&lt;sup&gt;&lt;a href=&quot;#footnote_4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;하기 위해 간단한 symmetric function을 사용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PointNet은 symmetric function을 사용한다. symmetric function은 입력 순서에 관계 없이 동일한 output을 내는 함수로 여기서는 n개의 vector를 입력으로 받는다. symmetric function의 예로는 +와 $\times$연산이 있다.&lt;/p&gt;

&lt;p&gt;단순히 sorting하는것이 더 간단해 보이지만 일반적으로 고차원 공간에서는 stable한 순서가 존재하지 않는다. 때문에 sorting으로는 ordering 문제가 완전히 해결되지 않고, 이 문제가 지속되기 때문에 network가 input에서 output으로 일관된 mapping을 학습하기 어렵다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/134469465-907b04b3-8421-43d5-bc75-785aa1552912.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;순서 invariance를 달성하기 위한 3가지 접근 방법&lt;/span&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;위 이미지에서 볼 수 있듯이 point를 정렬하지 않고 바로 처리하는 것보다 정렬한 뒤 MLP에 직접 적용하는 것이 좀 더 낫지만 여전히 성능은 좋지 않다. 또, RNN을 이용한 방법 또한 제안한 방법만큼 좋은 성능을 보여주지 못했다.&lt;/p&gt;

&lt;p&gt;그래서 PointNet은 다음과 같이 집합의 transform된 element들에 대해 symmetric function을 적용하여 생성된 point set에 정의된 general 함수를 근사화하는 것이다.&lt;/p&gt;

\[f(\{x_1,...,x_n\})\approx g(h(x_1),...,h(x_n))\]

&lt;center&gt;$f\;:2^{\mathbb R^N}\rightarrow \mathbb R,\;h\;:{\mathbb R^N}\rightarrow {\mathbb R^K},\;g$ (symmetric 함수) $:\underbrace{ {\mathbb R^K} \times \cdots \times {\mathbb R^K}}_ n \rightarrow {\mathbb R^N}$ &lt;/center&gt;

&lt;p&gt;Basic 모듈은 매우 간단하다. $h$를 다층 퍼셉트론 network를 이용해서, $g$는 single 변수 함수와 max pooling 함수의 구성으로 approximate한다. $h$의 collection을 통해 우리는 집합의 다른 property들을 알기위해 많은 $f$를 학습할 수 있다.&lt;/p&gt;

&lt;p&gt;이 모듈의 output은 input set의 global signature인 $[f_1,\,…\,,f_K]$ 벡터이다.&lt;/p&gt;

&lt;h4 id=&quot;local-and-global-information-aggregation&quot;&gt;Local and Global Information Aggregation&lt;/h4&gt;
&lt;p&gt;classification을 위한 shape global feature에 대해 SVM이나 다층 퍼셉트론 classifier는 쉽게 훈련시킬 수 있지만, point segmentation은 local과 global을 모두 필요로 하기 때문에 어려움이 있다. 이를 해결하기 위한 solution이 Fig 2에서 볼 수 있는 segmentation Network이다.&lt;/p&gt;

&lt;p&gt;이 network는 global point cloud의 feature 벡터를 계산하고 이를 각 point feature에 concatenate 한 뒤 각 point 별 feature에 다시 제공한다(Fig 1의 nx64에 global feature 1024를 모두 추가해준다). 그 다음 combine 된 point feature를 기반으로 새로운 각 point 별 feature를 추출한다. 이를 이용하면 각 point 별 feature가 local 및 global 정보를 모두 인식한다.&lt;/p&gt;

&lt;h4 id=&quot;joint-alignment-network&quot;&gt;Joint Alignment Network&lt;/h4&gt;
&lt;p&gt;point cloud가 rigid transformation 같은 특정한 geometric transformation 이 수행되는 경우 point cloud의 semantic labeling은 변하지 않아야 한다. 때문에 point set에 의해 학습된 것이 이러한 변환에 불변하기를 기대한다.&lt;/p&gt;

&lt;p&gt;저자는 spatial transformer networks에서 motivate된  mini-network(Fig 2 T-net)를 추가하였다. 이 network는 affine 변환 matrix를 예측하고 이를 input points의 coordiates에 바로 적용하여 간단히 해결하였다. mini-network 자체는 큰 network와 비슷하고, point independent feature extraction과 maxpooling, FC layer의 기본 모듈로 구성된다.&lt;/p&gt;

&lt;p&gt;그러나 featrue space의 transformation matrix는 spatial transform matrix보다 더 높은 차원으로 이루어져서 optimization의 난이도가 급격히 상승한다. 이를 위해 regularization term을 softmax training loss로 추가한다. 또, feature transformation matrix가 다음과 같은 직교 행렬에 가깝도록 제한한다.&lt;/p&gt;

\[L_{reg} = \left \| I-AA^T \right \|^2_F\]

&lt;p&gt;$A$는 mini-network에 의해 예측된 feature alignment matrix이다. 직교 변환은 input의 정보를 잃지 않기 때문에 필요하다.&lt;/p&gt;

&lt;h3 id=&quot;43-theoretical-analysis&quot;&gt;4.3. Theoretical Analysis&lt;/h3&gt;
&lt;h4 id=&quot;universal-approximation6&quot;&gt;Universal approximation&lt;sup&gt;&lt;a href=&quot;#footnote_6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&quot;footnote_1&quot;&gt;1&lt;/a&gt;: 입력 벡터 요소의 순서와 상관 없이 같은 출력을 생성하는 것. MLP가 이에 해당하며 CNN, RNN은 이에 해당하지 않는다.&lt;br /&gt;
&lt;a name=&quot;footnote_2&quot;&gt;2&lt;/a&gt;: rigid motion은 transformation을 하더라도 point들간의 distance와 방향은 그대로 유지되는 변환을 말한다. 여기에는 translation, rotation, reflection, glide reflection이 해당된다.&lt;br /&gt;
&lt;a name=&quot;footnote_3&quot;&gt;3&lt;/a&gt;: distance를 정의하는 방법. 가장 간단한 예시로 Euclidean distance를 들 수 있다.&lt;br /&gt;
&lt;a name=&quot;footnote_4&quot;&gt;4&lt;/a&gt;: ????&lt;br /&gt;
&lt;a name=&quot;footnote_5&quot;&gt;5&lt;/a&gt;:  M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In NIPS 2015&lt;br /&gt;
&lt;a name=&quot;footnote_6&quot;&gt;6&lt;/a&gt;: 1개의 히든 레이어를 가진 Neural Network를 이용해 어떠한 함수든 근사시킬 수 있다는 이론. 당연히 활성화 함수는 비선형 함수여야 한다&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">Abstract 기존에는 point cloud의 irregular한 형식때문에 regular한 3D voxel grid 또는 collection of images로 바꿔 사용하곤 했지만 이것은 불필요하게 많은 data가 렌더링 되는 문제가 있다. 그래서 여기서는 point cloud를 바로 사용하며 input point들의 permutation invariance1를 잘 고려하는 neural network를 설계하였다. PointNet은 object detection, part segmentation, scene semantic parsing에 이르는 application들을 위한 통일된 구조를 제공한다. 또한, 이론적으로, network가 무엇을 학습했는지와 network가 왜 perturbation(noise와 유사)과 corruption에 robust한지 이해하기 위한 분석을 제공한다.</summary></entry><entry><title type="html">PU-GAN: A Point Cloud Upsampling Adversarial Network</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-gan/" rel="alternate" type="text/html" title="PU-GAN: A Point Cloud Upsampling Adversarial Network" /><published>2021-09-10T00:00:00+09:00</published><updated>2021-09-10T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-gan</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-gan/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;PU-Net, EC-Net, MPU 등의 network들은 학습을 통해 point cloud upsampling의 장점을 입증했다. 그러나 이 네트워크들은 sparse하고 non-uniform한 low-quality inputs로부터 좋은 결과를 얻지 못할 수 있다. 그래서 upsampling과 data amendment 능력을 결합한 PU-GAN을 제안한다. 주된 기여는 latent space에서 다양한 point 분포를 생성하도록 generator를 학습시키고, 이 point set에 대해 implicit하게 평하가는 데 도움이 되는 discriminator network이다. 그러나 generator와 discriminator간의 균형을 맞추고 낮은 수렴 경향을 피하기 어려운 문제가 있다. Thus, we first improve the point generation quality, or equivalently the feature expansion capability, of the generator, by constructing an up-down-up unit to expand point features by upsampling also their differences for self-correction.  또, feature integration quality를 향상시키기 위해 self-attention unit을 사용한다. 마지막으로, 결과의 distribution uniformity를 향상시키고 discriminator가 target distribution에서 더 많은 latent pattern을 학습하도록 하기 위해 aadversarial, uniform, reconstruction term을 사용해 end-to-end 로 network를 학습시키는 compound loss를 추가로 사용한다.&lt;/p&gt;
&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;31-overview&quot;&gt;3.1 Overview&lt;/h3&gt;
&lt;p&gt;point 수가 $N$개인 순서 없는 sparse point set $\mathcal{P}={  p_{i}  }_ {i=1}^{N}$가 주어지면, $rN$개의 point를 가지는 dense point set  $\mathcal{Q}= {  q_{i} }_ {i=1}^{rN}$을 생성하는 것을 목표로 한다. 이때, $r$은 upsampling rate이다. $\mathcal{Q}$가 반드시 $\mathcal{P}$의 superset(상위집합)은 아니지만, 출력 $\mathcal{Q}$가 다음 2가지 조건을 만족하기를 원한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mathcal{Q}$는 $\mathcal{P}$와 latent target object의 같은 underlying geometry를 설명해야 하고, 때문에 $\mathcal{Q}$의 point들은 target object의 표면에 있어야 한다.&lt;/li&gt;
  &lt;li&gt;sparse하고 non-uniform한 input $\mathcal{P}$에 대해서도 $\mathcal{Q}$의 point들은 target object 표면에 uniformly-distribute하게 분포되어야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133037097-4dc912bb-f9d2-4a0f-83a0-40b3776db7f4.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 PU-GAN의 전체적인 네트워크 구조를 보면, generator는 sparse input $\mathcal{P}$로부터 dense output $\mathcal{Q}$를 생성하고, discriminator는 fake로 생성된 것을 찾는것을 목표로 한다.&lt;/p&gt;

&lt;h3 id=&quot;32-network-architecture&quot;&gt;3.2 Network Architecture&lt;/h3&gt;
&lt;h4 id=&quot;321--generator&quot;&gt;3.2.1  Generator&lt;/h4&gt;
&lt;p&gt;Figure 2에서 보이듯이, generator는 3개의 component를 가지고있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The feature extraction component&lt;/strong&gt; $N\times d$ ($d$ : Input point 속성의 차원 수. $i.e.$, 좌표, 색상, normal 등) 의 input $\mathcal{P}$로부터 feature $\mathbf{F}$를 추출하기 위해 가장 간단한 3D coordinates인것만을 고려하여 $d=3$의 case에 focusing하고, [Patch-based progressive 3D point set upsampling, CVPR2019]의 feature extraction 방법을 채택하여 서로 다른  layer들에 걸쳐 feature들을 통합하기 위해 dense connection이 도입됐다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The feature expansion component&lt;/strong&gt; $\mathbf{F}$를 확장하여 확장된 feature $\mathbf{F_{up}}$을 생성한다. 여기서는 generator가 더 다양한 point distribution을 생성할 수 있도록 $\mathbf{F_{up}}$의 feature variation을 강화시키기 위해 &lt;em&gt;up-down-up expansion unit&lt;/em&gt; (Figure2의 위쪽)을 설계한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The point set generation component&lt;/strong&gt; 먼저, multilayer perceptrons (MLPs) 세트를 통해 $\mathbf{F_{up}}$에서 3D coordiante 세트를 regression한다. feature expansion process가 여전히 local하기 때문에 $\mathbf{F_{up}}$의 feature (또는 latent space의 point들)이 input에 가깝다. 때문에 서로 멀리 떨어진 $rN$ points만을 남기기 위해 network에 farthest sampling setp을 포함한다. 이를 위해 $\mathbf{F}$에서 $\mathbf{F_{up}}$으로 확장할 때, $\mathbf{F_{up}}$에 $(r+2)N$ features를 생성한다.이 전략은 global perspective에서 point set 의 distribution uniformity를 강화한다.&lt;/p&gt;

&lt;h4 id=&quot;322-discriminator&quot;&gt;3.2.2 Discriminator&lt;/h4&gt;
&lt;p&gt;Discriminator의 목표는 generator에서 생성된 $rN$ points의 set을 구분하는 것이다. 이를 위해 처음에는 global feature를 추출하기 위해 [PCN: Point completion network, 3DV2018]의 basic network 구조를 이용한다. feature learning을 개선하기 위해  feature concatenation 한 뒤 self-attention unit을 추가한다. 이는 basic MLPs와 비교하여 feature integration을 개선하고 subsequent feature extraction 기능을 개선할 수 있다. 다음으로 global feature를 생성하기 위해 MLPs set와  max pooling를 적용하고 FC layer set를 통해 최종 confidence value를 regression한다. 이 값이 1에 가까울수록 높은 신뢰도의 target distribution에서 온 가능성이 있다고 예측한다.&lt;/p&gt;

&lt;h3 id=&quot;33-up-down-up-expansion-unit&quot;&gt;3.3 UP-down-up Expansion Unit&lt;/h3&gt;

&lt;p&gt;기존 PU-Net, EC-Net, MPU 등의 network들은 학습을 통해 point cloud upsampling의 장점을 입증했다. 그러나 이 네트워크들은 sparse하고 non-uniform한 low-quality inputs로부터 좋은 결과를 얻지 못할 수 있다.
그래서 기존의 point cloud를 deep learning을 사용해서 처리하는 방식과 GAN 기반의 3D shape processing 방식을 결합하여 upsampling을 수행하도록 하였다. 기존 GAN을 이용한 processing은 3D volume 방식이었고 point cloud를 사용한 것은 reconstruction에 가까워 upsampling은 최소로 수행하였다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">Introduction PU-Net, EC-Net, MPU 등의 network들은 학습을 통해 point cloud upsampling의 장점을 입증했다. 그러나 이 네트워크들은 sparse하고 non-uniform한 low-quality inputs로부터 좋은 결과를 얻지 못할 수 있다. 그래서 upsampling과 data amendment 능력을 결합한 PU-GAN을 제안한다. 주된 기여는 latent space에서 다양한 point 분포를 생성하도록 generator를 학습시키고, 이 point set에 대해 implicit하게 평하가는 데 도움이 되는 discriminator network이다. 그러나 generator와 discriminator간의 균형을 맞추고 낮은 수렴 경향을 피하기 어려운 문제가 있다. Thus, we first improve the point generation quality, or equivalently the feature expansion capability, of the generator, by constructing an up-down-up unit to expand point features by upsampling also their differences for self-correction. 또, feature integration quality를 향상시키기 위해 self-attention unit을 사용한다. 마지막으로, 결과의 distribution uniformity를 향상시키고 discriminator가 target distribution에서 더 많은 latent pattern을 학습하도록 하기 위해 aadversarial, uniform, reconstruction term을 사용해 end-to-end 로 network를 학습시키는 compound loss를 추가로 사용한다. Method 3.1 Overview point 수가 $N$개인 순서 없는 sparse point set $\mathcal{P}={ p_{i} }_ {i=1}^{N}$가 주어지면, $rN$개의 point를 가지는 dense point set $\mathcal{Q}= { q_{i} }_ {i=1}^{rN}$을 생성하는 것을 목표로 한다. 이때, $r$은 upsampling rate이다. $\mathcal{Q}$가 반드시 $\mathcal{P}$의 superset(상위집합)은 아니지만, 출력 $\mathcal{Q}$가 다음 2가지 조건을 만족하기를 원한다. $\mathcal{Q}$는 $\mathcal{P}$와 latent target object의 같은 underlying geometry를 설명해야 하고, 때문에 $\mathcal{Q}$의 point들은 target object의 표면에 있어야 한다. sparse하고 non-uniform한 input $\mathcal{P}$에 대해서도 $\mathcal{Q}$의 point들은 target object 표면에 uniformly-distribute하게 분포되어야 한다.</summary></entry><entry><title type="html">PU-Net: Point Cloud Upsampling Network</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-net-review/" rel="alternate" type="text/html" title="PU-Net: Point Cloud Upsampling Network" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-net-review</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-net-review/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Key point : point당 multi-level feature들을 학습하고 multi-branch convolution unit을 통해 feature space에 implicit하게 point set을 확장한다. 이후 확장된 feature는 multitude of feature들로 분할한 뒤 upsampling된 point set로 재구성한다. 이 Network는 patch-level에서 적용되고, upsampling된 point들이 surface에 균일한 분포로 underlying할 수 있도록 하는 joint loss function을 가진다. some baseline 방법들과 optimization-based 방법들과 성능을 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Recently, pioneering works [29, 30, 18] began to explore the possibility of reasoning point clouds by means of deep networks for understanding geometry and recognizing 3D structures. In these works, the deep networks directly extract features from the raw 3D point coordinates without using traditional features, e.g., normal and curvature.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;최근, pioneering work들이 geometry를 이해하고, 3D structure를 recognize하기 위한 deep 네트워크를 통해 point cloud의 reasoning 가능성을 explore하기 시작했다. 이러한 연구들에서 deep network는 raw 3D point coordinate로부터 normal과 curvature 같은 traditional feature들 없이 바로 feature들을 추출하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;First, unlike the image space, which is represented by a regular grid, point clouds do not have any spatial order and regular structure. Second, the generated points should describe the underlying geometry of a latent target object, meaning that they should roughly lie on the target object surface. Third, the generated points should be informative and should not clutter together. Having said that, the generated output point set should be more uniform on the target object surface. Thus, simple interpolation between input points cannot produce satisfactory results.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Image S.R과 달라야하는 이유 : 1. Regular grid로 표현되는 image와 달린 point cloud는 spatial 순서와 regular한 구조를 전혀 가지지 않는다. 2. 생성된 point cloud는 잠재적인 대상 object의 underlying geometry를 설명해야만 하며, 이는 대상 object의 표면에 rough하게 위치해야 한다는 것을 의미한다. 3. 생성된 point들은 정보를 제공해야 하고 서로 clutter?? 해서는 안 된다. 따라서, 생성된 output point set은 target object 표면에 균일하게 있어야 하므로 단순히 input point들을 interpolation 하는 것은 만족스러운 결과를 얻을 수 없다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Our network, namely PU-Net, learns geometry semantics of point-based patches from 3D models, and applies the learned knowledge to upsample a given point cloud. It should be noted that unlike previous network-based methods designed for 3D point sets [29, 30, 18], the number of input and output points in our network are not the same.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PU-Net은 3D model로부터 point 기반 patch의 geometry적 semantics를 학습하고 이 지식을 주어진 point cloud를 upsampling하는데 적용한다. 이전 network-based 방법(pioneering works)과 달리, PU-Net의 input과 output point의 수는 같지 않다는 점을 유의해야 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;We formulate two metrics, distribution uniformity and distance deviation from underlying surfaces, to quantitatively evaluate the upsampled point set, and test our method using variety of synthetic and real-scanned data.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Upsampling된 point set을 정량적으로 평가하고 다양한 실제, 합성 data를 이용해 방법을 test하기 위해 Distribution uniformity와 underlying surfaces로부터의 거리 편차라는 두 개의 metrics를 formulate했다.&lt;/p&gt;

&lt;h3 id=&quot;related-work&quot;&gt;Related work&lt;/h3&gt;
&lt;h4 id=&quot;optimization-based-methods&quot;&gt;optimization-based methods&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Alexa et al.&lt;/strong&gt; : local 탄젠트 space의 Voronoi 다이어그램의 vertex 들을 interpolation 한 point를 이용해 point set을 upsampleing한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lipman et al.&lt;/strong&gt; : L1 median에 기초한 point resampling과 surface reconstruction을 위한 Locally Optimal Projection(LOP)을 제시.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Huang et al.&lt;/strong&gt; : point set density 문제를 해결하기 위한 개선된 LOP 제시.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;좋은 결과를 보여주었지만, underlying surface가 smooth하다는 강한 가정을 하였기 때문에 방법의 범위가 제한된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wu et al.&lt;/strong&gt; : 일관성 있는 하나의 step으로 consolidation(통합)과 completion을 합치는 deep points 표현 방법을 제안한다. 그러나 global smoothness를 강제하지 않아 큰 noise에 취약하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 방법들은 data-driven 방법이 아니므로 priors(머신러닝이 아닌 방법. 이미 주어진 지식을 바탕으로 알고리즘 이용 등)에 크게 의존한다.&lt;/p&gt;

&lt;h4 id=&quot;deep-learning-based-methods&quot;&gt;deep-learning-based methods&lt;/h4&gt;
&lt;p&gt;대부분 기존의 work들은 point cloud를 volumetric grid도, geometric graph와 같은 다른 3D 표현으로 변환하여 처리한다. Qi et al. 은 처음으로 point cloud classification, segmentation을 위한 딥러닝 network를 제안했다. 다양한 방법이 있지만, To the best of our knowledge, upsampling에 초점을 둔 것은 없었다.&lt;/p&gt;

&lt;h2 id=&quot;2-network-architecture&quot;&gt;2. Network Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/132798267-c60cfc2d-e67d-48b7-8113-cd495d5ea480.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;90%&quot; height=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;The architecture of PU-Net&lt;/span&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Given a 3D point cloud with point coordinates in nonuniform distributions, our network aims to output a denser point cloud that follows the underlying surface of the target object while being uniform in distribution.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;불균일한 분포의 point coordinate를 가진 3D point cloud가 주어진 경우, 분포가 균일하면서도 object의 underlying surface를 따라가는 높은 밀도의 point cloud를 출력하는 것을 목표로 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Our network architecture (see Fig. 1) has four components: patch extraction, point feature embedding, feature expansion, and coordinate reconstruction.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Network Architecture는 4개의 component를 가진다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Patch-Extraction&lt;/strong&gt; : prior 3D model들의 집합으로부터 다양한 scale과 distribution에서 point의 patch를 추출한다. – fig1과 같이 다양한 patch를 추출한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Point Feature Embedding&lt;/strong&gt; : 계층적 feature learning과 multi-level feature aggregation(집계)을 통해 raw 3D coordinate를 feature space로 mapping한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature Expansion&lt;/strong&gt; : feature의 개수를 증가시킨다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Coordinate Reconstruction&lt;/strong&gt; : F.C layer series를 통해 output point cloud의 3D coordinate를 reconstruction한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;21-patch-extraction&quot;&gt;2.1 Patch Extraction&lt;/h3&gt;
&lt;p&gt;Training을 위한 prior information으로 다양한 모양의 3D object들을 수집한다. 기본적으로 network가 upsampling 하기 위해서는 object로부터 local geometry pattern들을 학습해야 한다. 이것이 patch 기반 접근법을 선택한 motive이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;In detail, we randomly select M points on the surface of these objects. From each selected point, we grow a surface patch on the object, such that any point on the patch is within a certain geodesic distance (d) from the selected point over the surface. Then, we use Poisson disk sampling to randomly generate N_hat points on each patch as the referenced ground truth point distribution on the patch. In our upsampling task, both local and global context contribute to a smooth and uniform output. Hence, we set d with varying sizes, so that we can extract patches of points on the prior objects with varying scale and density.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Detail. object의 surface에서 random하게 $M$개의 point를 선택한다. 각각 선택된 point의 object 표면에서의 patch는, patch 내의 모든 점이 선택된 point로부터 certain geodesic distance $(d)$ 내에 있도록 생성한다. 그리고 나서 ground truth의 patch 내 point 분포를 기준으로 하여 Poisson disk sampling을 이용해 각 patch에 $\hat{N}$개의 point를 생성한다. 이 Upsampling task에서 local 및 global context 모두 output이 smooth하고 uniform하도록 기여한다. 우리는 $d$를 다양한 크기로 설정하여 prior object에서 다양한 scale과 density로 point들의 patches를 추출할 수 있도록 하였다.&lt;/p&gt;

&lt;h3 id=&quot;22-point-feature-embedding&quot;&gt;2.2	Point Feature Embedding&lt;/h3&gt;
&lt;p&gt;patch로부터 Local 및 global geometry context를 학습하기 위해 다음 두 feature learning 전략을 사용하였으며, 그 이점들로 서로 보완한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical feature learning.&lt;/strong&gt; PointNet++의 계층적 feature 학습 메커니즘을 네트워크의 가장 frontal한 부분으로 채택한다. 계층적 feature 학습을 채택하기 위해 각 level에서 상대적으로 작은 grouping 반경을 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-level feature aggregation&lt;/strong&gt; network의 lower layer일수록 더 작은 local feature에 대응되고 그 반대도 마찬가지이다. 더 좋은 upsampling 결과를 위해 여기서는 서로 다른 level의 feature들을 optimal 하게 종합해야 한다.  몇몇 기존의 연구들은 cascaded multi-level feature 종합을 위해 skip-connection을 사용하였지만 이러한 top-down propagation 방식은 upsampling 문제에는 효율적이지 않았다. 그래서 여기서는  서로 다른 level의 feature들을 직접 결합하고, network가 각 level의 중요성을 배울 수 있도록 하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Since the input point set on each patch (see point feature embedding in Fig. 1) is subsampled gradually in hierarchical feature extraction, we concatenate the point features from each level by first restoring the features of all original points from the downsampled point features by the interpolation method in PointNet++&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;각 patch에 설정된 input point는 hierarchical feature extraction에서 점진적으로 subsampling 되므로 PointNet++의 interpolation 방법에 의해 downsampling된 모든 original point들의 feature를 처음으로 복원하여 각 level에서 point feature들을 concatenate한다. 특히 level $\ell$에 있는 interpolated된 point $x$의 feature는 다음 식에 의해 계산된다 :&lt;/p&gt;

\[f^{(\ell)}(x)=\frac{\sum_{i=1}^{3}w_{i}(x)f^{(\ell)}(x_{i})}{\sum_{i=1}^{3}w_{i}(x)}\]

&lt;p&gt;$w$는 inverse distance weight인 $w_{i}(x) = 1/d(x,x_{i})$으로 정의되고, $x_{1}, x_{2}, x_{3}$는 level $\ell$에서 $x$에서 가장 가까운 3개의 point이다. 이후 1X1 convolution을 이용해 서로 다른 level의 interpolated feature를 동일한 차원 $C$로 축소한다. 최종적으로 각 level의 feature들을 embedded point feature $f$로 concatenate 한다.&lt;/p&gt;
&lt;h3 id=&quot;23-feature-expansion&quot;&gt;2.3	Feature Expansion&lt;/h3&gt;
&lt;p&gt;Point Feature Embedding 이후 feture space에서 feature의 수를 증가시킨다. &lt;em&gt;point&lt;/em&gt;와 &lt;em&gt;feature&lt;/em&gt;은 서로 &lt;em&gt;interchangeable&lt;/em&gt;하기 때문에  이는 point의 수를 증가시키는 것과 같다. $f$의 차원이 $N\times \tilde{C}$일 때, $N$은 input point의 수이고, $\tilde{C}$는 concatenate된 embedded feature의 feature dimension이다.
feture expansion으로 $rN\times \tilde{C_{2}}$의 차원으로 feature ${f}’$을 출력한다. &lt;br /&gt;
여기서 $r$은 upsampling rate이고, $\tilde{C_{2}}$는 새로운 feature dimension이다.
 이미지와의 다른 특성으로 인해 sub-pixel convolution layer 기반의 효과적인 feature expansion을 제안한다. 이는 다음과 같이 표현된다 :&lt;/p&gt;

\[{f}'=\mathcal{RS}(\;[\,C_{1}^{2}(C_{1}^{1}(f)),\,...\,, C_{r}^{2}(C_{r}^{1}(f))\,]\;)\]

&lt;p&gt;$C_{i}^{1}(\cdot), C_{i}^{2}(\cdot)$는 두 set의 분리된 1X1 convolution이고, $\mathcal{RS}(\cdot)$는 $N\times r\tilde{C_{2}}$에서 $rN\times \tilde{C_{2}}$로  reshape 하는 operation이다.&lt;/p&gt;

&lt;p&gt;각 set의 첫 번째 convolution $C_{i}^{1}(\cdot)$로 생성된 feature set $r$은 높은 correlation을 가지고, 이로 인해 최종적으로 reconstructed된 3D point들이 서로 너무 가깝게 위치한다. 따라서 각 feature set에 대해 또 다른 convolution(별개의 weight를 가진)을 추가한다. 이렇게 $r$개의 feature sets에 대해 $r$개의 서로 다른 convolution이 학습되도록 network를 훈련시키므로, 새로운 feature들이 더 다양한 정보를 포함할 수 있어 correlation을 줄일 수 있다. 이 feature expansion 작업은 그림과 같이 $r$개의 feature set들을 각각 분리된 convolution을 적용하여 구현될 수 있고, 계산적으로 효율적인 그룹화된 convolution을 통해 구현될 수 있다.&lt;/p&gt;
&lt;h3 id=&quot;24-cordiante-reconstruction&quot;&gt;2.4 Cordiante Reconstruction&lt;/h3&gt;
&lt;p&gt;여기서는 $rN\times \tilde{C_{2}}$로 확장된 feature로부터 output point들의  3D coordinate를 재구성한다. 특히, 각 point의 feature를 FC layer들을 통과시켜 3D coordinate를 regression한다. 그 결과 최종적으로  upsampling된 $rN\times 3$의  point 좌표를 출력한다.&lt;/p&gt;

&lt;p&gt;처음으로 제안된 입력과 출력이 모두 3D 좌표의 point set인 end-to-end point set upsampling network. 기존에는 related work 부분.
object로부터 local geometry pattern을 학습하기 위해 계층적으로 학습하며 작은 local feature와 큰 local feature들을 다양하게 학습한다. 이후 feature space에서 feature의 수를 증가시켜 upsampling을 수행한다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">Abstract Key point : point당 multi-level feature들을 학습하고 multi-branch convolution unit을 통해 feature space에 implicit하게 point set을 확장한다. 이후 확장된 feature는 multitude of feature들로 분할한 뒤 upsampling된 point set로 재구성한다. 이 Network는 patch-level에서 적용되고, upsampling된 point들이 surface에 균일한 분포로 underlying할 수 있도록 하는 joint loss function을 가진다. some baseline 방법들과 optimization-based 방법들과 성능을 비교한다.</summary></entry></feed>