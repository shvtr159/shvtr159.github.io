<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://shvtr159.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shvtr159.github.io/" rel="alternate" type="text/html" /><updated>2021-09-27T17:23:47+09:00</updated><id>https://shvtr159.github.io/feed.xml</id><title type="html">Study Blog</title><subtitle>for study</subtitle><author><name>KYG</name></author><entry><title type="html">머신러닝 시스템의 종류</title><link href="https://shvtr159.github.io/ml/" rel="alternate" type="text/html" title="머신러닝 시스템의 종류" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>https://shvtr159.github.io/ml</id><content type="html" xml:base="https://shvtr159.github.io/ml/">&lt;p&gt;핸즈온 머신러닝의 머신러닝 내용 일부를 정리.&lt;/p&gt;
&lt;h2 id=&quot;머신러닝-시스템의-종류&quot;&gt;머신러닝 시스템의 종류&lt;/h2&gt;
&lt;p&gt;머신러닝 시스템은 다음과 같이 분류할 수 있지만 서로 배타적이지 않고 원하는 대로 연결 가능하다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;훈련 지도 여부 : 지도 학습, 비지도 학습, 준지도 학습, 강화 학습&lt;/li&gt;
  &lt;li&gt;실시간 훈련 여부 : 온라인 학습, 배치 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;훈련-지도-여부&quot;&gt;훈련 지도 여부&lt;/h3&gt;
&lt;h4 id=&quot;지도-학습-supervised-learning&quot;&gt;지도 학습 (supervised learning)&lt;/h4&gt;
&lt;p&gt;지도학습에는 훈련 데이터에 label(혹은 target이란 표현도 사용됨) 이라는 답이 포함된다. 지도학습에는 분류(classification), 회귀(regression)이 해당된다. 일부 회귀 알고리즘은 분류에 사용할 수도 있고, 반대의 경우도 있다. 그 예로 로지스틱 회귀는 class에 속할 확률을 출력한다.&lt;/p&gt;

&lt;p&gt;중요한 지도 학습 알고리즘들&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;k-최근접 이웃 (K-Nearest Neightbors)&lt;/li&gt;
  &lt;li&gt;선형 회귀 (Linear Regression)&lt;/li&gt;
  &lt;li&gt;로지스틱 회귀 (Logistic Regrassion)&lt;/li&gt;
  &lt;li&gt;서포트 벡터 머신(Supprot Vector Machines, SVM)&lt;/li&gt;
  &lt;li&gt;결정 트리(Decision Tree)와 랜덤 포레스트(Random Forests)&lt;/li&gt;
  &lt;li&gt;신경망 (Neural networks)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;비지도-학습-unsupervised-learning&quot;&gt;비지도 학습 (unsupervised learning)&lt;/h4&gt;
&lt;p&gt;lable 없는 훈련 데이터를 이용해 시스템 스스로 학습.&lt;/p&gt;

&lt;p&gt;대표적 비지도 학습&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;군집 (clustering)&lt;/li&gt;
  &lt;li&gt;시각화 (visualization)와 차원 축소 (dimensionality reduction)&lt;/li&gt;
  &lt;li&gt;이상치 탐지 (anomaly detection)&lt;/li&gt;
  &lt;li&gt;연관 규칙 학습 (Association rule learning)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;준지도-학습-semisupervised-learning&quot;&gt;준지도 학습 (semisupervised learning)&lt;/h4&gt;
&lt;p&gt;레이블이 일부만 있는 데이터. 대부분 지도 학습과 비지도 학습의 조합으로 이루어져 있다.&lt;/p&gt;

&lt;h4 id=&quot;강화-학습-reinforcement-learning&quot;&gt;강화 학습 (reinforcement learning)&lt;/h4&gt;
&lt;p&gt;에이전트(학습 시스템)이 환경을 관찰해서 행동을 실행하고 그 결과로 보상 또는 벌점을 받는다. 시간이 지나면서 가장 큰 보상을 얻기 위해 정책(policy)라고 부르는 최상의 전략을 스스로 학습한다.&lt;/p&gt;

&lt;h3 id=&quot;실시간-훈련-여부&quot;&gt;실시간 훈련 여부&lt;/h3&gt;
&lt;h4 id=&quot;배치-학습-batch-learning&quot;&gt;배치 학습 (batch learning)&lt;/h4&gt;
&lt;p&gt;시스템이 가용한 데이터를 모두 사용해 훈련한다. 즉 먼저 시스템을 훈련시키고 더 이상의 학습 없이 제품 시스템에 적용한다. 오프라인 학습(offline learning)이라고 한다. 이는 컴퓨팅 자원이 많이 필요하다는 점을 고려해야 한다.&lt;/p&gt;

&lt;h4 id=&quot;온라인-학습-online-learning&quot;&gt;온라인 학습 (online learning)&lt;/h4&gt;
&lt;p&gt;데이터를 순차적으로 한 개씩 또는 미니배치(mini-batch) 단위로 훈련한다. 매 학습 단계가 빠르고 비용이 적게 들어 데이터가 도착하는 즉시 학습할 수 있다. 그러나 나쁜 데이터가 주입되었을 때 시스템 성능이 점진적으로 감소하기 때문에 모니터링이 필요하다.&lt;/p&gt;</content><author><name>KYG</name></author><summary type="html">핸즈온 머신러닝의 머신러닝 내용 일부를 정리. 머신러닝 시스템의 종류 머신러닝 시스템은 다음과 같이 분류할 수 있지만 서로 배타적이지 않고 원하는 대로 연결 가능하다. 훈련 지도 여부 : 지도 학습, 비지도 학습, 준지도 학습, 강화 학습 실시간 훈련 여부 : 온라인 학습, 배치 학습</summary></entry><entry><title type="html">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation/" rel="alternate" type="text/html" title="PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation" /><published>2021-09-13T00:00:00+09:00</published><updated>2021-09-13T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;기존에는 point cloud의 irregular한 형식때문에  regular한 3D voxel grid 또는 collection of images로 바꿔 사용하곤 했지만 이것은 불필요하게 많은 data가 렌더링 되는 문제가 있다. 그래서 여기서는 point cloud를 바로 사용하며 input point들의 permutation invariance&lt;sup&gt;&lt;a href=&quot;#footnote_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;를 잘 고려하는 neural network를 설계하였다. PointNet은 object detection, part segmentation, scene semantic parsing에 이르는 application들을 위한 통일된 구조를 제공한다. 또한, 이론적으로, network가 무엇을 학습했는지와 network가 왜 perturbation(noise와 유사)과 corruption에 robust한지 이해하기 위한 분석을 제공한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;기존에는 point cloud의 irregular한 형식 때문에 대부분이 deep net architecture에 넣기 전  regular한 3D voxel grid 또는 collection of images (e.g, views)로 변형하곤 했다. 그러나 이것은 불필요하게 많은 data가 렌더링 되는 문제가 있다. 그래서 point cloud를 직접 사용하도록 하였다. 이를 위해서는 permutation invariance 한 성질로 인해 net computation에서 특정 대칭화가 필요하고, rigid motion&lt;sup&gt;&lt;a href=&quot;#footnote_2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;에 invariance하다는 사실을 고려해야 한다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;2. Related work&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Point Cloud Fetures&lt;/strong&gt; 기존 point cloud feature는  대부분 특정 task를 위해 handcrafted 되었다. 이는 일반적으로 intrinsic 또는 extrinsic하게 분류되는 특정 transformation에 대해 invariant하게 설계된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning on 3D Data&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Volumetric CNNs&lt;/em&gt; : voxelized shape에 적용되는 network이지만 data sparsity과 계산 비용때문에 resolution이  제한된다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Multiview CNNs&lt;/em&gt; : 3D point cloud나 shape를 2D 이미지로 렌더링한 다음 2D conv net를 이용해 분류. 그러나 scene understanding이나 3D task(point classification, shape completion 등)로 확장하는 것이 어렵다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Spectral CNNs&lt;/em&gt; : mesh에 적용하는 방법. 그러나 유기물과 같은 manifold mesh에 제약을 받고 가구와 같은 non-isometric 모양으로 어떻게 확장하는지 명확하지 않다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Feature-based DNNs&lt;/em&gt; : traditional shape features를 추출하여 3D data를 vector로 변환한 뒤 FC net를 이용해 shape를 분류한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Deep Learning on Unorderes Sets&lt;/strong&gt; 데이터 구조 관점에서, point cloud는 순서 없는 vector의 집합이다. 딥러닝에서 대부분의 작업은 sequence(음성 및 언어 처리), images와 volume(video 또는 3D data)와 같은 regular한  input에 초점을 맞추고 있지만, point set에 대해서는 잘 이루어지지 않았다.&lt;/p&gt;

&lt;h2 id=&quot;3-problem-statement&quot;&gt;3. Problem Statement&lt;/h2&gt;
&lt;p&gt;point cloud는 각 point $P_{i}$가 $(x, y, z)$좌표에 color, normal과 같은 추가적인 feature channels를 가지고, 이 point들이 모인 set을 ${P_i \mid i=1,…,n}$로 표현한다. 여기서는 point의 channel로 $(x,y,z)$ 좌표만을 사용한다.&lt;/p&gt;

&lt;p&gt;Object classification task에서, input point cloud는 shape에서 직접 sampling되거나, 장면 point cloud에서 사전에 segmentation된다. PointNet에서는 모든 $k$ 후보 classes에 대해 $k$개의 scores를 출력한다.&lt;/p&gt;

&lt;p&gt;Semantic segmentation에서는 input은 부분 region segmentation을 위한 단일 object이거나 object region segmentation 3D scene의 sub-volume일 수 있다. PointNet에서는 각 $n$개의 point 및 $m$개의 semantic subcategory에 대해 $n\times m$개의 score를 출력한다.&lt;/p&gt;

&lt;h2 id=&quot;4-deep-learning-on-point-sets&quot;&gt;4. Deep Learning on Point Sets&lt;/h2&gt;
&lt;h3 id=&quot;41-properties-of-point-sets-in-mathbb-rn&quot;&gt;4.1. Properties of Point Sets in $\mathbb R^n$&lt;/h3&gt;
&lt;p&gt;Network의 input은 Euclidean space 상 point의 subset이다. 이는 다음 3가지 특징을 가지고 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unordered.&lt;/strong&gt; 이미지의 pixel 배열이나 voxel 배열과 달리 point cloud는 특정한 순서가 없다. 다시 말해 네트워크가 N개의 3D point를 사용한다면 이 N개 point의 순서에 따라 결과가 달라지지 않아야 한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interaction among points.&lt;/strong&gt; point들은 distance metric&lt;sup&gt;&lt;a href=&quot;#footnote_3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;이 존재하는 공간에 있다. 즉, 이는 point들이 고립되어있지 않고 주변 point들과 의미 있는 subset을 형상한다는 것을 의미한다. 따라서, model은 주변 point에서 local 구조 및 local 구조 간의 combinatorial interaction을 확인할 수 있어야 한다. 간단히 이야기 하면 point들간의 거리 정보만을 통해 의미를 찾아야 한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Invariance under transformations.&lt;/strong&gt;  geometric object로서 point cloud의 학습된 표현은 특정 transformation을 수행해도 불변해야 한다. 예를 들어 모든 포인트들을 회전하거나 이동시켜도 그 특성을 변하지 않아야 한다는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-pointnet-architecture&quot;&gt;4.2. PointNet Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133418360-97952ac2-feeb-4b00-8ead-d466066648ea.png&quot; alt=&quot;image&quot; /&gt;&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Figure 2. PointNet의 구조&lt;/span&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;classification network는 $n$개의 point를 input으로 받으며, 여기서는 $(x, y, z)$ 좌표만을 고려하여 $n\times 3$의 vector를 input으로 받는다. 사진의 구조는 classification network이지만 classification network와 segmentation network는 서로 구조의 많은 부분을 공유한다. PointNet은 다음 3가지 key module을 가진다. 이 input을 transformation 한 뒤  다시 feature tranformation 한 다음 max pooling을 통해 point feature를 aggregate한다. 최종 output은 $k$개의 class에 대한 각 classification score이다. 사진의 구조는 classification network이지만 classification network와 segmentation network는 서로 구조의 많은 부분을 공유한다. Batchnorm은 ReLU와 함께 모든 layer에 적용하고, dropout은 classification net의 마지막 mlp에만 적용한다.&lt;/p&gt;

&lt;p&gt;자세한 내용으로 PointNet은 다음 3가지 key module을 가진다.&lt;/p&gt;
&lt;h4 id=&quot;symmetry-function-for-unordered-input&quot;&gt;Symmetry Function for Unordered Input&lt;/h4&gt;
&lt;p&gt;input 순서에 invariant한 model을 만들기 위해 다음 3가지 전략을 사용한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;입력 순서를 canonical order로 정렬한다.(여기서 canonical order는 표준 형식을 따르는 정렬으로 특정 알고리즘을 지칭하지는 않는다)&lt;/li&gt;
  &lt;li&gt;input을 RNN을 학습시키기 위한 sequence로 취급하지만, 모든 종류의 순열을 이용해 training data를 augmentation 한다.&lt;/li&gt;
  &lt;li&gt;각 point에서의 정보를 aggregate&lt;sup&gt;&lt;a href=&quot;#footnote_4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;하기 위해 간단한 symmetric function을 사용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;symmetric function은 입력 순서에 관계 없이 동일한 output을 내는 함수로 여기서는 n개의 vector를 입력으로 받는다. symmetric function의 예로는 +와 $\times$연산이 있다.&lt;/p&gt;

&lt;p&gt;그러나 일반적으로 고차원 공간에서는 stable한 순서가 존재하지 않는다. 때문에 sorting으로는 ordering 문제가 완전히 해결되지 않고, 이 문제가 지속되기 때문에 network가 input에서 output으로 일관된 mapping을 학습하기 어렵다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/134469465-907b04b3-8421-43d5-bc75-785aa1552912.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;순서 invariance를 달성하기 위한 3가지 접근 방법&lt;/span&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;위 이미지에서 볼 수 있듯이 point를 정렬하지 않고 바로 처리하는 것보다 정렬한 뒤 MLP에 직접 적용하는 것이 좀 더 낫지만 여전히 성능은 좋지 않다. 또, RNN을 이용한 방법 또한 제안한 방법만큼 좋은 성능을 보여주지 못했다.&lt;/p&gt;

&lt;p&gt;그래서 PointNet은 다음과 같이 집합의 transform된 element들에 대해 symmetric function을 적용하여 생성된 point set에 정의된 general 함수를 근사화하는 것이다.&lt;/p&gt;

\[f(\{x_1,...,x_n\})\approx g(h(x_1),...,h(x_n))\]

&lt;center&gt;$f\;:2^{\mathbb R^N}\rightarrow \mathbb R,\;h\;:{\mathbb R^N}\rightarrow {\mathbb R^K},\;g$ (symmetric 함수) $:\underbrace{ {\mathbb R^K} \times \cdots \times {\mathbb R^K}}_ n \rightarrow {\mathbb R^N}$ &lt;/center&gt;

&lt;p&gt;Basic 모듈은 매우 간단하다. $h$를 다층 퍼셉트론 network를 이용해서, $g$는 single 변수 함수와 max pooling 함수의 구성으로 approximate한다. $h$의 collection을 통해 우리는 집합의 다른 property들을 알기위해 많은 $f$를 학습할 수 있다.&lt;/p&gt;

&lt;p&gt;이 모듈의 output은 input set의 global signature인 $[f_1,\,…\,,f_K]$ 벡터이다.&lt;/p&gt;

&lt;h4 id=&quot;local-and-global-information-aggregation&quot;&gt;Local and Global Information Aggregation&lt;/h4&gt;
&lt;p&gt;classification을 위한 shape global feature에 대해 SVM이나 다층 퍼셉트론 classifier는 쉽게 훈련시킬 수 있지만, point segmentation은 local과 global을 모두 필요로 하기 때문에 어려움이 있다. 이를 해결하기 위한 solution이 Fig 2에서 볼 수 있는 segmentation Network이다.&lt;/p&gt;

&lt;p&gt;이 network는 global point cloud의 feature 벡터를 계산하고 이를 각 point feature에 concatenate 한 뒤 각 point 별 feature에 다시 제공한다(Fig 1의 nx64에 global feature 1024를 모두 추가해준다). 그 다음 combine 된 point feature를 기반으로 새로운 각 point 별 feature를 추출한다. 이를 이용하면 각 point 별 feature가 local 및 global 정보를 모두 인식한다.&lt;/p&gt;

&lt;h4 id=&quot;joint-alignment-network&quot;&gt;Joint Alignment Network&lt;/h4&gt;
&lt;p&gt;point cloud가 rigid transformation 같은 특정한 geometric transformation 이 수행되는 경우 point cloud의 semantic labeling은 변하지 않아야 한다. 때문에 point set에 의해 학습된 것이 이러한 변환에 불변하기를 기대한다.&lt;/p&gt;

&lt;p&gt;저자는 spatial transformer networks에서 motivate된  mini-network(Fig 2 T-net)를 추가하여 이를 해결하였다. 이 network는 affine 변환 matrix를 예측하고 이를 input points의 coordiates에 바로 적용하여 간단히 해결하였다. mini-network 자체는 큰 network와 비슷하고, point independent feature extraction과 maxpooling, FC layer의 기본 모듈로 구성된다.&lt;/p&gt;

&lt;p&gt;그러나 featrue space의 transformation matrix는 spatial transform matrix보다 더 높은 차원으로 이루어져서 optimization의 난이도가 급격히 상승한다. 이를 위해 regularization term을 softmax training loss로 추가한다. 또, feature transformation matrix가 다음과 같은 직교 행렬에 가깝도록 제한한다.&lt;/p&gt;

\[L_{reg} = \left \| I-AA^T \right \|^2_F\]

&lt;p&gt;$A$는 mini-network에 의해 예측된 feature alignment matrix이다. 직교 변환은 input의 정보를 잃지 않기 때문에 필요하다.&lt;/p&gt;

&lt;h3 id=&quot;43-theoretical-analysis&quot;&gt;4.3. Theoretical Analysis&lt;/h3&gt;
&lt;h4 id=&quot;universal-approximation6&quot;&gt;Universal approximation&lt;sup&gt;&lt;a href=&quot;#footnote_6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&quot;footnote_1&quot;&gt;1&lt;/a&gt;: 입력 벡터 요소의 순서와 상관 없이 같은 출력을 생성하는 것. MLP가 이에 해당하며 CNN, RNN은 이에 해당하지 않는다.&lt;br /&gt;
&lt;a name=&quot;footnote_2&quot;&gt;2&lt;/a&gt;: rigid motion은 transformation을 하더라도 point들간의 distance와 방향은 그대로 유지되는 변환을 말한다. 여기에는 translation, rotation, reflection, glide reflection이 해당된다.&lt;br /&gt;
&lt;a name=&quot;footnote_3&quot;&gt;3&lt;/a&gt;: distance를 정의하는 방법. 가장 간단한 예시로 Euclidean distance를 들 수 있다.&lt;br /&gt;
&lt;a name=&quot;footnote_4&quot;&gt;4&lt;/a&gt;: ????&lt;br /&gt;
&lt;a name=&quot;footnote_5&quot;&gt;5&lt;/a&gt;:  M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In NIPS 2015&lt;br /&gt;
&lt;a name=&quot;footnote_6&quot;&gt;6&lt;/a&gt;: 1개의 히든 레이어를 가진 Neural Network를 이용해 어떠한 함수든 근사시킬 수 있다는 이론. 당연히 활성화 함수는 비선형 함수여야 한다&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">Abstract 기존에는 point cloud의 irregular한 형식때문에 regular한 3D voxel grid 또는 collection of images로 바꿔 사용하곤 했지만 이것은 불필요하게 많은 data가 렌더링 되는 문제가 있다. 그래서 여기서는 point cloud를 바로 사용하며 input point들의 permutation invariance1를 잘 고려하는 neural network를 설계하였다. PointNet은 object detection, part segmentation, scene semantic parsing에 이르는 application들을 위한 통일된 구조를 제공한다. 또한, 이론적으로, network가 무엇을 학습했는지와 network가 왜 perturbation(noise와 유사)과 corruption에 robust한지 이해하기 위한 분석을 제공한다.</summary></entry><entry><title type="html">PU-GAN: A Point Cloud Upsampling Adversarial Network</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-gan/" rel="alternate" type="text/html" title="PU-GAN: A Point Cloud Upsampling Adversarial Network" /><published>2021-09-10T00:00:00+09:00</published><updated>2021-09-10T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-gan</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-gan/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;PU-Net, EC-Net, MPU 등의 network들은 학습을 통해 point cloud upsampling의 장점을 입증했다. 그러나 이 네트워크들은 sparse하고 non-uniform한 low-quality inputs로부터 좋은 결과를 얻지 못할 수 있다. 그래서 upsampling과 data amendment 능력을 결합한 PU-GAN을 제안한다. 주된 기여는 latent space에서 다양한 point 분포를 생성하도록 generator를 학습시키고, 이 point set에 대해 implicit하게 평하가는 데 도움이 되는 discriminator network이다. 그러나 generator와 discriminator간의 균형을 맞추고 낮은 수렴 경향을 피하기 어려운 문제가 있다. Thus, we first improve the point generation quality, or equivalently the feature expansion capability, of the generator, by constructing an up-down-up unit to expand point features by upsampling also their differences for self-correction.  또, feature integration quality를 향상시키기 위해 self-attention unit을 사용한다. 마지막으로, 결과의 distribution uniformity를 향상시키고 discriminator가 target distribution에서 더 많은 latent pattern을 학습하도록 하기 위해 aadversarial, uniform, reconstruction term을 사용해 end-to-end 로 network를 학습시키는 compound loss를 추가로 사용한다.&lt;/p&gt;
&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;31-overview&quot;&gt;3.1 Overview&lt;/h3&gt;
&lt;p&gt;point 수가 $N$개인 순서 없는 sparse point set $\mathcal{P}={  p_{i}  }_ {i=1}^{N}$가 주어지면, $rN$개의 point를 가지는 dense point set  $\mathcal{Q}= {  q_{i} }_ {i=1}^{rN}$을 생성하는 것을 목표로 한다. 이때, $r$은 upsampling rate이다. $\mathcal{Q}$가 반드시 $\mathcal{P}$의 superset(상위집합)은 아니지만, 출력 $\mathcal{Q}$가 다음 2가지 조건을 만족하기를 원한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mathcal{Q}$는 $\mathcal{P}$와 latent target object의 같은 underlying geometry를 설명해야 하고, 때문에 $\mathcal{Q}$의 point들은 target object의 표면에 있어야 한다.&lt;/li&gt;
  &lt;li&gt;sparse하고 non-uniform한 input $\mathcal{P}$에 대해서도 $\mathcal{Q}$의 point들은 target object 표면에 uniformly-distribute하게 분포되어야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133037097-4dc912bb-f9d2-4a0f-83a0-40b3776db7f4.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 PU-GAN의 전체적인 네트워크 구조를 보면, generator는 sparse input $\mathcal{P}$로부터 dense output $\mathcal{Q}$를 생성하고, discriminator는 fake로 생성된 것을 찾는것을 목표로 한다.&lt;/p&gt;

&lt;h3 id=&quot;32-network-architecture&quot;&gt;3.2 Network Architecture&lt;/h3&gt;
&lt;h4 id=&quot;321--generator&quot;&gt;3.2.1  Generator&lt;/h4&gt;
&lt;p&gt;Figure 2에서 보이듯이, generator는 3개의 component를 가지고있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The feature extraction component&lt;/strong&gt; $N\times d$ ($d$ : Input point 속성의 차원 수. $i.e.$, 좌표, 색상, normal 등) 의 input $\mathcal{P}$로부터 feature $\mathbf{F}$를 추출하기 위해 가장 간단한 3D coordinates인것만을 고려하여 $d=3$의 case에 focusing하고, [Patch-based progressive 3D point set upsampling, CVPR2019]의 feature extraction 방법을 채택하여 서로 다른  layer들에 걸쳐 feature들을 통합하기 위해 dense connection이 도입됐다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The feature expansion component&lt;/strong&gt; $\mathbf{F}$를 확장하여 확장된 feature $\mathbf{F_{up}}$을 생성한다. 여기서는 generator가 더 다양한 point distribution을 생성할 수 있도록 $\mathbf{F_{up}}$의 feature variation을 강화시키기 위해 &lt;em&gt;up-down-up expansion unit&lt;/em&gt; (Figure2의 위쪽)을 설계한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The point set generation component&lt;/strong&gt; 먼저, multilayer perceptrons (MLPs) 세트를 통해 $\mathbf{F_{up}}$에서 3D coordiante 세트를 regression한다. feature expansion process가 여전히 local하기 때문에 $\mathbf{F_{up}}$의 feature (또는 latent space의 point들)이 input에 가깝다. 때문에 서로 멀리 떨어진 $rN$ points만을 남기기 위해 network에 farthest sampling setp을 포함한다. 이를 위해 $\mathbf{F}$에서 $\mathbf{F_{up}}$으로 확장할 때, $\mathbf{F_{up}}$에 $(r+2)N$ features를 생성한다.이 전략은 global perspective에서 point set 의 distribution uniformity를 강화한다.&lt;/p&gt;

&lt;h4 id=&quot;322-discriminator&quot;&gt;3.2.2 Discriminator&lt;/h4&gt;
&lt;p&gt;Discriminator의 목표는 generator에서 생성된 $rN$ points의 set을 구분하는 것이다. 이를 위해 처음에는 global feature를 추출하기 위해 [PCN: Point completion network, 3DV2018]의 basic network 구조를 이용한다. feature learning을 개선하기 위해  feature concatenation 한 뒤 self-attention unit을 추가한다. 이는 basic MLPs와 비교하여 feature integration을 개선하고 subsequent feature extraction 기능을 개선할 수 있다. 다음으로 global feature를 생성하기 위해 MLPs set와  max pooling를 적용하고 FC layer set를 통해 최종 confidence value를 regression한다. 이 값이 1에 가까울수록 높은 신뢰도의 target distribution에서 온 가능성이 있다고 예측한다.&lt;/p&gt;

&lt;h3 id=&quot;33-up-down-up-expansion-unit&quot;&gt;3.3 UP-down-up Expansion Unit&lt;/h3&gt;

&lt;p&gt;기존 PU-Net, EC-Net, MPU 등의 network들은 학습을 통해 point cloud upsampling의 장점을 입증했다. 그러나 이 네트워크들은 sparse하고 non-uniform한 low-quality inputs로부터 좋은 결과를 얻지 못할 수 있다.
그래서 기존의 point cloud를 deep learning을 사용해서 처리하는 방식과 GAN 기반의 3D shape processing 방식을 결합하여 upsampling을 수행하도록 하였다. 기존 GAN을 이용한 processing은 3D volume 방식이었고 point cloud를 사용한 것은 reconstruction에 가까워 upsampling은 최소로 수행하였다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">Introduction PU-Net, EC-Net, MPU 등의 network들은 학습을 통해 point cloud upsampling의 장점을 입증했다. 그러나 이 네트워크들은 sparse하고 non-uniform한 low-quality inputs로부터 좋은 결과를 얻지 못할 수 있다. 그래서 upsampling과 data amendment 능력을 결합한 PU-GAN을 제안한다. 주된 기여는 latent space에서 다양한 point 분포를 생성하도록 generator를 학습시키고, 이 point set에 대해 implicit하게 평하가는 데 도움이 되는 discriminator network이다. 그러나 generator와 discriminator간의 균형을 맞추고 낮은 수렴 경향을 피하기 어려운 문제가 있다. Thus, we first improve the point generation quality, or equivalently the feature expansion capability, of the generator, by constructing an up-down-up unit to expand point features by upsampling also their differences for self-correction. 또, feature integration quality를 향상시키기 위해 self-attention unit을 사용한다. 마지막으로, 결과의 distribution uniformity를 향상시키고 discriminator가 target distribution에서 더 많은 latent pattern을 학습하도록 하기 위해 aadversarial, uniform, reconstruction term을 사용해 end-to-end 로 network를 학습시키는 compound loss를 추가로 사용한다. Method 3.1 Overview point 수가 $N$개인 순서 없는 sparse point set $\mathcal{P}={ p_{i} }_ {i=1}^{N}$가 주어지면, $rN$개의 point를 가지는 dense point set $\mathcal{Q}= { q_{i} }_ {i=1}^{rN}$을 생성하는 것을 목표로 한다. 이때, $r$은 upsampling rate이다. $\mathcal{Q}$가 반드시 $\mathcal{P}$의 superset(상위집합)은 아니지만, 출력 $\mathcal{Q}$가 다음 2가지 조건을 만족하기를 원한다. $\mathcal{Q}$는 $\mathcal{P}$와 latent target object의 같은 underlying geometry를 설명해야 하고, 때문에 $\mathcal{Q}$의 point들은 target object의 표면에 있어야 한다. sparse하고 non-uniform한 input $\mathcal{P}$에 대해서도 $\mathcal{Q}$의 point들은 target object 표면에 uniformly-distribute하게 분포되어야 한다.</summary></entry><entry><title type="html">PU-Net: Point Cloud Upsampling Network</title><link href="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-net-review/" rel="alternate" type="text/html" title="PU-Net: Point Cloud Upsampling Network" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-net-review</id><content type="html" xml:base="https://shvtr159.github.io/%EB%85%BC%EB%AC%B8/pu-net-review/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Key point : point당 multi-level feature들을 학습하고 multi-branch convolution unit을 통해 feature space에 implicit하게 point set을 확장한다. 이후 확장된 feature는 multitude of feature들로 분할한 뒤 upsampling된 point set로 재구성한다. 이 Network는 patch-level에서 적용되고, upsampling된 point들이 surface에 균일한 분포로 underlying할 수 있도록 하는 joint loss function을 가진다. some baseline 방법들과 optimization-based 방법들과 성능을 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Recently, pioneering works [29, 30, 18] began to explore the possibility of reasoning point clouds by means of deep networks for understanding geometry and recognizing 3D structures. In these works, the deep networks directly extract features from the raw 3D point coordinates without using traditional features, e.g., normal and curvature.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;최근, pioneering work들이 geometry를 이해하고, 3D structure를 recognize하기 위한 deep 네트워크를 통해 point cloud의 reasoning 가능성을 explore하기 시작했다. 이러한 연구들에서 deep network는 raw 3D point coordinate로부터 normal과 curvature 같은 traditional feature들 없이 바로 feature들을 추출하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;First, unlike the image space, which is represented by a regular grid, point clouds do not have any spatial order and regular structure. Second, the generated points should describe the underlying geometry of a latent target object, meaning that they should roughly lie on the target object surface. Third, the generated points should be informative and should not clutter together. Having said that, the generated output point set should be more uniform on the target object surface. Thus, simple interpolation between input points cannot produce satisfactory results.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Image S.R과 달라야하는 이유 : 1. Regular grid로 표현되는 image와 달린 point cloud는 spatial 순서와 regular한 구조를 전혀 가지지 않는다. 2. 생성된 point cloud는 잠재적인 대상 object의 underlying geometry를 설명해야만 하며, 이는 대상 object의 표면에 rough하게 위치해야 한다는 것을 의미한다. 3. 생성된 point들은 정보를 제공해야 하고 서로 clutter?? 해서는 안 된다. 따라서, 생성된 output point set은 target object 표면에 균일하게 있어야 하므로 단순히 input point들을 interpolation 하는 것은 만족스러운 결과를 얻을 수 없다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Our network, namely PU-Net, learns geometry semantics of point-based patches from 3D models, and applies the learned knowledge to upsample a given point cloud. It should be noted that unlike previous network-based methods designed for 3D point sets [29, 30, 18], the number of input and output points in our network are not the same.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PU-Net은 3D model로부터 point 기반 patch의 geometry적 semantics를 학습하고 이 지식을 주어진 point cloud를 upsampling하는데 적용한다. 이전 network-based 방법(pioneering works)과 달리, PU-Net의 input과 output point의 수는 같지 않다는 점을 유의해야 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;We formulate two metrics, distribution uniformity and distance deviation from underlying surfaces, to quantitatively evaluate the upsampled point set, and test our method using variety of synthetic and real-scanned data.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Upsampling된 point set을 정량적으로 평가하고 다양한 실제, 합성 data를 이용해 방법을 test하기 위해 Distribution uniformity와 underlying surfaces로부터의 거리 편차라는 두 개의 metrics를 formulate했다.&lt;/p&gt;

&lt;h3 id=&quot;related-work&quot;&gt;Related work&lt;/h3&gt;
&lt;h4 id=&quot;optimization-based-methods&quot;&gt;optimization-based methods&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Alexa et al.&lt;/strong&gt; : local 탄젠트 space의 Voronoi 다이어그램의 vertex 들을 interpolation 한 point를 이용해 point set을 upsampleing한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lipman et al.&lt;/strong&gt; : L1 median에 기초한 point resampling과 surface reconstruction을 위한 Locally Optimal Projection(LOP)을 제시.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Huang et al.&lt;/strong&gt; : point set density 문제를 해결하기 위한 개선된 LOP 제시.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;좋은 결과를 보여주었지만, underlying surface가 smooth하다는 강한 가정을 하였기 때문에 방법의 범위가 제한된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Wu et al.&lt;/strong&gt; : 일관성 있는 하나의 step으로 consolidation(통합)과 completion을 합치는 deep points 표현 방법을 제안한다. 그러나 global smoothness를 강제하지 않아 큰 noise에 취약하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 방법들은 data-driven 방법이 아니므로 priors(머신러닝이 아닌 방법. 이미 주어진 지식을 바탕으로 알고리즘 이용 등)에 크게 의존한다.&lt;/p&gt;

&lt;h4 id=&quot;deep-learning-based-methods&quot;&gt;deep-learning-based methods&lt;/h4&gt;
&lt;p&gt;대부분 기존의 work들은 point cloud를 volumetric grid도, geometric graph와 같은 다른 3D 표현으로 변환하여 처리한다. Qi et al. 은 처음으로 point cloud classification, segmentation을 위한 딥러닝 network를 제안했다. 다양한 방법이 있지만, To the best of our knowledge, upsampling에 초점을 둔 것은 없었다.&lt;/p&gt;

&lt;h2 id=&quot;2-network-architecture&quot;&gt;2. Network Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/132798267-c60cfc2d-e67d-48b7-8113-cd495d5ea480.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;90%&quot; height=&quot;90%&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;The architecture of PU-Net&lt;/span&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Given a 3D point cloud with point coordinates in nonuniform distributions, our network aims to output a denser point cloud that follows the underlying surface of the target object while being uniform in distribution.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;불균일한 분포의 point coordinate를 가진 3D point cloud가 주어진 경우, 분포가 균일하면서도 object의 underlying surface를 따라가는 높은 밀도의 point cloud를 출력하는 것을 목표로 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Our network architecture (see Fig. 1) has four components: patch extraction, point feature embedding, feature expansion, and coordinate reconstruction.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Network Architecture는 4개의 component를 가진다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Patch-Extraction&lt;/strong&gt; : prior 3D model들의 집합으로부터 다양한 scale과 distribution에서 point의 patch를 추출한다. – fig1과 같이 다양한 patch를 추출한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Point Feature Embedding&lt;/strong&gt; : 계층적 feature learning과 multi-level feature aggregation(집계)을 통해 raw 3D coordinate를 feature space로 mapping한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feature Expansion&lt;/strong&gt; : feature의 개수를 증가시킨다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Coordinate Reconstruction&lt;/strong&gt; : F.C layer series를 통해 output point cloud의 3D coordinate를 reconstruction한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;21-patch-extraction&quot;&gt;2.1 Patch Extraction&lt;/h3&gt;
&lt;p&gt;Training을 위한 prior information으로 다양한 모양의 3D object들을 수집한다. 기본적으로 network가 upsampling 하기 위해서는 object로부터 local geometry pattern들을 학습해야 한다. 이것이 patch 기반 접근법을 선택한 motive이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;In detail, we randomly select M points on the surface of these objects. From each selected point, we grow a surface patch on the object, such that any point on the patch is within a certain geodesic distance (d) from the selected point over the surface. Then, we use Poisson disk sampling to randomly generate N_hat points on each patch as the referenced ground truth point distribution on the patch. In our upsampling task, both local and global context contribute to a smooth and uniform output. Hence, we set d with varying sizes, so that we can extract patches of points on the prior objects with varying scale and density.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Detail. object의 surface에서 random하게 $M$개의 point를 선택한다. 각각 선택된 point의 object 표면에서의 patch는, patch 내의 모든 점이 선택된 point로부터 certain geodesic distance $(d)$ 내에 있도록 생성한다. 그리고 나서 ground truth의 patch 내 point 분포를 기준으로 하여 Poisson disk sampling을 이용해 각 patch에 $\hat{N}$개의 point를 생성한다. 이 Upsampling task에서 local 및 global context 모두 output이 smooth하고 uniform하도록 기여한다. 우리는 $d$를 다양한 크기로 설정하여 prior object에서 다양한 scale과 density로 point들의 patches를 추출할 수 있도록 하였다.&lt;/p&gt;

&lt;h3 id=&quot;22-point-feature-embedding&quot;&gt;2.2	Point Feature Embedding&lt;/h3&gt;
&lt;p&gt;patch로부터 Local 및 global geometry context를 학습하기 위해 다음 두 feature learning 전략을 사용하였으며, 그 이점들로 서로 보완한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical feature learning.&lt;/strong&gt; PointNet++의 계층적 feature 학습 메커니즘을 네트워크의 가장 frontal한 부분으로 채택한다. 계층적 feature 학습을 채택하기 위해 각 level에서 상대적으로 작은 grouping 반경을 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-level feature aggregation&lt;/strong&gt; network의 lower layer일수록 더 작은 local feature에 대응되고 그 반대도 마찬가지이다. 더 좋은 upsampling 결과를 위해 여기서는 서로 다른 level의 feature들을 optimal 하게 종합해야 한다.  몇몇 기존의 연구들은 cascaded multi-level feature 종합을 위해 skip-connection을 사용하였지만 이러한 top-down propagation 방식은 upsampling 문제에는 효율적이지 않았다. 그래서 여기서는  서로 다른 level의 feature들을 직접 결합하고, network가 각 level의 중요성을 배울 수 있도록 하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Since the input point set on each patch (see point feature embedding in Fig. 1) is subsampled gradually in hierarchical feature extraction, we concatenate the point features from each level by first restoring the features of all original points from the downsampled point features by the interpolation method in PointNet++&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;각 patch에 설정된 input point는 hierarchical feature extraction에서 점진적으로 subsampling 되므로 PointNet++의 interpolation 방법에 의해 downsampling된 모든 original point들의 feature를 처음으로 복원하여 각 level에서 point feature들을 concatenate한다. 특히 level $\ell$에 있는 interpolated된 point $x$의 feature는 다음 식에 의해 계산된다 :&lt;/p&gt;

\[f^{(\ell)}(x)=\frac{\sum_{i=1}^{3}w_{i}(x)f^{(\ell)}(x_{i})}{\sum_{i=1}^{3}w_{i}(x)}\]

&lt;p&gt;$w$는 inverse distance weight인 $w_{i}(x) = 1/d(x,x_{i})$으로 정의되고, $x_{1}, x_{2}, x_{3}$는 level $\ell$에서 $x$에서 가장 가까운 3개의 point이다. 이후 1X1 convolution을 이용해 서로 다른 level의 interpolated feature를 동일한 차원 $C$로 축소한다. 최종적으로 각 level의 feature들을 embedded point feature $f$로 concatenate 한다.&lt;/p&gt;
&lt;h3 id=&quot;23-feature-expansion&quot;&gt;2.3	Feature Expansion&lt;/h3&gt;
&lt;p&gt;Point Feature Embedding 이후 feture space에서 feature의 수를 증가시킨다. &lt;em&gt;point&lt;/em&gt;와 &lt;em&gt;feature&lt;/em&gt;은 서로 &lt;em&gt;interchangeable&lt;/em&gt;하기 때문에  이는 point의 수를 증가시키는 것과 같다. $f$의 차원이 $N\times \tilde{C}$일 때, $N$은 input point의 수이고, $\tilde{C}$는 concatenate된 embedded feature의 feature dimension이다.
feture expansion으로 $rN\times \tilde{C_{2}}$의 차원으로 feature ${f}’$을 출력한다. &lt;br /&gt;
여기서 $r$은 upsampling rate이고, $\tilde{C_{2}}$는 새로운 feature dimension이다.
 이미지와의 다른 특성으로 인해 sub-pixel convolution layer 기반의 효과적인 feature expansion을 제안한다. 이는 다음과 같이 표현된다 :&lt;/p&gt;

\[{f}&apos;=\mathcal{RS}(\;[\,C_{1}^{2}(C_{1}^{1}(f)),\,...\,, C_{r}^{2}(C_{r}^{1}(f))\,]\;)\]

&lt;p&gt;$C_{i}^{1}(\cdot), C_{i}^{2}(\cdot)$는 두 set의 분리된 1X1 convolution이고, $\mathcal{RS}(\cdot)$는 $N\times r\tilde{C_{2}}$에서 $rN\times \tilde{C_{2}}$로  reshape 하는 operation이다.&lt;/p&gt;

&lt;p&gt;각 set의 첫 번째 convolution $C_{i}^{1}(\cdot)$로 생성된 feature set $r$은 높은 correlation을 가지고, 이로 인해 최종적으로 reconstructed된 3D point들이 서로 너무 가깝게 위치한다. 따라서 각 feature set에 대해 또 다른 convolution(별개의 weight를 가진)을 추가한다. 이렇게 $r$개의 feature sets에 대해 $r$개의 서로 다른 convolution이 학습되도록 network를 훈련시키므로, 새로운 feature들이 더 다양한 정보를 포함할 수 있어 correlation을 줄일 수 있다. 이 feature expansion 작업은 그림과 같이 $r$개의 feature set들을 각각 분리된 convolution을 적용하여 구현될 수 있고, 계산적으로 효율적인 그룹화된 convolution을 통해 구현될 수 있다.&lt;/p&gt;
&lt;h3 id=&quot;24-cordiante-reconstruction&quot;&gt;2.4 Cordiante Reconstruction&lt;/h3&gt;
&lt;p&gt;여기서는 $rN\times \tilde{C_{2}}$로 확장된 feature로부터 output point들의  3D coordinate를 재구성한다. 특히, 각 point의 feature를 FC layer들을 통과시켜 3D coordinate를 regression한다. 그 결과 최종적으로  upsampling된 $rN\times 3$의  point 좌표를 출력한다.&lt;/p&gt;

&lt;p&gt;처음으로 제안된 입력과 출력이 모두 3D 좌표의 point set인 end-to-end point set upsampling network. 기존에는 related work 부분.
object로부터 local geometry pattern을 학습하기 위해 계층적으로 학습하며 작은 local feature와 큰 local feature들을 다양하게 학습한다. 이후 feature space에서 feature의 수를 증가시켜 upsampling을 수행한다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="논문" /><summary type="html">Abstract Key point : point당 multi-level feature들을 학습하고 multi-branch convolution unit을 통해 feature space에 implicit하게 point set을 확장한다. 이후 확장된 feature는 multitude of feature들로 분할한 뒤 upsampling된 point set로 재구성한다. 이 Network는 patch-level에서 적용되고, upsampling된 point들이 surface에 균일한 분포로 underlying할 수 있도록 하는 joint loss function을 가진다. some baseline 방법들과 optimization-based 방법들과 성능을 비교한다.</summary></entry><entry><title type="html">기타 정리</title><link href="https://shvtr159.github.io/point-cloud-feature/" rel="alternate" type="text/html" title="기타 정리" /><published>2021-07-28T00:00:00+09:00</published><updated>2021-07-28T00:00:00+09:00</updated><id>https://shvtr159.github.io/point-cloud-feature</id><content type="html" xml:base="https://shvtr159.github.io/point-cloud-feature/">&lt;h2 id=&quot;cnn-연산&quot;&gt;CNN 연산&lt;/h2&gt;
&lt;p&gt;3차원의 합성곱 연산에서 3차원 데이터의 모양은 (채널, 높이, 너비) 또는 (높이, 너비, 채널)과 같은 순서로 쓴다.  필터도 마찬가지로 하여 합성곱의 출력은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133269875-502f2f87-f493-4330-8d20-6b21beb26478.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지에서는 출력 데이터의 채널이 1인데 다수의 채널을 내보내기 위해서는 다음과 같이 여러개의 필터를 사용하면 &lt;strong&gt;필터 갯수만큼의 채널이 생성&lt;/strong&gt;된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133270157-b3de61c8-f1fb-4b53-b282-0300cbaaaa14.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;bias는 다음과 같이 대응 채널의 원소에 모두 더해지므로 차원에는 변화가 없다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133270362-c61205c2-c459-44fe-927d-8147ed63041c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;배치-처리&quot;&gt;배치 처리&lt;/h3&gt;
&lt;p&gt;배치 처리를 지원하기 위해 데이터의 차원을 하나 늘려 4차원 데이터로 저장한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/133270577-6679a8ce-ffe9-4819-832d-76f87c564e01.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 되면 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이루어진다. 즉 N회 분의 처리를 한번에 수행한다.&lt;/p&gt;
&lt;h2 id=&quot;feature-detector-vs-feature-descriptor&quot;&gt;feature detector vs feature descriptor&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;featrue detector&lt;/strong&gt; : interest point (key point, salient point) detector는 어떤 criterion에 따라 point를 선택하는 알고리즘이다. 일반적으로 interest point는 “cornerness” metric과 같이 어떤 함수의 local maximum을 의미한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;feature descriptor&lt;/strong&gt; : descriptor는 interest point의 주변 image patch를 설명하는 value의 vector이다. 이는 raw pixel value처럼 단순하거나, gradient orientation의 히스토그램처럼 복잡할 수 있다.&lt;/p&gt;

&lt;p&gt;일반적으로 interest point와 해당 descriptor를 합쳐서 &lt;strong&gt;local feature&lt;/strong&gt;라고 한다. local feature는 registration, 3D reconstruction, object detection/recognition 등에 사용된다.&lt;/p&gt;

&lt;p&gt;즉, Harris, Min Eigen, FAST 등은 interest point detector, 자세히는 corner detector라고 할 수 있다. SIFT는 detector와 descriptor 둘 다 포함하는데, detector는 DoG를 base로 blob-like 구조의 중심을 detect하고,  gradient orientation의 히스토그램을 base로 descriptor의 역할을 수행한다.&lt;/p&gt;</content><author><name>KYG</name></author><summary type="html">CNN 연산 3차원의 합성곱 연산에서 3차원 데이터의 모양은 (채널, 높이, 너비) 또는 (높이, 너비, 채널)과 같은 순서로 쓴다. 필터도 마찬가지로 하여 합성곱의 출력은 다음과 같다.</summary></entry><entry><title type="html">[Computer Graphics #10] Texture Mapping</title><link href="https://shvtr159.github.io/graphics/computer-graphics-10-texture-mapping/" rel="alternate" type="text/html" title="[Computer Graphics #10] Texture Mapping" /><published>2021-05-28T00:00:00+09:00</published><updated>2021-05-28T00:00:00+09:00</updated><id>https://shvtr159.github.io/graphics/computer-graphics-10-texture-mapping</id><content type="html" xml:base="https://shvtr159.github.io/graphics/computer-graphics-10-texture-mapping/">&lt;p&gt;실제로 모델을 만들 때는 vertex의 색으로 칠하는 것으로는 완벽하지 않다. 때문에 이에 맞는 texture를 모델에 입히는 과정이 필요하다. 각  geometry의 surface에 일치하는 real object의 부분의 polygon을 입혀준다.&lt;/p&gt;

&lt;h2 id=&quot;mapping&quot;&gt;Mapping&lt;/h2&gt;
&lt;p&gt;이때 다음과 같은 u, v의 Texture Coordinates를 사용한다. 
&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119942715-2121db80-bfcd-11eb-8167-b3d306bc2f19.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;weight를 이용해 model에서 vertex 위치를  interpolation 하면 model의 좌표에서 이미지의 좌표값이 나오므로 해당 이미지 좌표의 color값을 사용하면 된다. 이 interpolation을 수행하기 위해  위와 같이 내부의 값을 사용한다. 각 꼭짓점의 weight를 가지는데 이 weight는 반대편의 삼각형의 넓이에 의해 결정된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119943119-9097cb00-bfcd-11eb-9751-1d9ea40e622d.png&quot; alt=&quot;image&quot; width=&quot;45%&quot; height=&quot;45%&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119943181-a5745e80-bfcd-11eb-812e-b644afe0f334.png&quot; alt=&quot;image&quot; width=&quot;45%&quot; height=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러나 이미지 자체도 continuois하지 않기 때문에 u,v의 값이 보통 픽셀 사이로  떨어진다. 그럼 다시 이미지값에서도 interpolation을 수행하여 그 값을 계산한다. 이때는 &lt;strong&gt;pixel의 RGB color을 bilinear interpolation&lt;/strong&gt;하여 값을 알아낸다.&lt;/p&gt;

&lt;h2 id=&quot;screen-space-vs-world-space&quot;&gt;Screen Space vs. World Space&lt;/h2&gt;
&lt;p&gt;그러나 실제로 우리는 3D space 상에 존재하는 triangle이 2D인 screen에 projection된 모습을 보게 된다. 때문에 우리가 원하는 아래와 같이 실제 보고 싶은 모습과 달라진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119944686-8971bc80-bfcf-11eb-92f0-b972bf24e5a5.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 이유는 아래 triangle’s edge 부분이 3D상에서 실제 triangle의 상태인데, projection된 상태에서 interpolation하고 texture mapping을 수행했기 때문에 실제 triangle과는 달라지기 때문이다. 아래 사진을 보면 triangle’s Edge 부분의 선들(vertex간의 거리)은 모두 같은 길이를 가지고 있다. 그러나 projectino된 결과를 보면 그 관계가 유지되지 않는다. 그러면 Triangle에서 직접 수행하면 될 것 같지만 이는 3차원 공간에서 해야 하므로 쉽지 않다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119945406-5f6cca00-bfd0-11eb-9659-a3ec4d60bd9e.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이는 어떻게 projection이 되는지 관계를 알면 back projection을 수행하여 더 좋게 만들 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;solution&quot;&gt;Solution&lt;/h3&gt;
&lt;h4 id=&quot;1-mesh-refinement&quot;&gt;1. Mesh Refinement&lt;/h4&gt;
&lt;p&gt;그렇다면 triangle을 나눠서 수를 늘리고, 개수를 늘리면 더 좋아지지 않을까. 하는 생각으로 늘려보았다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119946977-2c2b3a80-bfd2-11eb-96e8-f73eded537f3.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 결과 처음보다는 좀 더 나아진 것 같지만 여전히 문제가 발생하고 있다. 또한, plat한 평면에 대해서는 triangle의 수를 늘리는 것은 더 좋지 않다.&lt;/p&gt;
&lt;h4 id=&quot;2-screen-space-barycentric-interpolation&quot;&gt;2. Screen Space Barycentric Interpolation&lt;/h4&gt;
&lt;p&gt;그래서 결국 힘들더라도 back projection을 수행한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;world coordinate의 각 끝 point(위 projection 이미지에서 v1, v2)의 x, y, z에서 일단 x, z축을 다음과 같이 본다.&lt;/li&gt;
&lt;/ul&gt;

\[p^{w}_{1}=\begin{pmatrix}
x_{1}\\ z_{1}
\end{pmatrix}\qquad
p^{w}_{2}=\begin{pmatrix}
x_{2}\\ z_{2}
\end{pmatrix}\]

&lt;ul&gt;
  &lt;li&gt;이 point를 screen으로 projection한다. 이 projection으로 $V_{1},V_{2}$가 $p_{1},p_{2}$로 projection 된다.&lt;/li&gt;
&lt;/ul&gt;

\[p_{1}=\frac{hx_{1}}{z_{1}}\qquad
p_{2}=\frac{hx_{2}}{z_{2}}\qquad(1)\]

&lt;ul&gt;
  &lt;li&gt;world coordiante에서의 point는 다음과 같이 나타난다.&lt;/li&gt;
&lt;/ul&gt;

\[p^{w}=(1-s)\begin{pmatrix}
x_{1}\\ z_{1}
\end{pmatrix}+s\begin{pmatrix}
x_{2}\\ z_{2}
\end{pmatrix}\qquad\]

&lt;ul&gt;
  &lt;li&gt;이 $p^{w}(s)$를 screen space로 projection한 것의 위치는 다음과 같이나타난다.&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;$$SCREE\_\,SPACE(p^{w}(s))) =h\frac{(1-s)x_{1}+sx_{2}}{(1-s)z_{1}+sz_{2}}\qquad(2)$$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;이제 식 (1)과 (2)의 관계를 나타내면 다음과 같이 나타나는데 h는 고정된 값이므로 t와 s의 관계를 알아내면 된다.&lt;/li&gt;
&lt;/ul&gt;

\[(1-t)\frac{hx_{1}}{z_{1}}+t\frac{hx_{2}}{z_{2}}=h\frac{(1-s)x_{1}+sx_{2}}{(1-s)z_{1}+sz_{2}}\]

&lt;ul&gt;
  &lt;li&gt;이를 정리하여 s와 t가 다음과 같은 관계를 가지는것을 찾을 수 있다. 즉 실제 world s에서 이런 coordinate를 알고 싶다면 screen에서 이 t를 사용하면 된다. 이 t는 $z_{1}$과 $z_{2}$에 의해 결정된다. 이 식에 $z_{1}$ $z_{2}$을 넣고 s를 계산해 낸다. 이것이 world와 screen에서 사용할 weight값을 계산한 것이다.&lt;/li&gt;
&lt;/ul&gt;

\[s=\frac{tz_{1}}{z_{2}+t(z_{1}-z_{2})}\]

&lt;ul&gt;
  &lt;li&gt;이것은 point에서 수행한 것이므로 이제 triangle에서 수행할 때는 다음과 같이 weight가 2개로 나타난다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119958859-66023e00-bfde-11eb-980c-7e0f39b5602c.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;proxy--objects&quot;&gt;Proxy  Objects&lt;/h3&gt;
&lt;p&gt;그러나 model이 복잡해지면 해당 위치에 어떤 이미지를 넣어야 하는지 하나하나 직접 할 수가 없다. 때문에 이렇게 복잡하게 하지 않아도 되는(예를 들면 반복되는 패턴)이라면 적절히 approximation하여 할당할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;step-1&quot;&gt;Step 1.&lt;/h4&gt;
&lt;p&gt;이를 위해 coordinate를 적당히 simplify한다. 예를 들면 각 면마다 모두 matrix를 계산하기에는 복잡하므로 원통, 직육면체 등 간단한 모습으로 생각하고 어떻게 projection하는지 나타내고 씌운다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119965589-64884400-bfe5-11eb-8560-a41517ead7aa.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;step-2&quot;&gt;Step 2.&lt;/h4&gt;
&lt;p&gt;그러나 실제로는 원통이 아니므로 실제 model의 surface에 mapping을 해야한다. 이 때는 model이 간단하다면 (a)와 같이, 좀 더 복잡하면 model의 surface normal 방향의 값을(b), overlab이 없다면 (c)와 같은 방법으로 수행할 수 있다&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119965893-b7fa9200-bfe5-11eb-841d-44f550299819.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;aliasing&quot;&gt;Aliasing&lt;/h2&gt;
&lt;p&gt;texture를 할당하고 난 후 나타날수 있는 가장 큰 문제점중 하나로, 내 texture가 가진 복잡도와 실제 vertex의 복잡도가 부합하지 않을 때 나타난다. 예를들어 이미지는 매우 복잡한데 vertex가 그에 미치지 못하면 원하지 않는 패턴이 생기게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119966169-00b24b00-bfe6-11eb-96eb-2f7d1064418d.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예를 들어 체스판 무늬에 경우로 보면 vertex가 많지 않다면 검정색 vertex 이후 vertex의 위치가  흰색에 위치하지 못하고 다음검정색 이미지 좌표에 위치해 다시 검정이 나오는 것처럼 듬성듬성 잘못된 값을 가져오게 된다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 vertex 복잡도에 맞게 작은 vertex에서도 사용할만 한 작은 size의 이미지를  각각 만들어준 뒤 그 이미지를 사용한다. 이 때, 한 방향으로만 작아지는 것은 MIP map, x, y 모든 방향으로 작아지는 것을 가지고 있는것을 RIP map 이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/119966937-c7c6a600-bfe6-11eb-8bf3-3dceb22dfa69.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;75%&quot; height=&quot;75%&quot; /&gt;&lt;/p&gt;</content><author><name>KYG</name></author><category term="Graphics" /><summary type="html">실제로 모델을 만들 때는 vertex의 색으로 칠하는 것으로는 완벽하지 않다. 때문에 이에 맞는 texture를 모델에 입히는 과정이 필요하다. 각 geometry의 surface에 일치하는 real object의 부분의 polygon을 입혀준다.</summary></entry><entry><title type="html">[Computer Graphics #9] Shaders 2</title><link href="https://shvtr159.github.io/graphics/computer-graphics-9-shader2/" rel="alternate" type="text/html" title="[Computer Graphics #9] Shaders 2" /><published>2021-05-11T00:00:00+09:00</published><updated>2021-05-11T00:00:00+09:00</updated><id>https://shvtr159.github.io/graphics/computer-graphics-9-shader2</id><content type="html" xml:base="https://shvtr159.github.io/graphics/computer-graphics-9-shader2/">&lt;p&gt;vertex를 연결해서 polygon을 만들고 그 polygon 위에 색을 줄 때 물체의 표면이 가지는 특징을 BRDF로 표현할 수 있었다. 색이라는 것은 사실 표면과 light source와의 상호작용에 의해 결정되기 때문에 정의 가능한 light source 또한 알아보았다. 최종적으로 특정 위치에서 볼 때 실제적으로 rendering을 하기 위해 사용되는 shader를 알아본다.&lt;/p&gt;

&lt;h2 id=&quot;polygonal-shading&quot;&gt;Polygonal Shading&lt;/h2&gt;
&lt;p&gt;실제 우리 눈에 보이는 모델은 vertex 사이를 채운  polygon의 집합으로 볼 수 있다. 때문에 이 polygon을 어떤 색으로 채우는지가 실질적으로 해야 할 일이다. 그래서 polygonal shading은 polygon에 어떤 색을 할당하는지에 대한 기법이라고 할 수 있다. polygonal shading의 대표적인 3가지 방법을 알아본다.&lt;/p&gt;

&lt;h3 id=&quot;1-flat-shading&quot;&gt;1. Flat shading&lt;/h3&gt;
&lt;p&gt;하나의 polygon 내에서 face가 완벽히 flat하고 빛이 들어오는 방향과 보는 방향 모두 동일하다고 가장한다. 한 방향으로 들어온 빛을 한 방향으로 나가는 빛으로 BRDF를 계산한다. 결국 polygon 하나에 shading 계산은 한 번만 수행하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/118248546-5d3e4200-b4df-11eb-8112-63cb8ea71499.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Flat shading 결과는 이미지 첫 번째와 같이 나타난다. 계산이 간단한 만큼 결과가 간단하고 이에 따른 2가지 문제를 확인할 수 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Lateral inhibition : 가운데 그림처럼 색의 값이 극단적으로 반대가 될 때, 그림에서 보이듯이 사이사이에 약간의 회색이 보이게 된다. 사람의 눈 안에서 검은색과 흰색의 밝기 값을 보았을 때 sensing에 서로 영향을 미치기 때문에 나타나는 문제이다.&lt;/li&gt;
  &lt;li&gt;Mach band : 밝기 값이 달라질 때, 그 밝기값이  달라지는 부분에서 다른 부분과의 대비로 인해 원래의 색보다 더 밝은색으로 보이게 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;좀 더 현실적으로 만들기 위해서는 normal을 다양하게 해야만 한다. 이를 위해 vertex간의 normal을 interpolation하여 polygon 위에 다양한 normal을 사용하고자 한다. 그러나 실제로 vertex 자체는 normal을 가질 수 없으므로 주위 polygon을 이용해 vertex의 normal을 결정한다.&lt;/p&gt;

&lt;h3 id=&quot;2-gouraud-shading&quot;&gt;2. Gouraud Shading&lt;/h3&gt;
&lt;p&gt;이렇게 주위의 face normal을 이용하는 방법을 gouraud shading이라고 한다. 해당 vertex를 포함하는 face들의 normal을 평균하여 vertex의 normal을 계산한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/118255339-1d7b5880-b4e7-11eb-8206-92f6af7add9a.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 하면 각 vertex마다 normal을 가지게 되는데 이제 face의 각 vertex의 normal에 가중치를 주고 interpolation하여  polygon 안에 다양한 새로운 normal들을 만든다. 즉 vertex normal로 계산된 color 값을 interpolation 하며 smooth하게 채워 넣는다. 이렇게 하면 주위에 vertex normal 수만큼 계산을 수행해야 하므로 flat shading보다 계산량이 늘어나게 된다.&lt;/p&gt;

&lt;h3 id=&quot;3-phong-shading&quot;&gt;3. Phong Shading&lt;/h3&gt;
&lt;p&gt;Gouraud shading과 같이 vertex의 normal을 구한다. 그러나 Phong Shading은 이 verex의 normal을 이용해 face에 normal 자체를 interpolation으로 채워 넣어 그 normal을 이용해 color값을 계산하여 채워 넣는다. 이는 하나의 face를 가상의 여러 face로 나눈 과를 낸다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/118257471-a1cedb00-b4e9-11eb-8066-9f177d56bf49.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;계산량은 새롭게 채워 넣은 normal 개수만큼 수행하게 되므로 훨씬 늘어난다.&lt;/p&gt;

&lt;h2 id=&quot;global-rendering&quot;&gt;Global Rendering&lt;/h2&gt;
&lt;p&gt;필요한 것들을 정의해줬으므로 이제 rendering할 준비가 됐다. 그러나  local lighting, global lighting을 또 고려해줘야 한다. global lighting은 물체에서의 반사 등을 모두 고려해주는 것으로 대부분 global lighting을 사용하고 local rendering은 거의 사용하지 않는다. global rendering을 할 때, 물체가 이동하거나 시점이 변화하기 때문에 어떤 부분에 어떤 빛의 interaction이 일어나는지를 찾아야 한다. 이런 일을 하기 위해서는 다음 effect들을 모두 판단해야만 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;shadow : 이 surface를 계산할 때, 어떤 light source는 보이고, 어떤 light source는 보이지 않는지를 판단해야 한다.&lt;/li&gt;
  &lt;li&gt;refractions : 굴절. 물체가 투명도가 있을 수도 있으므로 고려해야 한다.&lt;/li&gt;
  &lt;li&gt;Inter-object reflections&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이를 위해 우리는 &lt;strong&gt;Ray tracing&lt;/strong&gt;, 즉 빛을 쫒아가는 작업을 수행해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/118259038-97154580-b4eb-11eb-8870-ed047cf71ca7.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림과 같이 우리가 보았을 때 어떤 픽셀에 어떤 값이 들어오는가를 알아야 이미지를 만들 수 있다. 이 픽셀을 채우기 위해 픽셀 방향의 어떤 물체가 어떤 빛과 interaction 하는지를 알아내고자 한다. 알아내기 위해 우리는 그 방향으로 빛을 쏘는 &lt;strong&gt;Ray casting&lt;/strong&gt;을 수행한다. 이 빛으로 어떤 light source에서 반사된 빛이 오고, 어떤 light source는 가려져서 필요하지 않는지 등을 알 수 있다.&lt;/p&gt;

&lt;p&gt;이러한  가려지고, 가려지지 않는 부분을 계산하기 위해 $S_{L}$이라는 shadow term이 추가되어 다음과 같이 나타난다.  $S_{i}$는 빛이 가려지면 0, 이외의 경우는 1의 값을 가진다.&lt;/p&gt;
&lt;center&gt;$$I=I_{E}+K_{A}I_{A}+\sum_{L}(K_{D}(N\cdot L)+K_{S}(V\cdot R)^{n})S_{L}I_{L}$$&lt;/center&gt;

&lt;p&gt;그러나 우리는 더 실제적으로 만들기 위해 이 뿐만 아니라 multiple reflection과 투명도로 인해 생기는 부분도 계산해줘야 한다. 이를 위한 다른 데서 reflection되 온 빛, 투명하기 때문에 투과되어 온 빛을 계산하는 term들을 추가해준다.&lt;/p&gt;
&lt;center&gt;$$I=I_{E}+K_{A}I_{A}+\sum_{L}(K_{D}(N\cdot L)+K_{S}(V\cdot R)^{n})S_{L}I_{L}+K_{S}I_{R}+K_{T}I_{T}$$&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;$K_{S}I_{R}$ : Radiance for mirror reflection ray. 반사되어온 빛을 위한 term&lt;/li&gt;
  &lt;li&gt;$K_{T}I_{T}$ : Radiance for refraction ray. 어느 정도의 투명도를 거쳐 굴절되어 오는 빛을 위한 term.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$K$는 coefficient. $K_{T}$는 transparency coefficient로 $K_{T}$가 1이면 완전 투명, 0이면 전혀 투명하지 않은 것으로 0~1값으로 투명도를 나타낸다.&lt;/p&gt;

&lt;p&gt;이 ray tracing을 계산하기 위해 어떤 물체의 transmission, reflection term을 다음과 같은 형태로 저장한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/118261195-30ddf200-b4ee-11eb-933d-c29952a7f73c.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;</content><author><name>KYG</name></author><category term="Graphics" /><summary type="html">vertex를 연결해서 polygon을 만들고 그 polygon 위에 색을 줄 때 물체의 표면이 가지는 특징을 BRDF로 표현할 수 있었다. 색이라는 것은 사실 표면과 light source와의 상호작용에 의해 결정되기 때문에 정의 가능한 light source 또한 알아보았다. 최종적으로 특정 위치에서 볼 때 실제적으로 rendering을 하기 위해 사용되는 shader를 알아본다.</summary></entry><entry><title type="html">[Computer Graphics #8-2] Optics &amp;amp; Lighting - Shaders</title><link href="https://shvtr159.github.io/graphics/computer-graphics-8-2-optics-lighting-shading-2/" rel="alternate" type="text/html" title="[Computer Graphics #8-2] Optics &amp;amp; Lighting - Shaders" /><published>2021-05-09T00:00:00+09:00</published><updated>2021-05-09T00:00:00+09:00</updated><id>https://shvtr159.github.io/graphics/computer-graphics-8-2-optics-lighting-shading-2</id><content type="html" xml:base="https://shvtr159.github.io/graphics/computer-graphics-8-2-optics-lighting-shading-2/">&lt;p&gt;Shading 1에서 shading을 위해 알아야 할 성질과 그 성질을 표현하는 함수 중 BRDF의 Phong model에 대해서 알아보았다. 이번에는 이어서 BRDF에서 사용하는 term에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;BRDF에서 사용하는 모든 parameter는 다음과 같이 표현된다.&lt;/p&gt;
&lt;center&gt;$$\mathrm{BRDF}(\lambda, \omega_{i},\omega_{o}, u, v)$$&lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;$\lambda$ : 빛의 파장&lt;/li&gt;
  &lt;li&gt;$ \omega_{i} = (\theta_{i}, \phi_{i})$ : 2D 입사광의 방향&lt;/li&gt;
  &lt;li&gt;$ \omega_{o} = (\theta_{o}, \phi_{o})$ : 2D 반사광의 방향&lt;/li&gt;
  &lt;li&gt;$(u,v)$ : 표면에서의 위치&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 파장은 R, G, B에 맞는 각각의 BRDF를 사용할 것이기 때문에 생략할 수 있다. 또, 같은 물질의 표면이 전부 완전히 같은 분자 배열을 나타내 같은 모습을 보이지 않지만, 사람들에게는 그 차이가 잘 보이지 않기 때문에 이는 하나로 생략할 수 있다. 그 결과 우리는 다음과 같이 단순화하게 된다.&lt;/p&gt;
&lt;center&gt;$$\mathrm{BRDF}(\omega_{i},\omega_{o})$$&lt;/center&gt;

&lt;p&gt;이제 $\omega_{i}$와 $\omega_{o}$를 잘 나타내기 위해서 이와 관련된 것에 대해 알아본다.&lt;/p&gt;

&lt;h2 id=&quot;incoming-light&quot;&gt;Incoming Light&lt;/h2&gt;
&lt;p&gt;만약 자동차의 3D model이 있다면, 가장 처음에 할 일은 이 차를 만들기 위해 vertex를 만들고 연결하여 triangle을 만드는 것이다. 이후에 model에 알맞은 색을 얹는다. 그 뒤에 빛이 반사되는 결과들을 표현하기 위해 light source를 추가한다. 그러나 여기까지 만으로는 현실의 차와 같을 수 없다. 자동차의 표면은 그 주위의 모습까지 반사되어 우리 눈에 들어오기 때문이다. 실제로도 현실에 하얀 물체가 있을 때 light source가 없는 부분에서도 그 주위에서 반사된 빛에 의해 다른 색을 띤다.&lt;/p&gt;

&lt;p&gt;그러나 이처럼 다른 물체에서 반사되온 빛을 우리가 보기 위해서는 light source에서 온 빛의 반사 등 모든 것을 계산해야 하는데 이는 매우 힘들다. 그래서 대신 ambient light와 비슷한 방법을 사용한다. 현실 같은 모습을 보이기 위해서 이 차에 들어오는 빛이 어떤 light source, 즉 어떤 물체가 light source 역할을 하여 차량 model에 왔는지를 얻어내고자 하는 것이다. 즉, &lt;strong&gt;실제 light source는 생략하고 차량의 모습을 나타내기 위해 필요한 빛이 어떻게 들어오는지 Incoming light 만을 생각한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 Incoming Light를 얻는 여러 방법이 있지만, 일반적으로 Incoming Light를 저장하는 방법을 사용한다. 그 대표적인 방법은  light probe로 크기 등을 아는 완벽한 구 모양의 거울 재질 물체를 놓고 사진을 찍는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117568353-c5191500-b0fa-11eb-945f-9528fc702c7c.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 방법을 사용하였을 때 특정 방향에서 반사되어 카메라에 보이는 빛은 구를 뚫고 들어가 구 가운데 point에 도달하게 될 빛이다. 즉 구의 위치에 따라 해당 위치로 들어오는 빛을 확인할 수 있다. 예를 들어 위의 구를 찍고 있는 모습은 해당 위치에서 구를 향해 빛이 들어간 뒤 그 빛이 반사되어 카메라로 들어와 보이게 되는 것이다. &lt;strong&gt;즉 지금 보이는 모습들은 모두 구의 입장에서 light source라는 것이다.&lt;/strong&gt; 예를 들어 만약 차에서 이를 나타낼 때는 차의 표면에 하늘의 위치에서 들어온 빛이면 그 하늘색의 light source에서 나온 빛을 차에 보내는 것이다. 최종적으로는 이런 방식으로 온 모든 빛이 Incoming Light가 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117568542-c991fd80-b0fb-11eb-9fa2-823e44302fe9.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Incoming Light를 model에 적용한 모습&lt;/span&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 얻은 Incoming Light를 model에 적용하기 위해 light probe를 model을 감싸는 반구의 형태로 나타내면, model에 들어온 많은 빛들 중 model의 표면에 반사되어 camera에 오는 빛이 어떤 것이 되는지 계산 할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;the-lighting-equation&quot;&gt;The Lighting Equation&lt;/h2&gt;
&lt;p&gt;Rendering을 한다는 것은 보고 있는 pixel에서 camera로 들어오는 Light에 영향을 미치는 Light source를 모두 찾아내서 그 reflection 결과를 얻어내고 합치는 일이다. 때문에 나가는 빛은 들어오는 모든 빛들의 합이므로 $L_{o} = \sum_{i\in in}^{}L_{o\;\mathrm{due\;to}\;i}(\omega_{i}, \omega_{o})$로 표현되고, BRDF가 우리에게 $L_{o\;\mathrm{due\;to}\;i}(\omega_{i}, \omega_{o})$를 알려준다.&lt;/p&gt;

&lt;p&gt;이 개념에서 좀 더 realistic하게 만들기 위한 방법으로, &lt;strong&gt;내가 보고 있는 것은 point보다는 surface의 작은 면적에서 반사되는 빛들의 합이라고 생각할 수 있다.&lt;/strong&gt; 면적으로 생각하지 않으면 빛의 밝기에 따른 interaction을 고려하기가 힘들기 때문이다. 들어오는 빛의 양이 많고 적음으로 밝기 같은것이 결정되는데 point는 물리적이지 못해서 이런 것을 결정할 수 없다. 때문에 우리가 보는 것은 다음 식과 같이 바뀌어 그 면적에 들어오는 빛들의 합이라고 할 수 있다.
&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117569080-d2380300-b0fe-11eb-9912-ef3443dcd656.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;solid-angle&quot;&gt;Solid Angle&lt;/h3&gt;
&lt;p&gt;이제 빛을 3차원에서 계산해야 하기 때문에 3차원에서의 단위면적을 결정해야만 한다. radian을 3차원으로 확장해서 넓이가 $r^{2}$이 되는 원뿔의 range를 steradian이라고 한다. 3차원 원점을 중심으로 모든 방향으로 균일한 면적대비 빛을 계산할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Solid Angle&lt;/strong&gt; : $\omega = \frac{A_{\mathrm{\,on\;spehere}}}{r^{2}}$, (구는 4$\pi$ steradian을 가진다) $A=4\pir^{2}$
&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117569423-8ede9400-b100-11eb-8979-4a06da09ca17.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;빛의-radiant-intensity&quot;&gt;빛의 Radiant Intensity&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;anisotropic&lt;/strong&gt; : light source를 둘러싸는 선을 그리면 빛은 무조건 선을 통과해서 나가야 한다. 그러면 이 빛들을 모두 합치면 light source의 크기와 같다. 때문에 특정 방향으로 나가는 빛만을 고려하고 싶으면 해당 영역에서만 integration을 수행하면 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$$I(\omega)=\frac{d\Phi}{d\omega}$$&lt;/center&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;isotropic&lt;/strong&gt; : 빛이 모든 방향으로 균일하게 퍼져나가는 상태로, 이 빛은 전체 광량 중에서 면적을 차지하는 만큼의 비율을 통해 계산할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;$$\Phi=\int_{\mathrm{sphere}}^{}Id\omega = 4\pi I$$&lt;/center&gt;
&lt;p&gt;이때, $\Phi$는 빛의 total power이다.&lt;/p&gt;

&lt;h3 id=&quot;표면에서의-irradiance&quot;&gt;표면에서의 Irradiance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117569756-33ada100-b102-11eb-9c15-7dac1ea3e547.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 그림과 같이 빛이 A라는 면적에 들어온다면 바닥 면은 7개의 빛과 상호작용 하지만, 수직하지 않고 각을 가진다면 들어오는 빛의 양이 줄어들게 된다. 이를 고려해야 특정 영역에 떨어지는 빛의 양을 계산할 수 있기 때문에 오른쪽 그림과 같이 고려해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;solid-angle-radiant-intensity-vs-area-irradiance&quot;&gt;Solid Angle (Radiant Intensity) vs Area (Irradiance)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117569883-b2a2d980-b102-11eb-97a5-f2b38e469ccb.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어딘가 light source가 있을 때, cos값을 곱해줘서 들어오는 빛의 양을 조절해줘야 하고, light source와의 거리에 따라 빛의 세기가 점점 줄어들기 때문에 이를 고려해주기 위해 거리의 제곱에 비례해서 줄어들도록 나눠준다. 그러나 r 하나만으로 계산하게 되면 밝기의 변화 차이가 너무 크게 바뀌거나 하므로 $r^{2}$ 대신 $a+bd+cd^{2}$의 3개의 term을 가진 변수를 이용하기도 한다.&lt;/p&gt;
&lt;center&gt;$$d\omega = \frac{dA\cos\theta}{r^{2}}$$&lt;/center&gt;

&lt;p&gt;이때 둘 중 light source에서 나가는 빛의 angle을 고려하여 생각하는 것은 source의 성격을 좀 더 중요시하게 된다. 그러나 실제로 표현하고 approximation을 하기 위해서는 area에 들어오는 빛을 계산하는 것이 더 좋기 때문에 Area를 주로 고려하는 방법을 사용한다.&lt;/p&gt;

&lt;p&gt;이제 Light Equation은 $L_{o\;\mathrm{due\;to}\;i}(\omega_{i}, \omega_{o}) = BRDF(\omega_{i}, \omega_{o})dE_{i}$ 에서 $dE = Ld\omega\cos\theta$에 의해
$L_{o\;\mathrm{due\;to}\;i}(\omega_{i}, \omega_{o}) = BRDF(\omega_{i}, \omega_{o})L_{i}d\omega_{i}\cos\theta_{i}$로 표현할 수 있고, 최종적으로 모든 방향에서 들어오는 빛을 고려하여 다음과 같이 정리할 수 있다.&lt;/p&gt;
&lt;center&gt;$$L_{o} = \int_{i\in in}BRDF(\omega_{i}, \omega_{o})L_{i}d\omega_{i}\cos\theta_{i}$$&lt;/center&gt;
&lt;h3 id=&quot;point-lights&quot;&gt;Point lights&lt;/h3&gt;
&lt;p&gt;지금 light equation은 기본적으로 point light를 사용하고 있다. 때문에 여러 개의 point light를 사용하면 그 sum으로 빛의 양을 계산한다. 그러나 &lt;strong&gt;원래 point light가 아니었던 것을 point light로 근사하면서 몇 가지 문제가 생긴다.&lt;/strong&gt; 일단 면적으로 표현하는 것보다는 광량이 줄어들고, point light 하나이다 보니 그림자가 완전히 검게 나타난다. 이런 상황에서는 ambient light가 정의되더라도 그림자와 그림자가 아닌 부분의 차이가 매우 크게 나타난다. 실제로는 그 경계가 모호하다. 때문에 point light를 쓰더라도 여러 개를 정의해서 사용해야 한다.&lt;/p&gt;

&lt;h2 id=&quot;surface&quot;&gt;Surface&lt;/h2&gt;
&lt;p&gt;BRDF는 물체의 반사 특성을 나타내는 만큼 Surface의 특징에 관한 term 또한 가지고 있다.&lt;/p&gt;
&lt;h3 id=&quot;diffuse-materials&quot;&gt;Diffuse Materials&lt;/h3&gt;
&lt;p&gt;이전에 보았듯이 Diffuse Material은 입사한 빛을 모든 방향으로 같은 양을 반사시킨다. 때문에 diffuse materials는 빛이 들어오는 각도만 중요하다. 그 이유는 들어오는 각도에 따라 광량이 달라지기 때문이고, diffuse materials는 빛이 항상 모든 방향으로 일정하게 나가기 때문이다. 이러한 이유로 diffuse materials일 경우 BRDF가 중요하지 않아 상수로 대체된다. 그래서 실제로 완벽한 diffuse materials는 없지만, 벽과 같은 비슷한 surface들은 다음과 같이 근사할 수 있다.&lt;/p&gt;
&lt;center&gt;$$BRDF(\omega_{i}, \omega_{o}) = k_{d}&lt;/center&gt;
&lt;center&gt;L_{o} = BRDF(\omega_{i}, \omega_{o})\hat{I}_{i}\cos\theta_{i} = k_{d}\hat{I}_{i}\cos\theta_{i}\qquad(=k_{d}\hat{I}_{i}\max(0,-\omega_{i}\cdot \hat{N})$$&lt;/center&gt;

&lt;p&gt;diffuse material이라도 광원의 위치에 따라 specular 처럼 보일 수 있다. 그러나 이는 그림자가 져서 그런 것으로 들어온 빛의 위치 때문이지 물체의 특성이 아니다.&lt;/p&gt;

&lt;h3 id=&quot;ambient-lighting&quot;&gt;Ambient Lighting&lt;/h3&gt;
&lt;p&gt;BRDF에도 들어오고 diffuse되고 한 결과에 ambient light에 의해 반사되는 빛을 표현해주기 위한 term이 있다. 이는 사용될 빛에다 상수를 곱해서 표현된다. 이는 간단하게 $L_{o}=k_{a} \hat{I}_{i}$로 표현되며 k는 각  object에 대한 ambient reflectivity이다.&lt;/p&gt;

&lt;p&gt;또한, R,G,B에 따라 다른 incoming ambient light를 사용하고 그에 따른 각각의 k도 존재하므로 다음과 같이 정리된다.&lt;/p&gt;
&lt;center&gt;$$(L_{o,R},\,L_{o,G},\,L_{o,B})=(k_{a,R}\hat{I}_{i,R}, \,k_{a,G}\hat{I}_{i,G}, \,k_{a,B}\hat{I}_{i,B})$$&lt;/center&gt;

&lt;p&gt;ambient light에 대해 이야기 했으므로 일반적인 Light Type을 다시 한번 정리해보자.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Ambient light&lt;/strong&gt; : 방향성이 없고 전체 균일하게 나타난다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Directional light&lt;/strong&gt; : daylight처럼 parallel하게 들어오는 light. 때문에 daylight는 point light만으로는 표현하기 힘들 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Point light&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spoit light&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Area light&lt;/strong&gt; : point가 아닌 면적에서 나오는 빛으로 point를 모아서 표현하거나 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reflection-model&quot;&gt;Reflection model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117576610-8480c200-b121-11eb-82ff-3af21149f3e3.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 본 것처럼 중요한 term은 diffuse  term, 이와 반대되는 specular term, 그리고 ambient term이 있을 수 있다. 지금까지 설명한 것으로 볼 때, diffuse RGB, specular RGB, ambient RGB 이 9개의 종류의 빛이 camera로 들어오는 빛을 결정하는 요소가 된다. 이를 이용하면 좀 더 쉽게 BRDF를 결정할 수 있다.&lt;/p&gt;

&lt;p&gt;이 요소들을 모아 &lt;strong&gt;Phong model&lt;/strong&gt;은 다음과 같이 쓸 수 있다.&lt;/p&gt;
&lt;center&gt;$$I=k_{d}I_{d}\,\mathbf{l\cdot n}+k_{s}I_{s}\,(\mathbf{v\cdot r})^{\alpha}+k_{a}I_{a}$$&lt;/center&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116072889-a6faf000-a6ca-11eb-9896-1332243a5284.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 나가는 빛은 각 term에 해당하는 coefficient를 곱하고 모두 더해주는 형태로 나타난다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;diffuse term&lt;/strong&gt;은 들어오는 빛(I)에만 영향을 받기 때문에 I, 그리고 들어오는 빛과의 각도를 알아야 하기 때문에 normal 방향(n)이 영향을 주고,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;specular term&lt;/strong&gt;은 어디로 나가는지(r) 중요하고, 이 주로 나가는 r에서 어떻게 떨어진 곳에서 보는지(v)에 따라 결정해줘야 하기 때문에 이 둘이 중요하고,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ambient&lt;/strong&gt;는 전체적인 빛의 크기를 생각해주면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/117576999-e261d980-b122-11eb-93f0-93d896521817.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 phong model은 이렇게 3가지 term을 모든 Light source에 대해서 더해주면 특정 시점에서의 빛을 계산해 줄 수 있다. 결국 BRDF는 이렇게 결정된다고 알 수 있다. 그 term들의 합으로 위의 그림과 같이 표현될 수 있다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="Graphics" /><summary type="html">Shading 1에서 shading을 위해 알아야 할 성질과 그 성질을 표현하는 함수 중 BRDF의 Phong model에 대해서 알아보았다. 이번에는 이어서 BRDF에서 사용하는 term에 대해 알아본다.</summary></entry><entry><title type="html">[Computer Graphics #8-1] Optics &amp;amp; Lighting - Shading</title><link href="https://shvtr159.github.io/graphics/computer-graphics-8-optics-lighting/" rel="alternate" type="text/html" title="[Computer Graphics #8-1] Optics &amp;amp; Lighting - Shading" /><published>2021-04-25T00:00:00+09:00</published><updated>2021-04-25T00:00:00+09:00</updated><id>https://shvtr159.github.io/graphics/computer-graphics-8-optics-lighting</id><content type="html" xml:base="https://shvtr159.github.io/graphics/computer-graphics-8-optics-lighting/">&lt;p&gt;지금까지는 물체의 vertex, mesh 등을 만들어 물체의 특징과 형상을 나타내는 것에 대해 배웠다. 그러나 실체 물체를 볼 때 물체가 같은 색과 형상을 가지고 있더라도 광원의 위치에 따라 다른 색으로 보이게 된다.&lt;/p&gt;

&lt;p&gt;또, 빛이 물체의 표면에 닿게 되면 반사되기도 하고 흡수되기도 한다. 같은 물체라도 광원의 위치에 따라 색이 달라지고 흰 광원에서 나온 빛에 의해서도 물체는 다른 색으로 보일 수 있다. 이렇게 물체에 따라 흡수량과 반사량 등등이 달라지고 이로 인해 물체의 모습이 달라진다.&lt;/p&gt;

&lt;p&gt;이러한 것들 때문에 물체의 실제 형상에 맞도록 물체의 밝기 값을 변화시키는 일이 필요한데 이를 &lt;strong&gt;shading&lt;/strong&gt;이라고 한다. shading이 추가되면 물체의 색을 보고 광원의 위치, 물체의 재질을 추측하는 등 다양한 정보를 드러낼 수 있다.&lt;/p&gt;

&lt;p&gt;shading을 하기 위해 필요한 고려사항으로는&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Light sources&lt;/li&gt;
  &lt;li&gt;Material properties&lt;/li&gt;
  &lt;li&gt;Location of viewer&lt;/li&gt;
  &lt;li&gt;Surface orientation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;등이 있다. shading을 실제와 같이 잘 나타내기 위해서는 어떻게 빛들이 동작하고 객체의 어떤 성질 때문에 어떤 형상이 나타나는지를 알아야 한다.&lt;/p&gt;
&lt;h2 id=&quot;scattering&quot;&gt;Scattering&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116068145-df97cb00-a6c4-11eb-9c08-a79405ce488a.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;광원에서 나온 빛이 만약 A에 닿고 반사되면 그 빛이 B로 가서 다시 반사될 수 있다. 즉 빛은 어떤 물체에 부딪혔을때  때문에 어떤 물체에서 반사되어 눈으로 들어온 빛은 광원에서 바로 반사돼온 빛이 아니라 여기저기 반사된 뒤 온 빛일 수도 있다. &lt;strong&gt;즉, 우리가 보고 있는 것은 수많은 Light path의 조합이다.&lt;/strong&gt; 그렇기 때문에 광원에서 반사되는 빛만을 확인한다면 실제 같지가 않다.&lt;/p&gt;

&lt;p&gt;이렇게 빛이 쏘여졌을 때, 주변과 어떤 상호작용을 하고 카메라 쪽으로 모여지는지를 계산하기 위해서 &lt;strong&gt;Rendering Equation&lt;/strong&gt;을 사용한다. 그러나 실제와 같이 표현하기 위해서는 각 픽셀마다의 값들을 계산해야 하는데 이는 너무 많은 변수가 있어 계산 자체가 힘들다. 때문에 이런 모든 상황들이 반영되는 것은 불가능하므로 어떤 component를 넣었을 때 어떤 효과가 나는지 알고 있어야 좀 더 realistic하기 위해서는 어떤 것을 추가해야 하는지 알 수 있다.&lt;/p&gt;
&lt;h3 id=&quot;global-effects&quot;&gt;Global Effects&lt;/h3&gt;
&lt;p&gt;그 종류를 몇 가지 보자면, 어떤 scene을 보고 있을 때, 어떤 빛은 광원으로부터 직접 눈으로 들어오고, 또 어떤 빛은 물체에 반사되어 들어온다. 또 어떤 물체에 일부 빛은 흡수되거나 투명한 물체에 일부는 투과되고 일부의 빛만 반사되어 올 수 있다. 이러한 상황에서 광원이 쏘여지지 않은 부분은 그림자가 생긴다. 그런데 이때 &lt;strong&gt;그림자는 완벽한 black이 아니다.&lt;/strong&gt; 그 이유는 광원으로부터 직접 오는 빛은 없지만 &lt;strong&gt;다른 물체에 반사된 빛이 shadow부분에 닿고 거기서 또다시 반사된 빛이 눈에 들어오기 때문이다.&lt;/strong&gt; 이는 하나의 큰 광원만 존재하는 것이 아니라 &lt;strong&gt;밝기가 다른 다양한 광원으로부터 나온 빛이 눈에 들어오는 것&lt;/strong&gt;이라고 생각할 수 있다. 이 관점에서 보면 내 주변의 모든 물체들은 모두 광원이라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;그러나 Rendering을 할 때, 이렇게 모든 상황을 다 표현하는 것이 realistic 하게 하는 데 가장 좋지만, 어렵기 때문에 적절히 approximate하는 것이 필요하다. 이런 것을 얼마만큼 해야 하는지가 중요하다.&lt;/p&gt;

&lt;h2 id=&quot;light-sources&quot;&gt;Light Sources&lt;/h2&gt;
&lt;p&gt;Light를 이해하기 위해 이에 대해 알아본다. 광원은 물체를 볼 수 있도록 하는 가장 기본적인 원천이다. 일반적으로 가장 많이 사용하는 광원은 daylight이다. 그리고 우리는 이러한 빛들을 묘사해야 한다. 그러면 광원을 표현하기 위해 어떻게 해야 하는가?&lt;/p&gt;

&lt;p&gt;실제 Light는 volume을 차지하는데 이렇게 되면 volume을 차지하는 각 부분에서 모두 빛이 나오게 된다. 그러다 보면 너무 많은 곳에서 빛이 오는 것처럼 되니 계산하기에 매우 어려워진다.&lt;/p&gt;

&lt;p&gt;때문에 광원을 실제와는 다르게 이상적인 광원을 설정한다. 이러한 Simple Light Source는 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Point source&lt;/strong&gt; : 광원이 volume을 차지하는 것이 아닌 하나의 point로 설정한다. Light가 오는 위치를 하나로 근사할 수 있어 물체로 오는 빛을 그 점에서 오는 하나의 직선으로 표현할 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spotlight&lt;/strong&gt; : 한 점에서 퍼져나가서 특정 영역만 비추는 light source. point source는 모든 방향으로 빛이 퍼져나가지만, 이는 특정 범위 내로만 빛이 나간다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ambient light&lt;/strong&gt; : 같은 양의 빛이 어디에나 존재하는 light source로 공간의 모든 부분에 light source가 존재한다. 전체적으로 어둡거나 밝은 분위기를 가져다주는 요건이다. multiple reflection을 모두 수행할 수 없기 때문에 reflection의 정도에 따른 변화를 표현해 줄 수 있도록 이를 전반적인 밝기로 근사시켰다. 실제로는 물론 존재하지 않는 가상의 light source이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-light-works&quot;&gt;How Light Works&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116069697-a6605a80-a6c6-11eb-935e-d3b813ec4d73.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;빛이 물체에 닿게 되면 대표적으로 다음 3가지가 일어난다&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reflection (반사)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Absorption (흡수)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transmission (투과)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;물체에서 일어나는 투과, 반사와 같은 일들은 물체의 매우 중요한 특징이다. 그래서 이 정보들을 모델에 담기 위해서 &lt;strong&gt;distribution function&lt;/strong&gt;을 이용하여 그 관계를 표현한다. 이는 빛이 반사되거나 투과되는 분포가 어떤지를 마치 function처럼 표현한 것이다. 이 function을 이용해 어떤 function의 어떤 parameter인지를 설정하면 각 object의 특징들을 나타낼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;distribution-function&quot;&gt;distribution function&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;BRDF&lt;/strong&gt; (Bidirectional Reflectance Distribution Function) : 모델의 반사에 대한 정보를 표현하는 function&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BTDF&lt;/strong&gt; (Bidirectional Transmittance Distribution Function) : 모델의 투과에 대한 정보를 표현하는 function&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BSSRDF&lt;/strong&gt; (Bidirectional Surface Scattering Reflectance Distribution Function) : 모델의 반사와 투과 모두를 표현하는 함수&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116071009-423e9600-a6c8-11eb-9bf1-be314aed5a9d.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; width=&quot;70%&amp;quot;, height=&amp;quot;70%&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;BRDF vs BSSRDF&lt;/span&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이처럼 같은 모양과 색을 가진 물체더라도 빛의 특성에 따라 전혀 다른 물체가 될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;surface-types&quot;&gt;Surface Types&lt;/h2&gt;
&lt;p&gt;이렇게 물체의 표면에 닿았을 때 일어나는 빛의 특성과 그 성격에 따라 realistic한 정도가 달라지기 때문에 surface에서는 어떤 일이 일어나는지에 대해 알아본다.&lt;/p&gt;

&lt;p&gt;물체의 표면은 크게 Smooth한 표면과 rough한 표면이 있다. Smooth는 표면이 fine하고 flat하다. 그 예로는 종이, 모니터 등이 있다. Rough한 표면은 표면이 꺼끌꺼끌하고 모양이 복잡하다. 그 예로는 돌, 사포 등이 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116072319-e07f2b80-a6c9-11eb-9638-ff6b8549a433.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;물체는 smooth한 surface일수록 들어온 빛이 한 방향으로 반사되고자 하는 경향이 강하고, rough한 surface일수록 모든 방향으로 반사되고자 하는 경향이 크다.&lt;/p&gt;

&lt;h3 id=&quot;phong-model&quot;&gt;Phong Model&lt;/h3&gt;
&lt;p&gt;이런 것들을 표현해주는 BRDF중에서 하나의 예가 Phong Model이다.  빛이 들어와서 퍼져나가는 성격을 설명해주는 가장 간단한 BRDF 모델로 다음 3개의 term을 가지고 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Diffuse : 얼마나 퍼트려주는지 (like rough)&lt;/li&gt;
  &lt;li&gt;Specular : 얼마나 한쪽으로 모아서 반사되는지의 정도 (like smooth)&lt;/li&gt;
  &lt;li&gt;Ambient : 위에 이야기했던 ambient와 비슷하게 전체적으로 밝고 어두운 정도를 조절&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또, 이를 표현하기 위해 다음 4개의 vector를 이용한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;To source : 빛이 어디에서 오는지 (I)&lt;/li&gt;
  &lt;li&gt;To viewer : 반사각을 기준으로 어디에서 보는지 (v)&lt;/li&gt;
  &lt;li&gt;Normal : 빛이 어느 방향으로 나갈지를 결정하기 위한 normal 방향 (n)과 반사각 (r)&lt;/li&gt;
  &lt;li&gt;Perfect reflector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116072889-a6faf000-a6ca-11eb-9896-1332243a5284.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약 물체가 이상적인 반사를 하는 물체라면 이를 Ideal Reflector라고 하고 이 물체로 들어온 빛의 입사각과 반사각은 같다.&lt;/p&gt;

&lt;h3 id=&quot;lambertian-surface&quot;&gt;Lambertian Surface&lt;/h3&gt;
&lt;p&gt;거울과 같이 들어오는 빛을 한 방향으로 모두 반사시키는 이상적인 smooth surface와 반대로 이상적인 rough surface의 형태를 뜻한다. 완벽하게 빛이 퍼져나가기 때문에 어디서 빛이 들어오든 간에 모든 방향으로 같은 양을 반사한다. 때문에 이 물체는 아무리 돌리더라도 물체의 부분 간에 밝기 차이가 없이 동일하게 보인다.&lt;/p&gt;

&lt;h3 id=&quot;specular-surface&quot;&gt;Specular Surface&lt;/h3&gt;
&lt;p&gt;대부분의 표면은 이상적일 수 없다. 때문에 모두는 아니어도 한쪽 방향으로 많은 빛을 반사시키고자 하는 smooth surface의 경우 specular highlight를 가지게 되는데 이는 광원으로부터 들어온 빛들이 가장 많이 반사되는 방향에 camera가 위치하게 되면서 나타난다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116073743-ca726a80-a6cb-11eb-89dd-834638265d54.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Specular Reflection을 모델링 하기 위해서 Phong model은 다음과 같은 term을 사용한다.&lt;/p&gt;

\[I_{r}\sim k_{s}Icos^{\alpha}\phi\]

&lt;ul&gt;
  &lt;li&gt;$I_{r}$ : reflected intensity&lt;/li&gt;
  &lt;li&gt;$k_{s}$ : absorption coefficient. 반사될 빛의 총량을 결정&lt;/li&gt;
  &lt;li&gt;$I$ : incoming intensty : 들어온 빛의 세기&lt;/li&gt;
  &lt;li&gt;$cos{\alpha}\phi$ : shininess coefficient. specular term 내부.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이때,  shininess coefficient인 $cos^{\alpha}\phi$은 $\alpha$값이 커질수록 cos함수가 뾰족해지면서 specularity가 점점 심해진다. specularity가 점점 심해지면 빛이 한쪽 방향으로만 반사시키는 특성이 강해지게  된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/116074937-55a03000-a6cd-11eb-95e5-26fb35b0597a.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;때문에 $\alpha$값이 100~200일 경우 보통 metal과 같은 표면이 되고,  $\alpha$값이 5~10일 경우 보통 플라스틱과 같은 표면을 나타내게 된다.&lt;/p&gt;</content><author><name>KYG</name></author><category term="Graphics" /><summary type="html">지금까지는 물체의 vertex, mesh 등을 만들어 물체의 특징과 형상을 나타내는 것에 대해 배웠다. 그러나 실체 물체를 볼 때 물체가 같은 색과 형상을 가지고 있더라도 광원의 위치에 따라 다른 색으로 보이게 된다.</summary></entry><entry><title type="html">[Computer Graphics #7] Meshing and Geometry 2</title><link href="https://shvtr159.github.io/graphics/computer-graphics-7-meshing-and-geometry-2/" rel="alternate" type="text/html" title="[Computer Graphics #7] Meshing and Geometry 2" /><published>2021-04-21T00:00:00+09:00</published><updated>2021-04-21T00:00:00+09:00</updated><id>https://shvtr159.github.io/graphics/computer-graphics-7-meshing-and-geometry-2</id><content type="html" xml:base="https://shvtr159.github.io/graphics/computer-graphics-7-meshing-and-geometry-2/">&lt;p&gt;Meshing and Geometry 1 에서는 기본적인 geometry를 위한 triangle을 만드는 것에 대해서 배워보았다. 이번 geometry2 에서는 post-processing을 해서라도 좀 더 다양한 shape를 만드는 방법에 대한 geometry를 알아본다.&lt;/p&gt;

&lt;h2 id=&quot;points-to-curve&quot;&gt;Points to Curve&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/115556206-61fe4480-a2eb-11eb-8fa8-245eaaddc283.png&quot; alt=&quot;image&quot; title=&quot;https://slidetodoc.com/chapter-14-polynomial-interpolation-interpolation-extrapolation-interpolation-data/&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;regression &amp;amp; interpolation&lt;/span&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;우리가 vertex 위에 면을 붙이는 과정을&lt;strong&gt;regression&lt;/strong&gt; 또는 &lt;strong&gt;interpolation&lt;/strong&gt;에 해당하는 과정과 비교해서 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;일반적으로 regression의 경우 한정된 관측값을 이용해서 noise가 포함됐을 것을 가정하고 그 data들이 나타내는 모양이 어떤 모양에 가까울 것인지를 추론한다. 이 모양과 실제 관측값의 차이는 약간의 error로 간주할 수 있다.&lt;/p&gt;

&lt;p&gt;interpolation은 주어진 점에 해당하는 값들이 정확하다고 가정하고 그 사이의 값들을 추정하여 data의 모양을 나타낸다. meshing을 할 때, vertex가 존재하고 그 vertex들 사이를 triangle로 채웠다. 이 채웠다는 말은 존재하지 않는 부분에 대해 면으로 메꾸도록 interpolation을 수행했다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;그렇지만 model을 나타내기 위한 vertex는 수없이 많기 때문에 이 vertex들을 모두 만족시킬 수 있는 하나의 고차함수로 regression/interpolation하는 일은 거의 불가능에 가깝다. 그래서 우리는 적절한 형태를 나타낼 수 있도록 local 영역들을 따로따로 수행하며 적절한 face들을 만들어 낸다. 이를 &lt;strong&gt;Spline&lt;/strong&gt;한다고 한다.&lt;/p&gt;

&lt;h3 id=&quot;spline&quot;&gt;Spline&lt;/h3&gt;
&lt;p&gt;위에서 설명했듯이 spline은  lower-order 다항식을 일부 data만을 이용하여 생성하고 붙여나가는 piecewise한 방법으로 수행된다. spline의 장점은 (1) 진동을 최소화하고, (2) low-order 특성으로 인한 반올림 오차를 줄이고 (3) 더 나은 근사치를 제공한다. spline을 수행할 때는 data가 noisy한 경우 그 부분을 다 반영하다 보면 문제가 생기므로 이를 위한 적절한 local 영역을 찾아내는 것이 중요하다. 이 영역들을 찾아 따로따로 수행하고 연결한다.&lt;/p&gt;

&lt;h4 id=&quot;b-spline-basis-spline&quot;&gt;B-spline (Basis-spline)&lt;/h4&gt;
&lt;p&gt;spline 방법 중 하나로 control point를 사용하여 모양을 변화시킨다. B-spline에서 vertex들을 연결하는 선의 모양은 basis function에 의해 결정된다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;j차 i번째 B-spline basis function은 다음과 같이 재귀적으로 정의된다.&lt;/p&gt;

\[N_{i,0}(u)=\left\{\begin{matrix}
1 \quad(if\; u_{i}\leq u&amp;lt; u_{i+1})\\ 
0 \quad(otherwise)
\end{matrix}\right.\]

\[N_{i,j}(u)=\frac{u-u_{i}}{u_{i+j}-u_{i}}N_{i,j-1}(u)+\frac{u_{i+j+1}-u}{u_{i+j+1}-u_{i+1}}N_{i+1,j-1}(u)\]
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/115740145-803a7200-a3c9-11eb-900f-0e1c6b5c761e.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이때, 0차를 convolution하여 1차를 만들고, 1차를 convolution하여 2차를 만드는 방식으로 차수가 높아질수록 더 smooth한 curve가 된다. 보통 2차나 3차를 이용하여 만든다.
그러면 B-spline function은 다음과 같이 정의된다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;\(C(u) = \sum_{i=0}^{n}N_{i,p}(u)P_{i}\)&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;P는 2D/3D 상에서의 control point&lt;/li&gt;
    &lt;li&gt;$n+1 = m - p$ 식을 통해 간격 m의 수와 차수 p에 기반해 얼마나 많은 control point가 필요한지 결정해 준다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제 위의 function을 이용해 curve를 만든다. B-spline에는 line 지나야 하는 vertex인 &lt;strong&gt;knot&lt;/strong&gt;와 지나지 않고 curve의 모양을 결정해주는 &lt;strong&gt;control point&lt;/strong&gt;가 있다. knot들을 n차 basis function을 이용하여 연결해 주는데 C0, C1 등을 고려하며 자연스럽게 연결할 수 있다. 그러나 그림과 같이 function은 symmetric하기 때문에 단순히 연결만 하게 된다면 부자연스러울 수 있다. 이때 control point를 이용하게 되는데 control point에 의해 knot 사이의 값들의 가중치가 달라지며 curve의 모양이 변하게 된다.&lt;/p&gt;

&lt;h4 id=&quot;nurbs-non-uniform-rational-b-spline&quot;&gt;NURBS (Non-Uniform Rational B-Spline)&lt;/h4&gt;
&lt;p&gt;B-spline에 weight가 추가되어 curve의 모양을 결정한다. NURBS curve는 다음과 같은 식으로 정의된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;\(C(u) = \frac{\sum_{i=0}^{n}N_{i,p}(u)w_{i}P_{i}}{ \sum_{i=0}^{n}N_{i,p}(u)w_{i}}\)&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;N은 B-spline의 basis function, P는 control point, w는 P의 weight를 의미한다.&lt;/li&gt;
    &lt;li&gt;w, 즉 control point의 가중치가 커지면 curve가 이 control point에 더 가까워지고, 작아지면 더 멀어진다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/115749596-3609be80-a3d2-11eb-9a82-1c32cd8a2032.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 idea는 surface로 확장될 수 있고, 이 방식을 이용해 복잡한 천의 움직임 등을 vertex를 추가하거나 수정하지 않아도 자연스럽게 나타낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;implicit-surfaces&quot;&gt;Implicit Surfaces&lt;/h2&gt;
&lt;p&gt;지금까지 surface에  triangle을 만들어 값을 채우는 것처럼 explicit하게 Surface를 정의했다. &lt;strong&gt;Implicit Surfaces는 모든 공간에 값이 존재하고, 값이 0인 부분을 surface, 안쪽은 음의 값, 바깥쪽은 양의 값으로 설정한다.&lt;/strong&gt; 이 방법은 object의 내부를 확인하기 쉽고, 액체처럼 object의 변화가 많은 object를 표현하기에 좋다.&lt;/p&gt;

&lt;h3 id=&quot;constructive-solid-geometry-csg&quot;&gt;Constructive Solid Geometry (CSG)&lt;/h3&gt;
&lt;p&gt;복잡한 모양을 표현할 때 모양이 규칙성을 가지고, 일정한 모양의 object들의 합, 차로 만들 수 있다면 기본 component를 이용해서 만들어 낸다. 복잡한 객체의 모양을 기본 component의 mesh의 합, 차, 교집합을 이용해 만들기 때문에 복잡한 객체의 triangle 등을 모두 가질 필요가 없다. 주로 자연의 객체가 아니라 사람이 만든 객체들을 표현하기에 좋다.
&lt;a href=&quot;https://en.wikipedia.org/wiki/Constructive_solid_geometry&quot; class=&quot;align-center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/115756390-204bc780-a3d9-11eb-973a-2ec172b98ff9.png&quot; alt=&quot;image&quot; title=&quot;https://en.wikipedia.org/wiki/Constructive_solid_geometry&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 Union, Difference, Intersection의 연산으로 생성되며 CSG object는 binary tree에 의해 표현되며 implicit surface에 쉽게 적용된다.&lt;/p&gt;

&lt;h3 id=&quot;marching-cubes&quot;&gt;Marching Cubes&lt;/h3&gt;
&lt;p&gt;Implicit Surface나 CSG로 생성된 model들은 최종적으로 explicit하게 나타내야 눈에 보이게 완성할 수 있다. 이때 사용하는 대표적인 방법이 Marching cube이다.&lt;/p&gt;

&lt;p&gt;이 방법은 Cube가 이동하면서 값들을 scan하게 된다. 매 volume마다 값을 확인하는데 만약 &lt;strong&gt;cube 내에 +값과 -값이 공존할 경우 triangle을 생성한다.&lt;/strong&gt; 이때의 값은 cube의 꼭짓점에서의 값을 확인하게 되는데 한 꼭짓점에 연결된 다른 꼭짓점의 값의 부호가 다르다면 그 사이에 surface가 존재하는 것이므로  그 사이에 vertex를 찍고 연결하여 표면을 만든다. 이때 surface는 아래의 table과 같이 점의 위치에 따라 정해진 생성 방법에 따라 만들어진다.
&lt;img src=&quot;https://user-images.githubusercontent.com/79836443/115752845-7880ca80-a3d5-11eb-8392-084ca01b80aa.png&quot; alt=&quot;image&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;span style=&quot;color:rgb(150, 150, 150)&quot;&gt;Look up table&lt;/span&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 방법은 다음 영상을 보면 쉽게 이해할 수 있다.
&lt;a href=&quot;https://www.youtube.com/watch?v=kIfVICKtA1g&quot;&gt;https://www.youtube.com/watch?v=kIfVICKtA1g&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;3d-scanning&quot;&gt;3D Scanning&lt;/h2&gt;
&lt;p&gt;복잡한 model을 만들기 위해서 하나하나 모두 만들어 내는 것은 어려운 일이다. 때문에 3D 객체를 scanning해서 model을 얻어내고, refine하여 생성하기도 한다. 다음과 같은 방법들로 좀 더 쉽게 복잡한 3D model들을 얻어낼 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multiview Stereo&lt;/li&gt;
  &lt;li&gt;Shape from silhouette&lt;/li&gt;
  &lt;li&gt;Photometric stereo&lt;/li&gt;
  &lt;li&gt;Kinect&lt;/li&gt;
&lt;/ul&gt;</content><author><name>KYG</name></author><category term="Graphics" /><summary type="html">Meshing and Geometry 1 에서는 기본적인 geometry를 위한 triangle을 만드는 것에 대해서 배워보았다. 이번 geometry2 에서는 post-processing을 해서라도 좀 더 다양한 shape를 만드는 방법에 대한 geometry를 알아본다.</summary></entry></feed>